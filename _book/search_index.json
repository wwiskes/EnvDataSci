[["introduction-to-the-tidyverse.html", "Chapter 3 Introduction to the tidyverse 3.1 Background: Exploratory Data Analysis 3.2 The Tidyverse and what well explore in this chapter 3.3 Tibbles 3.4 Statistical summary of variables 3.5 Visualizing data with a Tukey box plot 3.6 Database operations with dplyr 3.7 The dot operator 3.8 Exercises", " Chapter 3 Introduction to the tidyverse At this point, weve learned the basics of working with the R language. From here well want to explore how to analyze data, both statistically and spatially. Were going to use an exploratory approach with significant application of visualization both in terms of graphs as well as maps. So lets start by exploring this exploratory approach 3.1 Background: Exploratory Data Analysis In 1961, John Tukey proposed a new approach to data analysis, defining it as Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. He followed this up in 1977 with Exploratory Data Analysis. Exploratory data analysis (EDA) in part as an approach to analyzing data via summaries and graphics. The key word is exploratory, in contrast with confirmatory statistics. Both are important, but ignoring exploration is ignoring enlightenment. Some purposes of EDA are: to suggest hypotheses to assess assumptions on which inference will be based to select appropriate inferential statistical tools to guide further data collection These concepts led to the development of S at Bell Labs (John Chambers, 1976), then R, built on clear design and extensive, clear graphics. 3.2 The Tidyverse and what well explore in this chapter The Tidyverse refers to a suite of R packages developed at RStudio (see R Studio and R for Data Science) for facilitating data processing and analysis. While R itself is designed around EDA, the Tidyverse takes it further. Some of the packages in the Tidyverse that are widely used are: dplyr : data manipulation like a database readr : better methods for reading and writing rectangular data tidyr : reorganization methods that extend dplyrs database capabilities purrr : expanded programming toolkit including enhanced apply methods tibble : improved data frame stringr : string manipulation library ggplot2 : graphing system based on the grammar of graphics In this chapter, well be mostly exploring dplyr, with a few other things thrown in like reading data frames with readr. For simplicity, we can just include library(tidyverse) to get everything. 3.3 Tibbles Tibbles are an improved type of data frame part of the Tidyverse serve the same purpose as a data frame, and all data frame operations work Advantages display better can be composed of more complex objects like lists, etc. can be grouped How created Reading from a CSV, using one of a variety of Tidyverse functions similarly named to base functions: read_csv creates a tibble (in general, underscores are used in the Tidyverse) read.csv creates a regular data frame You can also use the tibble() function library(tidyverse) # includes readr, ggplot2, and dplyr which we&#39;ll use in this chapter ## -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- ## v ggplot2 3.3.2 v purrr 0.3.4 ## v tibble 3.0.4 v dplyr 1.0.2 ## v tidyr 1.1.2 v stringr 1.4.0 ## v readr 1.4.0 v forcats 0.5.0 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(iGIScData) csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_1987_BaySites.csv&quot;, package=&quot;iGIScData&quot;) TRI87 &lt;- read_csv(csvPath) ## ## -- Column specification -------------------------------------------------------- ## cols( ## TRI_FACILITY_ID = col_character(), ## count = col_double(), ## FACILITY_NAME = col_character(), ## COUNTY = col_character(), ## air_releases = col_double(), ## fugitive_air = col_double(), ## stack_air = col_double(), ## LATITUDE = col_double(), ## LONGITUDE = col_double() ## ) a &lt;- rnorm(10) b &lt;- runif(10) ab &lt;- tibble(a,b) ab ## # A tibble: 10 x 2 ## a b ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.714 0.261 ## 2 1.03 0.774 ## 3 0.755 0.402 ## 4 -1.02 0.723 ## 5 0.0158 0.483 ## 6 0.625 0.0377 ## 7 -2.30 0.0183 ## 8 0.895 0.974 ## 9 1.19 0.491 ## 10 0.959 0.777 3.3.1 read_csv vs. read.csv You might be tempted to use read.csv from base R They look a lot alike, so you might confuse them You dont need to load library(readr) read.csv fixes some things and that might be desired: problematic field names like MLY-TAVG-NORMAL become MLY.TAVG.NORMAL numbers stored as characters are converted to numbers 01 becomes 1, 02 becomes 2, etc. However, there are potential problems You may not want some of those changes, and want to specify those changes separately There are known problems that read_csv avoids Recommendation: Use read_csv and write_csv. 3.4 Statistical summary of variables A simple statistical summary is very easy to do: summary(eucoakrainfallrunoffTDR) ## site site # date month ## Length:90 Min. :1.000 Length:90 Length:90 ## Class :character 1st Qu.:2.000 Class :character Class :character ## Mode :character Median :4.000 Mode :character Mode :character ## Mean :4.422 ## 3rd Qu.:6.000 ## Max. :8.000 ## ## rain_mm rain_oak rain_euc runoffL_oak ## Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 0.000 ## 1st Qu.:16.00 1st Qu.:16.00 1st Qu.:14.75 1st Qu.: 0.000 ## Median :28.50 Median :30.50 Median :30.00 Median : 0.450 ## Mean :37.99 Mean :35.08 Mean :34.60 Mean : 2.032 ## 3rd Qu.:63.25 3rd Qu.:50.50 3rd Qu.:50.00 3rd Qu.: 2.800 ## Max. :99.00 Max. :98.00 Max. :96.00 Max. :14.000 ## NA&#39;s :18 NA&#39;s :2 NA&#39;s :2 NA&#39;s :5 ## runoffL_euc slope_oak slope_euc aspect_oak ## Min. : 0.00 Min. : 9.00 Min. : 9.00 Min. :100.0 ## 1st Qu.: 0.07 1st Qu.:12.00 1st Qu.:12.00 1st Qu.:143.0 ## Median : 1.20 Median :24.50 Median :23.00 Median :189.0 ## Mean : 2.45 Mean :21.62 Mean :19.34 Mean :181.9 ## 3rd Qu.: 3.30 3rd Qu.:30.50 3rd Qu.:25.00 3rd Qu.:220.0 ## Max. :16.00 Max. :32.00 Max. :31.00 Max. :264.0 ## NA&#39;s :3 ## aspect_euc surface_tension_oak surface_tension_euc ## Min. :106.0 Min. :37.40 Min. :28.51 ## 1st Qu.:175.0 1st Qu.:72.75 1st Qu.:32.79 ## Median :196.5 Median :72.75 Median :37.40 ## Mean :191.2 Mean :68.35 Mean :43.11 ## 3rd Qu.:224.0 3rd Qu.:72.75 3rd Qu.:56.41 ## Max. :296.0 Max. :72.75 Max. :72.75 ## NA&#39;s :22 NA&#39;s :22 ## runoff_rainfall_ratio_oak runoff_rainfall_ratio_euc ## Min. :0.00000 Min. :0.000000 ## 1st Qu.:0.00000 1st Qu.:0.003027 ## Median :0.02046 Median :0.047619 ## Mean :0.05357 Mean :0.065902 ## 3rd Qu.:0.08485 3rd Qu.:0.083603 ## Max. :0.42000 Max. :0.335652 ## NA&#39;s :5 NA&#39;s :3 3.5 Visualizing data with a Tukey box plot ggplot(data = eucoakrainfallrunoffTDR) + geom_boxplot(mapping = aes(x=site, y=runoffL_euc)) ## Warning: Removed 3 rows containing non-finite values (stat_boxplot). 3.6 Database operations with dplyr As part of exploring our data, well typically simplify or reduce it for our purposes. The following methods are quickly discovered to be essential as part of exploring and analyzing data. select rows using logic, such as population &gt; 10000, with filter select variable columns you want to retain with select add new variables and assign their values with mutate sort rows based on a a field with arrange summarize by group 3.6.1 Select, mutate, and the pipe The pipe %&gt;%: Read %&gt;% as and then This is bigger than it sounds and opens up a lot of possibilities. See example below, and observe how the expression becomes several lines long. In the process, well see examples of new variables with mutate and selecting (and in the process ordering) variables: runoff &lt;- eucoakrainfallrunoffTDR %&gt;% mutate(Date = as.Date(date,&quot;%m/%d/%Y&quot;), rain_subcanopy = (rain_oak + rain_euc)/2) %&gt;% select(site, Date, rain_mm, rain_subcanopy, runoffL_oak, runoffL_euc, slope_oak, slope_euc) runoff ## # A tibble: 90 x 8 ## site Date rain_mm rain_subcanopy runoffL_oak runoffL_euc slope_oak ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AB1 2006-11-08 29 29 4.79 6.7 32 ## 2 AB1 2006-11-12 22 18.5 3.2 4.3 32 ## 3 AB1 2006-11-29 85 65 9.7 16 32 ## 4 AB1 2006-12-12 82 87.5 14 14.2 32 ## 5 AB1 2006-12-28 43 54 9.75 4.33 32 ## 6 AB1 2007-01-29 7 54 1.4 0 32 ## 7 AB1 2007-02-09 56 44 10.1 0 32 ## 8 AB1 2007-02-13 63 42.5 3.90 1.40 32 ## 9 AB1 2007-02-28 NA 56 4.75 8.65 32 ## 10 AB1 2007-03-22 NA 2 NA 0.11 32 ## # ... with 80 more rows, and 1 more variable: slope_euc &lt;dbl&gt; Note: to just rename a variable, use rename instead of mutate. It will stay in position. 3.6.2 filter filter lets you select observations that meet criteria, similar to an SQL WHERE clause. runoff2007 &lt;- runoff %&gt;% filter(Date &gt;= as.Date(&quot;01/01/2007&quot;, &quot;%m/%d/%Y&quot;)) runoff2007 ## # A tibble: 51 x 8 ## site Date rain_mm rain_subcanopy runoffL_oak runoffL_euc slope_oak ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AB1 2007-01-29 7 54 1.4 0 32 ## 2 AB1 2007-02-09 56 44 10.1 0 32 ## 3 AB1 2007-02-13 63 42.5 3.90 1.40 32 ## 4 AB1 2007-02-28 NA 56 4.75 8.65 32 ## 5 AB1 2007-03-22 NA 2 NA 0.11 32 ## 6 AB1 2007-04-23 NA 33.5 6.94 9.20 32 ## 7 AB1 2007-05-05 NA 31 6.34 7.43 32 ## 8 AB2 2007-01-29 4 3.5 1.26 0.05 24 ## 9 AB2 2007-02-09 37 41.5 6.3 3.3 24 ## 10 AB2 2007-02-13 43 49.5 6.78 1.14 24 ## # ... with 41 more rows, and 1 more variable: slope_euc &lt;dbl&gt; Filtering out NA with !is.na Heres an important one. There are many times you need to avoid NAs. We commonly see summary statistics using na.rm = TRUE in order to ignore NAs when calculating a statistic like mean. To simply filter out NAs from a vector or a variable use a filter: feb_filt &lt;- feb_s %&gt;% filter(!is.na(TEMP)) 3.6.3 Writing a data frame to a csv Lets say you have created a data frame, maybe with read_csv runoff20062007 &lt;- read_csv(csvPath) Then you do some processing to change it, maybe adding variables, reorganizing, etc., and you want to write out your new eucoak, so you just need to use write_csv write_csv(eucoak, \"data/tidy_eucoak.csv\") 3.6.4 Summarize by group Youll find that you need to use this all the time with real data. You have a bunch of data where some categorical variable is defining a grouping, like our site field in the eucoak data. Wed like to just create average slope, rainfall, and runoff for each site. Note that it involves two steps, first defining which field defines the group, then the various summary statistics wed like to store. In this case all of the slopes under oak remain the same for a given site  its a site characteristic  and the same applies to the euc site, so we can just grab the first value (mean would have also worked of course). eucoakSiteAvg &lt;- runoff %&gt;% group_by(site) %&gt;% summarize( rain = mean(rain_mm, na.rm = TRUE), rain_subcanopy = mean(rain_subcanopy, na.rm = TRUE), runoffL_oak = mean(runoffL_oak, na.rm = TRUE), runoffL_euc = mean(runoffL_euc, na.rm = TRUE), slope_oak = first(slope_oak), slope_euc = first(slope_euc) ) ## `summarise()` ungrouping output (override with `.groups` argument) eucoakSiteAvg ## # A tibble: 8 x 7 ## site rain rain_subcanopy runoffL_oak runoffL_euc slope_oak slope_euc ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AB1 48.4 43.1 6.80 6.03 32 31 ## 2 AB2 34.1 35.4 4.91 3.65 24 25 ## 3 KM1 48 36.1 1.94 0.592 30.5 25 ## 4 PR1 56.5 37.6 0.459 2.31 27 23 ## 5 TP1 38.4 30.0 0.877 1.66 9 9 ## 6 TP2 34.3 32.9 0.0955 1.53 12 10 ## 7 TP3 32.1 27.8 0.381 0.815 25 18 ## 8 TP4 32.5 35.7 0.231 2.83 12 12 Summarizing by group with TRI data csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_2017_CA.csv&quot;, package=&quot;iGIScData&quot;) TRI_BySite &lt;- read_csv(csvPath) %&gt;% mutate(all_air = `5.1_FUGITIVE_AIR` + `5.2_STACK_AIR`) %&gt;% filter(all_air &gt; 0) %&gt;% group_by(FACILITY_NAME) %&gt;% summarize( FACILITY_NAME = first(FACILITY_NAME), air_releases = sum(all_air, na.rm = TRUE), mean_fugitive = mean(`5.1_FUGITIVE_AIR`, na.rm = TRUE), LATITUDE = first(LATITUDE), LONGITUDE = first(LONGITUDE)) ## Warning: Missing column names filled in: &#39;X110&#39; [110] ## Warning: 3807 parsing failures. ## row col expected actual file ## 1 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## 2 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## 3 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## 4 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## 5 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## ... ... ........... ........... .................................................................................. ## See problems(...) for more details. 3.6.5 Count Count is a simple variant on summarize by group, since the only statistic is the count of events. tidy_eucoak %&gt;% count(tree) ## # A tibble: 2 x 2 ## tree n ## &lt;chr&gt; &lt;int&gt; ## 1 euc 90 ## 2 oak 90 Another way is to use n(): tidy_eucoak %&gt;% group_by(tree) %&gt;% summarize(n = n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## tree n ## &lt;chr&gt; &lt;int&gt; ## 1 euc 90 ## 2 oak 90 3.6.6 Sorting after summarizing Using the marine debris data from NOAA Marine Debris Programs Marine Debris Monitoring and Assessment Project shorelineLatLong &lt;- ConcentrationReport %&gt;% group_by(`Shoreline Name`) %&gt;% summarize( latitude = mean((`Latitude Start`+`Latitude End`)/2), longitude = mean((`Longitude Start`+`Longitude End`)/2) ) %&gt;% arrange(latitude) ## `summarise()` ungrouping output (override with `.groups` argument) shorelineLatLong ## # A tibble: 38 x 3 ## `Shoreline Name` latitude longitude ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aimee Arvidson 33.6 -118. ## 2 Balboa Pier #2 33.6 -118. ## 3 Bolsa Chica 33.7 -118. ## 4 Junipero Beach 33.8 -118. ## 5 Malaga Cove 33.8 -118. ## 6 Zuma Beach, Malibu 34.0 -119. ## 7 Zuma Beach 34.0 -119. ## 8 Will Rodgers 34.0 -119. ## 9 Carbon Beach 34.0 -119. ## 10 Nicholas Canyon 34.0 -119. ## # ... with 28 more rows 3.7 The dot operator The dot . operator derives from UNIX syntax, and refers to here. For accessing files in the current folder, the path is ./filename A similar specification is used in piped sequences The advantage of the pipe is you dont have to keep referencing the data frame. The dot is then used to connect to items inside the data frame: csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_1987_BaySites.csv&quot;, package=&quot;iGIScData&quot;) TRI87 &lt;- read_csv(csvPath) ## ## -- Column specification -------------------------------------------------------- ## cols( ## TRI_FACILITY_ID = col_character(), ## count = col_double(), ## FACILITY_NAME = col_character(), ## COUNTY = col_character(), ## air_releases = col_double(), ## fugitive_air = col_double(), ## stack_air = col_double(), ## LATITUDE = col_double(), ## LONGITUDE = col_double() ## ) stackrate &lt;- TRI87 %&gt;% mutate(stackrate = stack_air/air_releases) %&gt;% .$stackrate head(stackrate) ## [1] 0.0000000 0.0000000 0.0000000 0.0000000 0.6666667 1.0000000 3.8 Exercises Create a tibble with 20 rows of two variables norm and unif with norm created with rnorm() and unif created with runif(). Read in TRI_2017_CA.csv in two ways, as a normal data frame assigned to df and as a tibble assigned to tb. What field names result for whats listed in the CSV as 5.1_FUGITIVE_AIR? Use the summary function to investigate the variables in either the data.frame or tibble you just created. What type of field and what values are assigned to BIA_CODE? Create a boxplot of body_mass_g by species from the penguins data frame in the palmerpenguins package. Access the data with data(package = palmerpenguins), and also remember library(ggplot2) or library(tidyverse). Use select, mutate, and the pipe to create a penguinMass tibble where the only original variable retained is species, but with body_mass_kg created as \\(\\frac{1}{1000}\\) the body_mass_g. The statement should start with penguinMass &lt;- penguins and use a pipe plus the other functions after that. Now, also with penguins, create FemaleChinstaps to include only the female Chinstrap penguins. Start with FemaleChinstraps &lt;- penguins %&gt;% Now, summarize by species groups to create mean and standard deviation variables from bill_length_mm, bill_depth_mm, flipper_length_mm, and body_mass_g. Preface the variable names with either avg. or sd. Include na.rm=T with all statistics function calls. Create an penguinSort tibble, sorted by body_mass_g. "]]
