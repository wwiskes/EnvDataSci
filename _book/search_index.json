[["index.html", "Introduction to R for GIS Chapter 1 Prerequisites 1.1 Data", " Introduction to R for GIS Jerry Davis 2021-03-31 Chapter 1 Prerequisites This book is intended to work in concert with a series of lectures and discussions among the participants in Geog 9031 Introduction to R in the GIS Certificate Program at San Francisco State University. Data packages will be created on GitHub. Participants need to have R and RStudio installed, and at least the following packages: tidyverse (to include ggplot2, dplyr, tidyr, stringr, etc.) ggplot2 dplyr stringr tidyr lubridate palmerpenguins sf raster tmap 1.1 Data Well be using data from various sources, but one repository that includes data weve developed in the iGISc at SFSU is stored on GitHub and youll need to install that package. GitHub packages require a bit more work on the users part since we need to first install devtools then use that to install the GitHub data package: install.packages(&quot;devtools&quot;) devtools::install_github(&quot;iGISc/iGIScData&quot;) You may also need the rlang package installed. If you see a message about Rtools, you can ignore it since that is only needed for building tools from C++ and things like that. Then you can access it just like other built-in data by including: library(iGIScData) To see whats in it, youll see the various datasets listed in: data(package=&quot;iGIScData&quot;) "],["introduction-to-r.html", "Chapter 2 Introduction to R 2.1 Variables 2.2 Functions 2.3 Expressions and Statements 2.4 Data Types 2.5 Rectangular data 2.6 Data Structures in R 2.7 Programming and Logic 2.8 Exercises", " Chapter 2 Introduction to R Were assuming youre either new to R or need a refresher. Well start with some basic R operations entered directly in the console in RStudio. 2.1 Variables Variables are objects that store values. Every computer language, like in math, stores values by assigning them constants or results of expressions. x &lt;- 5 uses the R standard assignment operator &lt;- though you can also use =. Well use &lt;- because it is more common and avoids some confusion with other syntax. Variable names must start with a letter, have no spaces, and not use any names that are built into the R language or used in package libraries, such as reserved words like for or function names like log() x &lt;- 5 y &lt;- 8 longitude &lt;- -122.4 latitude &lt;- 37.8 my_name &lt;- &quot;Inigo Montoya&quot; To check the value of a variable or other object, you can just enter the name in the console, or even in the code in a code chunk. x ## [1] 5 y ## [1] 8 longitude ## [1] -122.4 latitude ## [1] 37.8 my_name ## [1] &quot;Inigo Montoya&quot; This is counter to the way printing out values work in programming, and you will need to know how this method works as well because you will want to use your code to develop tools that accomplish things, and there are also limitations to what you can see by just naming variables. To see the values of variables in programming mode, use the print() function, or to concatenate character string output, use paste(): print(x) ## [1] 5 print(y) ## [1] 8 print(latitude) ## [1] 37.8 paste(&quot;The location is latitude&quot;, latitude, &quot;longitude&quot;, longitude) ## [1] &quot;The location is latitude 37.8 longitude -122.4&quot; paste(&quot;My name is&quot;, my_name, &quot;-- Prepare to die.&quot;) ## [1] &quot;My name is Inigo Montoya -- Prepare to die.&quot; 2.2 Functions Once you have variables or other objects to work with, most of your work involves functions such as the well-known math functions log10(100) log(exp(5)) cos(pi) sin(90 * pi/180) Most of your work will involve functions and there are too many to name, even in the base functions, not to mention all the packages we will want to use. You will likely have already used the install.packages() and library() functions that add in an array of other functions. Later well also learn how to write our own functions, a capability that is easy to accomplish and also gives you a sense of what developing your own package might be like. Arithmetic operators There are of course all the normal arithmetic operators (that are actually functions) like + - * /. Youre probably familiar with these from using equations in Excel if not in some other programming language you may have learned. These operators look a bit different from how theyd look when creating a nicely formatted equation.\\(\\frac{NIR - R}{NIR + R}\\) instead has to look like (NIR-R)/(NIR+R). Similarly * must be used to multiply; theres no implied multiplication that we expect in a math equation like \\(x(2+y)\\) which would need to be written x*(2+y). In contrast to those four well-known operators, the symbol used to exponentiate  raise to a power  varies among programming languages. R uses ** so the the Pythagorean theorem \\(c^2=a^2+b^2\\) would be written c**2 = a**2 + b**2 except for the fact that it wouldnt make sense as a statement to R. Well need to talk about expressions and statements. 2.3 Expressions and Statements The concepts of expressions and statements are very important to understand in any programming language. An expression in R (or any programming language) has a value just like a variable has a value. An expression will commonly combine variables and functions to be evaluated to derive the value of the expression. Here are some examples of expressions: 5 x x*2 sin(x) sqrt(a**2 + b**2) (-b+sqrt(b**2-4*a*c))/2*a paste(&quot;My name is&quot;, aname) Note that some of those expressions used previously assigned variables  x, a, b, c, aname. An expression can be entered in the console to display its current value. cos(pi) ## [1] -1 print(cos(pi)) ## [1] -1 A statement in R does something. It represents a directive were assigning to the computer, or maybe the environment were running on the computer (like RStudio, which then runs R). A simple print() statement seems a lot like what we just did when we entered an expression in the console, but recognize that it does something: print(&quot;Hello, World&quot;) ## [1] &quot;Hello, World&quot; Which is the same as just typing Hello, World, but thats just because the job of the console is to display what we are looking for [where we are the ones doing something], or if our statement includes something to display. Statements in R are usually put on one line, but you can use a semicolon to have multiple statements on one line, if desired: x &lt;- 5; print(x); print(x**2) ## [1] 5 ## [1] 25 Many (perhaps most) statements dont actually display anything. For instance: x &lt;- 5 doesnt display anything, but it does assign the value 5 to the variable x, so it does something. Its an assignment statement and uses that special assignment operator &lt;- . Most languages just use = which the designers of R didnt want to use, to avoid confusing it with the equal sign meaning is equal to. An assignment statement assigns an expression to a variable. If that variable already exists, it is reused with the new value. For instance its completely legit (and commonly done in coding) to update the variable in an assignment statement. This is very common when using a counter variable: i = i + 1 Youre simply updating the index variable with the next value. This also illustrates why its not an equation: \\(i=i+1\\) doesnt work as an equation (unless i is actually \\(\\infty\\) but thats just really weird.) And c**2 = a**2 + b**2 doesnt make sense as an R statement because c**2 isnt a variable to be created. The ** part is interpreted as raise to a power. What is to the left of the assignment operator = must be a variable to be assigned the value of the expression. 2.4 Data Types Variables, constants and other data elements in R have data types. Common types are numeric and character. x &lt;- 5 class(x) ## [1] &quot;numeric&quot; class(4.5) ## [1] &quot;numeric&quot; class(&quot;Fred&quot;) ## [1] &quot;character&quot; 2.4.1 Integers By default, R creates double-precision floating-point numeric variables To create integer variables: - append an L to a constant, e.g. 5L is an integer 5 - convert with as.integer Were going to be looking at various as. functions in R, more on that later, but we should look at as.integer() now. Most other languages use int() for this, and what it does is converts any number into an integer, truncating it to an integer, not rounding it. as.integer(5) ## [1] 5 as.integer(4.5) ## [1] 4 To round a number, theres a round() function or you can easily use as.integer adding 0.5: x &lt;- 4.8 y &lt;- 4.2 as.integer(x + 0.5) ## [1] 5 round(x) ## [1] 5 as.integer(y + 0.5) ## [1] 4 round(y) ## [1] 4 Integer divison: 5 %/% 2 ## [1] 2 Integer remainder from division (the modulus, using a %% to represent the modulo): 5 %% 2 ## [1] 1 Surprisingly, the values returned by integer division or the remainder are not stored as integers. R seems to prefer floating point 2.5 Rectangular data A common data format used in most types of research is rectangular data such as in a spreadsheet, with rows and columns, where rows might be observations and columns might be variables. Well read this type of data in from spreadsheets or even more commonly from comma-separated-variable (CSV) text files that spreadsheet programs like Excel commonly read in just like their native format. library(iGIScData) sierraFeb ## # A tibble: 82 x 7 ## STATION_NAME COUNTY ELEVATION LATITUDE LONGITUDE PRECIPITATION TEMPERATURE ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GROVELAND 2, CA US Tuolu~ 853. 37.8 -120. 176. 6.1 ## 2 CANYON DAM, CA US Plumas 1390. 40.2 -121. 164. 1.4 ## 3 KERN RIVER PH 3, CA US Kern 824. 35.8 -118. 67.1 8.9 ## 4 DONNER MEMORIAL ST PAR~ Nevada 1810. 39.3 -120. 167. -0.9 ## 5 BOWMAN DAM, CA US Nevada 1641. 39.5 -121. 277. 2.9 ## 6 BRUSH CREEK RANGER STA~ Butte 1085. 39.7 -121. 296. NA ## 7 GRANT GROVE, CA US Tulare 2012. 36.7 -119. 186. 1.7 ## 8 LEE VINING, CA US Mono 2072. 38.0 -119. 71.9 0.4 ## 9 OROVILLE MUNICIPAL AIR~ Butte 57.9 39.5 -122. 138. 10.3 ## 10 LEMON COVE, CA US Tulare 156. 36.4 -119. 62.7 11.3 ## # ... with 72 more rows 2.6 Data Structures in R We looked briefly at numeric and character string (well abbreviate simply as string from here on). Well also look at factors and dates/times later on. 2.6.1 Vectors A vector is an ordered collection of numbers, strings, vectors, data frames, etc. What we mostly refer to as vectors are formally called atomic vectors which requires that they be homogeneous sets of whatever type were referring to, such as a vector of numbers, or a vector of strings, or a vector of dates/times. You can create a simple vector with the c() function: lats &lt;- c(37.5,47.4,29.4,33.4) lats ## [1] 37.5 47.4 29.4 33.4 states = c(&quot;VA&quot;, &quot;WA&quot;, &quot;TX&quot;, &quot;AZ&quot;) states ## [1] &quot;VA&quot; &quot;WA&quot; &quot;TX&quot; &quot;AZ&quot; zips = c(23173, 98801, 78006, 85001) zips ## [1] 23173 98801 78006 85001 The class of a vector is the type of data it holds temp &lt;- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7) class(temp) ## [1] &quot;numeric&quot; Vectors can only have one data class, and if mixed with character types, numeric elements will become character: mixed &lt;- c(1, &quot;fred&quot;, 7) class(mixed) ## [1] &quot;character&quot; mixed[3] # gets a subset, example of coercion ## [1] &quot;7&quot; 2.6.1.1 NA Data science requires dealing with missing data by storing some sort of null value, called various things: - null - nodata - NA not available or not applicable as.numeric(c(&quot;1&quot;,&quot;Fred&quot;,&quot;5&quot;)) # note NA introduced by coercion ## Warning: NAs introduced by coercion ## [1] 1 NA 5 Ignoring NA in statistical summaries is commonly used. Where normally the summary statistic can only return NA mean(as.numeric(c(&quot;1&quot;, &quot;Fred&quot;, &quot;5&quot;))) ## Warning in mean(as.numeric(c(&quot;1&quot;, &quot;Fred&quot;, &quot;5&quot;))): NAs introduced by coercion ## [1] NA  with na.rm=T you can still get the result for all actual data: mean(as.numeric(c(&quot;1&quot;, &quot;Fred&quot;, &quot;5&quot;)), na.rm=T) ## Warning in mean(as.numeric(c(&quot;1&quot;, &quot;Fred&quot;, &quot;5&quot;)), na.rm = T): NAs introduced by coercion ## [1] 3 Dont confuse with nan (not a number) which is used for things like imaginary numbers (explore the help for more on this) is.na(NA) ## [1] TRUE is.nan(NA) ## [1] FALSE is.na(as.numeric(&#39;&#39;)) ## [1] TRUE is.nan(as.numeric(&#39;&#39;)) ## [1] FALSE i &lt;- sqrt(-1) ## Warning in sqrt(-1): NaNs produced is.na(i) # interestingly nan is also na ## [1] TRUE is.nan(i) ## [1] TRUE 2.6.1.2 Sequences An easy way to make a vector from a sequence of values. The following 3 examples are equivalent: seq(1,10) c(1:10) c(1,2,3,4,5,6,7,8,9,10) The seq() function has special uses like using a step parameter: seq(2,10,2) ## [1] 2 4 6 8 10 2.6.1.3 Vectorization and vector arithmetic Arithmetic on vectors operates element-wise elev &lt;- c(52,394,510,564,725,848,1042,1225,1486,1775,1899,2551) elevft &lt;- elev / 0.3048 elevft ## [1] 170.6037 1292.6509 1673.2283 1850.3937 2378.6089 2782.1522 3418.6352 4019.0289 ## [9] 4875.3281 5823.4908 6230.3150 8369.4226 Another example, with 2 vectors: temp03 &lt;- c(13.1,11.4,9.4,10.9,8.9,8.4,6.7,7.6,2.8,1.6,1.2,-2.1) temp02 &lt;- c(10.7,9.7,7.7,9.2,7.3,6.7,4.0,5.0,0.9,-1.1,-0.8,-4.4) tempdiff &lt;- temp03 - temp02 tempdiff ## [1] 2.4 1.7 1.7 1.7 1.6 1.7 2.7 2.6 1.9 2.7 2.0 2.3 2.6.1.4 Plotting vectors Vectors of Feb temperature, elevation and latitude at stations in the Sierra: temp &lt;- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7, 4.0, 5.0, 0.9, -1.1, -0.8, -4.4) elev &lt;- c(52, 394, 510, 564, 725, 848, 1042, 1225, 1486, 1775, 1899, 2551) lat &lt;- c(39.52, 38.91, 37.97, 38.70, 39.09, 39.25, 39.94, 37.75, 40.35, 39.33, 39.17, 38.21) Plot individually plot(temp) Figure 2.1: Temperature plot(elev) Figure 2.2: Elevation plot(lat) Figure 2.3: Latitude Then plot as a scatterplot plot(elev,temp) Figure 2.4: Temperature~Elevation 2.6.1.5 Named indices Vector indices can be named. codes &lt;- c(380, 124, 818) codes ## [1] 380 124 818 codes &lt;- c(italy = 380, canada = 124, egypt = 818) codes ## italy canada egypt ## 380 124 818 str(codes) ## Named num [1:3] 380 124 818 ## - attr(*, &quot;names&quot;)= chr [1:3] &quot;italy&quot; &quot;canada&quot; &quot;egypt&quot; Why? I guess so you can refer to observations by name instead of index. The following are equivalent: codes[1] ## italy ## 380 codes[&quot;italy&quot;] ## italy ## 380 2.6.2 Lists Lists can be heterogeneous, with multiple class types. Lists are actually used a lot in R, but we wont see them for a while. 2.6.3 Matrices Vectors are commonly used as a column in a matrix (or as well see, a data frame), like a variable temp &lt;- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7, 4.0, 5.0, 0.9, -1.1, -0.8, -4.4) elev &lt;- c(52, 394, 510, 564, 725, 848, 1042, 1225, 1486, 1775, 1899, 2551) lat &lt;- c(39.52, 38.91, 37.97, 38.70, 39.09, 39.25, 39.94, 37.75, 40.35, 39.33, 39.17, 38.21) Building a matrix from vectors as columns sierradata &lt;- cbind(temp, elev, lat) class(sierradata) ## [1] &quot;matrix&quot; &quot;array&quot; 2.6.3.1 Dimensions for arrays and matrices Note: a matrix is just a 2D array. Arrays have 1, 3, or more dimensions. dim(sierradata) ## [1] 12 3 a &lt;- 1:12 dim(a) &lt;- c(3, 4) # matrix class(a) ## [1] &quot;matrix&quot; &quot;array&quot; dim(a) &lt;- c(2,3,2) # 3D array class(a) ## [1] &quot;array&quot; dim(a) &lt;- 12 # 1D array class(a) ## [1] &quot;array&quot; b &lt;- matrix(1:12, ncol=1) # 1 column matrix is allowed 2.6.4 Data frames A data frame is a database with fields (as vectors) with records (rows), so is very important for data analysis and GIS. Theyre kind of like a spreadsheet with rules (first row is field names, fields all one type). So even though theyre more complex than a list, we use them so frequently they become quite familiar [whereas I continue to find lists confusing, especially when discovering them as what a particular function returns.] library(palmerpenguins) data(package = &#39;palmerpenguins&#39;) head(penguins) ## # A tibble: 6 x 8 ## species island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex year ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 Adelie Torgers~ 39.1 18.7 181 3750 male 2007 ## 2 Adelie Torgers~ 39.5 17.4 186 3800 fema~ 2007 ## 3 Adelie Torgers~ 40.3 18 195 3250 fema~ 2007 ## 4 Adelie Torgers~ NA NA NA NA &lt;NA&gt; 2007 ## 5 Adelie Torgers~ 36.7 19.3 193 3450 fema~ 2007 ## 6 Adelie Torgers~ 39.3 20.6 190 3650 male 2007 Creating a data frame out of a matrix mydata &lt;- as.data.frame(sierradata) plot(data = mydata, x = elev, y = temp) ## Warning in plot.window(...): &quot;data&quot; is not a graphical parameter ## Warning in plot.xy(xy, type, ...): &quot;data&quot; is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;data&quot; is not a graphical ## parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;data&quot; is not a graphical ## parameter ## Warning in box(...): &quot;data&quot; is not a graphical parameter ## Warning in title(...): &quot;data&quot; is not a graphical parameter Figure 2.5: Temperature~Elevation Read a data frame from a CSV Well be looking at this more in the next chapter, but a common need is to read data from a spreadsheet stored in the CSV format. Normally, youd have that stored with your project and can just specify the file name, but well access CSVs from the iGIScData package. Since you have this installed, it will already be on your computer, but not in your project folder. The path to it can be derived using the system.file() function. Reading a csv in readr (part of the tidyverse that well be looking at in the next chapter) is done with read_csv(): library(readr) csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_1987_BaySites.csv&quot;, package=&quot;iGIScData&quot;) TRI87 &lt;- read_csv(csvPath) ## Parsed with column specification: ## cols( ## TRI_FACILITY_ID = col_character(), ## count = col_double(), ## FACILITY_NAME = col_character(), ## COUNTY = col_character(), ## air_releases = col_double(), ## fugitive_air = col_double(), ## stack_air = col_double(), ## LATITUDE = col_double(), ## LONGITUDE = col_double() ## ) TRI87 ## # A tibble: 335 x 9 ## TRI_FACILITY_ID count FACILITY_NAME COUNTY air_releases fugitive_air stack_air ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 91002FRMND585BR 2 FORUM IND SAN M~ 1423 1423 0 ## 2 92052ZPMNF2970C 1 ZEP MFG CO SANTA~ 337 337 0 ## 3 93117TLDYN3165P 2 TELEDYNE MEC SANTA~ 12600 12600 0 ## 4 94002GTWSG477HA 2 MORGAN ADVAN~ SAN M~ 18700 18700 0 ## 5 94002SMPRD120SE 2 SEM PRODS INC SAN M~ 1500 500 1000 ## 6 94025HBLNN151CO 2 HEUBLEIN INC SAN M~ 500 0 500 ## 7 94025RYCHM300CO 10 TE CONNECTIV~ SAN M~ 144871 47562 97309 ## 8 94025SNFRD990OB 1 SANFORD META~ SAN M~ 9675 9675 0 ## 9 94026BYPCK3575H 2 BAY PACKAGIN~ SAN M~ 80000 32000 48000 ## 10 94026CDRSY3475E 2 CDR SYS CORP SAN M~ 126800 0 126800 ## # ... with 325 more rows, and 2 more variables: LATITUDE &lt;dbl&gt;, LONGITUDE &lt;dbl&gt; Sort, Index, &amp; Max/Min head(sort(TRI87$air_releases)) ## [1] 2 5 5 7 9 10 index &lt;- order(TRI87$air_releases) head(TRI87$FACILITY_NAME[index]) # displays facilities in order of their air releases ## [1] &quot;AIR PRODUCTS MANUFACTURING CORP&quot; ## [2] &quot;UNITED FIBERS&quot; ## [3] &quot;CLOROX MANUFACTURING CO&quot; ## [4] &quot;ICI AMERICAS INC WESTERN RESEARCH CENTER&quot; ## [5] &quot;UNION CARBIDE CORP&quot; ## [6] &quot;SCOTTS-SIERRA HORTICULTURAL PRODS CO INC&quot; i_max &lt;- which.max(TRI87$air_releases) TRI87$FACILITY_NAME[i_max] # was NUMMI at the time ## [1] &quot;TESLA INC&quot; 2.6.5 Factors Factors are vectors with predefined values - Normally used for categorical data. - Built on an integer vector - Levels are the set of predefined values. fruit &lt;- factor(c(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;, &quot;banana&quot;)) fruit # note that levels will be in alphabetical order ## [1] apple banana orange banana ## Levels: apple banana orange class(fruit) ## [1] &quot;factor&quot; typeof(fruit) ## [1] &quot;integer&quot; An equivalent conversion: fruitint &lt;- c(1, 2, 3, 2) # equivalent conversion fruit &lt;- factor(fruitint, labels = c(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;)) str(fruit) ## Factor w/ 3 levels &quot;apple&quot;,&quot;banana&quot;,..: 1 2 3 2 2.6.5.1 Categorical Data and Factors While character data might be seen as categorical (e.g. urban, agricultural, forest land covers), to be used as categorical variables they must be made into factors. grain_order &lt;- c(&quot;clay&quot;, &quot;silt&quot;, &quot;sand&quot;) grain_char &lt;- sample(grain_order, 36, replace = TRUE) grain_fact &lt;- factor(grain_char, levels = grain_order) grain_char ## [1] &quot;silt&quot; &quot;sand&quot; &quot;silt&quot; &quot;sand&quot; &quot;clay&quot; &quot;silt&quot; &quot;sand&quot; &quot;clay&quot; &quot;sand&quot; &quot;clay&quot; &quot;clay&quot; &quot;silt&quot; ## [13] &quot;clay&quot; &quot;silt&quot; &quot;clay&quot; &quot;sand&quot; &quot;clay&quot; &quot;clay&quot; &quot;sand&quot; &quot;clay&quot; &quot;sand&quot; &quot;clay&quot; &quot;sand&quot; &quot;clay&quot; ## [25] &quot;clay&quot; &quot;clay&quot; &quot;clay&quot; &quot;sand&quot; &quot;clay&quot; &quot;silt&quot; &quot;sand&quot; &quot;sand&quot; &quot;sand&quot; &quot;sand&quot; &quot;sand&quot; &quot;sand&quot; grain_fact ## [1] silt sand silt sand clay silt sand clay sand clay clay silt clay silt clay sand ## [17] clay clay sand clay sand clay sand clay clay clay clay sand clay silt sand sand ## [33] sand sand sand sand ## Levels: clay silt sand To make a categorical variable a factor: fruit &lt;- c(&quot;apples&quot;, &quot;oranges&quot;, &quot;bananas&quot;, &quot;oranges&quot;) farm &lt;- c(&quot;organic&quot;, &quot;conventional&quot;, &quot;organic&quot;, &quot;organic&quot;) ag &lt;- as.data.frame(cbind(fruit, farm)) ag$fruit &lt;- factor(ag$fruit) ag$fruit ## [1] apples oranges bananas oranges ## Levels: apples bananas oranges Factor example sierraFeb$COUNTY &lt;- factor(sierraFeb$COUNTY) str(sierraFeb$COUNTY) ## Factor w/ 21 levels &quot;Amador&quot;,&quot;Butte&quot;,..: 20 14 7 12 12 2 19 11 2 19 ... 2.7 Programming and Logic Given the exploratory nature of the R language, we sometimes forget that it provides significant capabilities as a programming language where we can solve more complex problems by coding procedures and using logic to control the process and handle a range of possible scenarios. Programming languages are used for a wide range of purposes, from developing operating systems built from low-level code to high-level scripting used to run existing functions in libraries. R and Python are commonly used for scripting, and you may be familiar with using arcpy to script ArcGIS geoprocessing tools. But whether low- or high-level, some common operational structures are used in all computer programming languages: Conditional operations: If a condition is true, do this, and maybe otherwise do something else. if x!=0 {print(1/x)} else {print(\"Can't divide by 0\")} Loops for(i in 1:10) print(paste(i, 1/i)) Functions (defining your own then using it in your main script) turnright &lt;- function(ang){(ang + 90) %% 360} turnright(c(260, 270, 280)) ## [1] 350 0 10 Free-standing scripts As we move forward, well be wanting to develop complete, free-standing scripts that have all of the needed libraries and data. Your scripts should stand on their own. One example of this that may seem insignificant is using print() statements instead of just naming the object or variable in the console. While that is common in exploratory work, we need to learn to create free-standing scripts. However, free standing still allows for loading libraries of functions well be using. Were still talking about high-level (scripting), not low-level programming, so we can depend on those libraries that any user can access by installing those packages. If we develop our own packages, we just need to provide the user the ability to install those packages. 2.7.1 Subsetting with logic Well use a package that includes data from Irizarry, Rafael (2020) Introduction to Data Science section 2.13.1. Identify all states with murder rates  that of Italy. library(dslabs) data(murders) murder_rate &lt;- murders$total / murders$population * 100000 i &lt;- murder_rate &lt;= 0.71 murders$abb[i] ## [1] &quot;HI&quot; &quot;IA&quot; &quot;NH&quot; &quot;ND&quot; &quot;VT&quot; which library(readr) TRI87 &lt;- read_csv(&quot;data/TRI_1987_BaySites.csv&quot;) ## Parsed with column specification: ## cols( ## TRI_FACILITY_ID = col_character(), ## count = col_double(), ## FACILITY_NAME = col_character(), ## COUNTY = col_character(), ## air_releases = col_double(), ## fugitive_air = col_double(), ## stack_air = col_double(), ## LATITUDE = col_double(), ## LONGITUDE = col_double() ## ) i &lt;- which(TRI87$air_releases &gt; 1e6) TRI87$FACILITY_NAME[i] ## [1] &quot;VALERO REFINING CO-CALI FORNIA BENICIA REFINERY&quot; ## [2] &quot;TESLA INC&quot; ## [3] &quot;TESORO REFINING &amp; MARKETING CO LLC&quot; ## [4] &quot;HGST INC&quot; %in% library(readr) csvPath = system.file(&quot;extdata&quot;,&quot;TRI_1987_BaySites.csv&quot;, package=&quot;iGIScData&quot;) TRI87 &lt;- read_csv(csvPath) ## Parsed with column specification: ## cols( ## TRI_FACILITY_ID = col_character(), ## count = col_double(), ## FACILITY_NAME = col_character(), ## COUNTY = col_character(), ## air_releases = col_double(), ## fugitive_air = col_double(), ## stack_air = col_double(), ## LATITUDE = col_double(), ## LONGITUDE = col_double() ## ) i &lt;- TRI87$COUNTY %in% c(&quot;NAPA&quot;,&quot;SONOMA&quot;) TRI87$FACILITY_NAME[i] ## [1] &quot;SAWYER OF NAPA&quot; ## [2] &quot;BERINGER VINEYARDS&quot; ## [3] &quot;CAL-WOOD DOOR INC&quot; ## [4] &quot;SOLA OPTICAL USA INC&quot; ## [5] &quot;KEYSIGHT TECHNOLOGIES INC&quot; ## [6] &quot;SANTA ROSA STAINLESS STEEL&quot; ## [7] &quot;OPTICAL COATING LABORATORY INC&quot; ## [8] &quot;MGM BRAKES&quot; ## [9] &quot;SEBASTIANI VINEYARDS INC, SONOMA CASK CELLARS&quot; 2.7.2 Apply functions There are many apply functions in R, and they largely obviate the need for looping. For instance: apply derives values at margins of rows and columns, e.g. to sum across rows or down columns # matrix apply  the same would apply to data frames matrix12 &lt;- 1:12 dim(matrix12) &lt;- c(3,4) rowsums &lt;- apply(matrix12, 1, sum) colsums &lt;- apply(matrix12, 2, sum) sum(rowsums) ## [1] 78 sum(colsums) ## [1] 78 zero &lt;- sum(rowsums) - sum(colsums) matrix12 ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Apply functions satisfy one of the needs that spreadsheets are used for. Consider how of ten you use sum, mean or similar functions in Excel. sapply sapply applies functions to either: all elements of a vector  unary functions only sapply(1:12, sqrt) ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 3.000000 ## [10] 3.162278 3.316625 3.464102 or all variables of a data frame (not a matrix), where it works much like a column-based apply (since variables are columns) but more easily interpreted without the need of specifying columns with 2: sapply(cars,mean) # same as apply(cars,2,mean) ## speed dist ## 15.40 42.98 temp02 &lt;- c(10.7,9.7,7.7,9.2,7.3,6.7,4.0,5.0,0.9,-1.1,-0.8,-4.4) temp03 &lt;- c(13.1,11.4,9.4,10.9,8.9,8.4,6.7,7.6,2.8,1.6,1.2,-2.1) sapply(as.data.frame(cbind(temp02,temp03)),mean) # has to be a data frame ## temp02 temp03 ## 4.575000 6.658333 While various apply functions are in base R, the purrr package takes these further. See: purrr cheat sheet 2.8 Exercises Assign variables for your name, city, state and zip code, and use paste() to combine them, and assign them to the variable me. What is the class of me? Knowing that trigonometric functions require angles (including azimuth directions) to be provided in radians, and that degrees can be converted into radians by dividing by 180 and multiplying that by pi, derive the sine of 30 degrees with an R expression. (Base R knows what pi is, so you can just use pi) If two sides of a right triangle on a map can be represented as \\(dX\\) and \\(dY\\) and the direct line path between them \\(c\\), and the coordinates of 2 points on a map might be given as \\((x1,y1)\\) and \\((x2,y2)\\), with \\(dX=x2-x1\\) and \\(dY=y2-y1\\), use the Pythagorean theorem to derive the distance between them and assign that expression to \\(c\\). You can create a vector uniform random numbers from 0 to 1 using runif(n=30) where n=30 says to make 30 of them. Use the round() function to round each of the values, and provide what you created and explain what happened. Create two vectors of 10 numbers each with the c() function, then assigning to x and y. Then plot(x,y), and provide the three lines of code you used to do the assignment and plot. Change your code from #5 so that one value is NA (entered simply as NA, no quotation marks), and derive the mean value for x. Then add ,na.rm=T to the parameters for mean(). Also do this for y. Describe your results and explain what happens. Create two sequences, a and b, with a all odd numbers from 1 to 99, b all even numbers from 2 to 100. Then derive c through vector division of b/a. Plot a and c together as a scatterplot. Build the sierradata data frame from the data at the top of the Matrices section, also given here: temp &lt;- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7, 4.0, 5.0, 0.9, -1.1, -0.8, -4.4) elev &lt;- c(52, 394, 510, 564, 725, 848, 1042, 1225, 1486, 1775, 1899, 2551) lat &lt;- c(39.52, 38.91, 37.97, 38.70, 39.09, 39.25, 39.94, 37.75, 40.35, 39.33, 39.17, 38.21) Create a data frame from it using the same steps, and plot temp against latitude. From the sierradata matrix built with cbind(), derive colmeans using the mean parameter on the columns 2 for apply(). Do the same thing with the sierra data data frame with sapply(). "],["introduction-to-the-tidyverse.html", "Chapter 3 Introduction to the tidyverse 3.1 Background: Exploratory Data Analysis 3.2 The Tidyverse and what well explore in this chapter 3.3 Tibbles 3.4 Statistical summary of variables 3.5 Visualizing data with a Tukey box plot 3.6 Database operations with dplyr 3.7 The dot operator 3.8 Exercises", " Chapter 3 Introduction to the tidyverse At this point, weve learned the basics of working with the R language. From here well want to explore how to analyze data, both statistically and spatially. Were going to use an exploratory approach with significant application of visualization both in terms of graphs as well as maps. So lets start by exploring this exploratory approach 3.1 Background: Exploratory Data Analysis In 1961, John Tukey proposed a new approach to data analysis, defining it as Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. He followed this up in 1977 with Exploratory Data Analysis. Exploratory data analysis (EDA) in part as an approach to analyzing data via summaries and graphics. The key word is exploratory, in contrast with confirmatory statistics. Both are important, but ignoring exploration is ignoring enlightenment. Some purposes of EDA are: to suggest hypotheses to assess assumptions on which inference will be based to select appropriate inferential statistical tools to guide further data collection These concepts led to the development of S at Bell Labs (John Chambers, 1976), then R, built on clear design and extensive, clear graphics. 3.2 The Tidyverse and what well explore in this chapter The Tidyverse refers to a suite of R packages developed at RStudio (see R Studio and R for Data Science) (figure from Grolemund &amp; Wickham 2017) for facilitating data processing and analysis. While R itself is designed around EDA, the Tidyverse takes it further. Some of the packages in the Tidyverse that are widely used are: dplyr : data manipulation like a database readr : better methods for reading and writing rectangular data tidyr : reorganization methods that extend dplyrs database capabilities purrr : expanded programming toolkit including enhanced apply methods tibble : improved data frame stringr : string manipulation library ggplot2 : graphing system based on the grammar of graphics In this chapter, well be mostly exploring dplyr, with a few other things thrown in like reading data frames with readr. For simplicity, we can just include library(tidyverse) to get everything. 3.3 Tibbles Tibbles are an improved type of data frame part of the Tidyverse serve the same purpose as a data frame, and all data frame operations work Advantages display better can be composed of more complex objects like lists, etc. can be grouped How created Reading from a CSV, using one of a variety of Tidyverse functions similarly named to base functions: read_csv creates a tibble (in general, underscores are used in the Tidyverse) read.csv creates a regular data frame You can also use the tibble() function library(tidyverse) # includes readr, ggplot2, and dplyr which we&#39;ll use in this chapter library(iGIScData) csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_1987_BaySites.csv&quot;, package=&quot;iGIScData&quot;) TRI87 &lt;- read_csv(csvPath) ## Parsed with column specification: ## cols( ## TRI_FACILITY_ID = col_character(), ## count = col_double(), ## FACILITY_NAME = col_character(), ## COUNTY = col_character(), ## air_releases = col_double(), ## fugitive_air = col_double(), ## stack_air = col_double(), ## LATITUDE = col_double(), ## LONGITUDE = col_double() ## ) a &lt;- rnorm(10) b &lt;- runif(10) ab &lt;- tibble(a,b) ab ## # A tibble: 10 x 2 ## a b ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.327 0.219 ## 2 -0.238 0.0608 ## 3 -0.368 0.608 ## 4 -0.719 0.964 ## 5 -1.15 0.409 ## 6 -0.840 0.876 ## 7 0.888 0.325 ## 8 0.0396 0.486 ## 9 -1.41 0.959 ## 10 0.652 0.177 3.3.1 read_csv vs. read.csv You might be tempted to use read.csv from base R They look a lot alike, so you might confuse them You dont need to load library(readr) read.csv fixes some things and that might be desired: problematic field names like MLY-TAVG-NORMAL become MLY.TAVG.NORMAL numbers stored as characters are converted to numbers 01 becomes 1, 02 becomes 2, etc. However, there are potential problems You may not want some of those changes, and want to specify those changes separately There are known problems that read_csv avoids Recommendation: Use read_csv and write_csv. 3.4 Statistical summary of variables A simple statistical summary is very easy to do: summary(eucoakrainfallrunoffTDR) ## site site # date month ## Length:90 Min. :1.000 Length:90 Length:90 ## Class :character 1st Qu.:2.000 Class :character Class :character ## Mode :character Median :4.000 Mode :character Mode :character ## Mean :4.422 ## 3rd Qu.:6.000 ## Max. :8.000 ## ## rain_mm rain_oak rain_euc runoffL_oak runoffL_euc ## Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 0.000 Min. : 0.00 ## 1st Qu.:16.00 1st Qu.:16.00 1st Qu.:14.75 1st Qu.: 0.000 1st Qu.: 0.07 ## Median :28.50 Median :30.50 Median :30.00 Median : 0.450 Median : 1.20 ## Mean :37.99 Mean :35.08 Mean :34.60 Mean : 2.032 Mean : 2.45 ## 3rd Qu.:63.25 3rd Qu.:50.50 3rd Qu.:50.00 3rd Qu.: 2.800 3rd Qu.: 3.30 ## Max. :99.00 Max. :98.00 Max. :96.00 Max. :14.000 Max. :16.00 ## NA&#39;s :18 NA&#39;s :2 NA&#39;s :2 NA&#39;s :5 NA&#39;s :3 ## slope_oak slope_euc aspect_oak aspect_euc surface_tension_oak ## Min. : 9.00 Min. : 9.00 Min. :100.0 Min. :106.0 Min. :37.40 ## 1st Qu.:12.00 1st Qu.:12.00 1st Qu.:143.0 1st Qu.:175.0 1st Qu.:72.75 ## Median :24.50 Median :23.00 Median :189.0 Median :196.5 Median :72.75 ## Mean :21.62 Mean :19.34 Mean :181.9 Mean :191.2 Mean :68.35 ## 3rd Qu.:30.50 3rd Qu.:25.00 3rd Qu.:220.0 3rd Qu.:224.0 3rd Qu.:72.75 ## Max. :32.00 Max. :31.00 Max. :264.0 Max. :296.0 Max. :72.75 ## NA&#39;s :22 ## surface_tension_euc runoff_rainfall_ratio_oak runoff_rainfall_ratio_euc ## Min. :28.51 Min. :0.00000 Min. :0.000000 ## 1st Qu.:32.79 1st Qu.:0.00000 1st Qu.:0.003027 ## Median :37.40 Median :0.02046 Median :0.047619 ## Mean :43.11 Mean :0.05357 Mean :0.065902 ## 3rd Qu.:56.41 3rd Qu.:0.08485 3rd Qu.:0.083603 ## Max. :72.75 Max. :0.42000 Max. :0.335652 ## NA&#39;s :22 NA&#39;s :5 NA&#39;s :3 3.5 Visualizing data with a Tukey box plot ggplot(data = eucoakrainfallrunoffTDR) + geom_boxplot(mapping = aes(x=site, y=runoffL_euc)) ## Warning: Removed 3 rows containing non-finite values (stat_boxplot). 3.6 Database operations with dplyr As part of exploring our data, well typically simplify or reduce it for our purposes. The following methods are quickly discovered to be essential as part of exploring and analyzing data. select rows using logic, such as population &gt; 10000, with filter select variable columns you want to retain with select add new variables and assign their values with mutate sort rows based on a a field with arrange summarize by group 3.6.1 Select, mutate, and the pipe The pipe %&gt;%: Read %&gt;% as and then This is bigger than it sounds and opens up a lot of possibilities. See example below, and observe how the expression becomes several lines long. In the process, well see examples of new variables with mutate and selecting (and in the process ordering) variables: runoff &lt;- eucoakrainfallrunoffTDR %&gt;% mutate(Date = as.Date(date,&quot;%m/%d/%Y&quot;), rain_subcanopy = (rain_oak + rain_euc)/2) %&gt;% select(site, Date, rain_mm, rain_subcanopy, runoffL_oak, runoffL_euc, slope_oak, slope_euc) runoff ## # A tibble: 90 x 8 ## site Date rain_mm rain_subcanopy runoffL_oak runoffL_euc slope_oak slope_euc ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AB1 2006-11-08 29 29 4.79 6.7 32 31 ## 2 AB1 2006-11-12 22 18.5 3.2 4.3 32 31 ## 3 AB1 2006-11-29 85 65 9.7 16 32 31 ## 4 AB1 2006-12-12 82 87.5 14 14.2 32 31 ## 5 AB1 2006-12-28 43 54 9.75 4.33 32 31 ## 6 AB1 2007-01-29 7 54 1.4 0 32 31 ## 7 AB1 2007-02-09 56 44 10.1 0 32 31 ## 8 AB1 2007-02-13 63 42.5 3.90 1.40 32 31 ## 9 AB1 2007-02-28 NA 56 4.75 8.65 32 31 ## 10 AB1 2007-03-22 NA 2 NA 0.11 32 31 ## # ... with 80 more rows Note: to just rename a variable, use rename instead of mutate. It will stay in position. 3.6.2 filter filter lets you select observations that meet criteria, similar to an SQL WHERE clause. runoff2007 &lt;- runoff %&gt;% filter(Date &gt;= as.Date(&quot;01/01/2007&quot;, &quot;%m/%d/%Y&quot;)) runoff2007 ## # A tibble: 51 x 8 ## site Date rain_mm rain_subcanopy runoffL_oak runoffL_euc slope_oak slope_euc ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AB1 2007-01-29 7 54 1.4 0 32 31 ## 2 AB1 2007-02-09 56 44 10.1 0 32 31 ## 3 AB1 2007-02-13 63 42.5 3.90 1.40 32 31 ## 4 AB1 2007-02-28 NA 56 4.75 8.65 32 31 ## 5 AB1 2007-03-22 NA 2 NA 0.11 32 31 ## 6 AB1 2007-04-23 NA 33.5 6.94 9.20 32 31 ## 7 AB1 2007-05-05 NA 31 6.34 7.43 32 31 ## 8 AB2 2007-01-29 4 3.5 1.26 0.05 24 25 ## 9 AB2 2007-02-09 37 41.5 6.3 3.3 24 25 ## 10 AB2 2007-02-13 43 49.5 6.78 1.14 24 25 ## # ... with 41 more rows Filtering out NA with !is.na Heres an important one. There are many times you need to avoid NAs. We commonly see summary statistics using na.rm = TRUE in order to ignore NAs when calculating a statistic like mean. To simply filter out NAs from a vector or a variable use a filter: feb_filt &lt;- feb_s %&gt;% filter(!is.na(TEMP)) 3.6.3 Writing a data frame to a csv Lets say you have created a data frame, maybe with read_csv runoff20062007 &lt;- read_csv(csvPath) Then you do some processing to change it, maybe adding variables, reorganizing, etc., and you want to write out your new eucoak, so you just need to use write_csv write_csv(eucoak, \"data/tidy_eucoak.csv\") 3.6.4 Summarize by group Youll find that you need to use this all the time with real data. You have a bunch of data where some categorical variable is defining a grouping, like our site field in the eucoak data. Wed like to just create average slope, rainfall, and runoff for each site. Note that it involves two steps, first defining which field defines the group, then the various summary statistics wed like to store. In this case all of the slopes under oak remain the same for a given site  its a site characteristic  and the same applies to the euc site, so we can just grab the first value (mean would have also worked of course). eucoakSiteAvg &lt;- runoff %&gt;% group_by(site) %&gt;% summarize( rain = mean(rain_mm, na.rm = TRUE), rain_subcanopy = mean(rain_subcanopy, na.rm = TRUE), runoffL_oak = mean(runoffL_oak, na.rm = TRUE), runoffL_euc = mean(runoffL_euc, na.rm = TRUE), slope_oak = first(slope_oak), slope_euc = first(slope_euc) ) ## `summarise()` ungrouping output (override with `.groups` argument) eucoakSiteAvg ## # A tibble: 8 x 7 ## site rain rain_subcanopy runoffL_oak runoffL_euc slope_oak slope_euc ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AB1 48.4 43.1 6.80 6.03 32 31 ## 2 AB2 34.1 35.4 4.91 3.65 24 25 ## 3 KM1 48 36.1 1.94 0.592 30.5 25 ## 4 PR1 56.5 37.6 0.459 2.31 27 23 ## 5 TP1 38.4 30.0 0.877 1.66 9 9 ## 6 TP2 34.3 32.9 0.0955 1.53 12 10 ## 7 TP3 32.1 27.8 0.381 0.815 25 18 ## 8 TP4 32.5 35.7 0.231 2.83 12 12 Summarizing by group with TRI data csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_2017_CA.csv&quot;, package=&quot;iGIScData&quot;) TRI_BySite &lt;- read_csv(csvPath) %&gt;% mutate(all_air = `5.1_FUGITIVE_AIR` + `5.2_STACK_AIR`) %&gt;% filter(all_air &gt; 0) %&gt;% group_by(FACILITY_NAME) %&gt;% summarize( FACILITY_NAME = first(FACILITY_NAME), air_releases = sum(all_air, na.rm = TRUE), mean_fugitive = mean(`5.1_FUGITIVE_AIR`, na.rm = TRUE), LATITUDE = first(LATITUDE), LONGITUDE = first(LONGITUDE)) ## Warning: Missing column names filled in: &#39;X110&#39; [110] ## Warning: 3807 parsing failures. ## row col expected actual file ## 1 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## 2 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## 3 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## 4 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## 5 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## ... ... ........... ........... .................................................................................. ## See problems(...) for more details. 3.6.5 Count Count is a simple variant on summarize by group, since the only statistic is the count of events. tidy_eucoak %&gt;% count(tree) ## # A tibble: 2 x 2 ## tree n ## &lt;chr&gt; &lt;int&gt; ## 1 euc 90 ## 2 oak 90 Another way is to use n(): tidy_eucoak %&gt;% group_by(tree) %&gt;% summarize(n = n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## tree n ## &lt;chr&gt; &lt;int&gt; ## 1 euc 90 ## 2 oak 90 3.6.6 Sorting after summarizing Using the marine debris data from NOAA Marine Debris Programs Marine Debris Monitoring and Assessment Project shorelineLatLong &lt;- ConcentrationReport %&gt;% group_by(`Shoreline Name`) %&gt;% summarize( latitude = mean((`Latitude Start`+`Latitude End`)/2), longitude = mean((`Longitude Start`+`Longitude End`)/2) ) %&gt;% arrange(latitude) ## `summarise()` ungrouping output (override with `.groups` argument) shorelineLatLong ## # A tibble: 38 x 3 ## `Shoreline Name` latitude longitude ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aimee Arvidson 33.6 -118. ## 2 Balboa Pier #2 33.6 -118. ## 3 Bolsa Chica 33.7 -118. ## 4 Junipero Beach 33.8 -118. ## 5 Malaga Cove 33.8 -118. ## 6 Zuma Beach, Malibu 34.0 -119. ## 7 Zuma Beach 34.0 -119. ## 8 Will Rodgers 34.0 -119. ## 9 Carbon Beach 34.0 -119. ## 10 Nicholas Canyon 34.0 -119. ## # ... with 28 more rows 3.7 The dot operator The dot . operator derives from UNIX syntax, and refers to here. For accessing files in the current folder, the path is ./filename A similar specification is used in piped sequences The advantage of the pipe is you dont have to keep referencing the data frame. The dot is then used to connect to items inside the data frame: csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_1987_BaySites.csv&quot;, package=&quot;iGIScData&quot;) TRI87 &lt;- read_csv(csvPath) ## Parsed with column specification: ## cols( ## TRI_FACILITY_ID = col_character(), ## count = col_double(), ## FACILITY_NAME = col_character(), ## COUNTY = col_character(), ## air_releases = col_double(), ## fugitive_air = col_double(), ## stack_air = col_double(), ## LATITUDE = col_double(), ## LONGITUDE = col_double() ## ) stackrate &lt;- TRI87 %&gt;% mutate(stackrate = stack_air/air_releases) %&gt;% .$stackrate head(stackrate) ## [1] 0.0000000 0.0000000 0.0000000 0.0000000 0.6666667 1.0000000 3.8 Exercises Create a tibble with 20 rows of two variables norm and unif with norm created with rnorm() and unif created with runif(). Read in TRI_2017_CA.csv in two ways, as a normal data frame assigned to df and as a tibble assigned to tb. What field names result for whats listed in the CSV as 5.1_FUGITIVE_AIR? Use the summary function to investigate the variables in either the data.frame or tibble you just created. What type of field and what values are assigned to BIA_CODE? Create a boxplot of body_mass_g by species from the penguins data frame in the palmerpenguins package. Access the data with data(package = palmerpenguins), and also remember library(ggplot2) or library(tidyverse). Use select, mutate, and the pipe to create a penguinMass tibble where the only original variable retained is species, but with body_mass_kg created as \\(\\frac{1}{1000}\\) the body_mass_g. The statement should start with penguinMass &lt;- penguins and use a pipe plus the other functions after that. Now, also with penguins, create FemaleChinstaps to include only the female Chinstrap penguins. Start with FemaleChinstraps &lt;- penguins %&gt;% Now, summarize by species groups to create mean and standard deviation variables from bill_length_mm, bill_depth_mm, flipper_length_mm, and body_mass_g. Preface the variable names with either avg. or sd. Include na.rm=T with all statistics function calls. Create an penguinSort tibble, sorted by body_mass_g. "],["visualization.html", "Chapter 4 Visualization 4.1 ggplot2 4.2 Plotting one variable 4.3 Plotting two variables 4.4 Color systems 4.5 Titles and subtitles 4.6 Pairs Plot 4.7 Exercises", " Chapter 4 Visualization In this section well explore visualization methods in R. Visualization has been a key element of R since its inception, since visualization is central to the exploratory philosophy of the language. The base plot system generally does a good job in coming up with the most likely graphical output based on the data you provide. plot(penguins$body_mass_g, penguins$flipper_length_mm) Figure 4.1: Flipper length by species plot(penguins$species, penguins$flipper_length_mm) Figure 4.2: Flipper length by species 4.1 ggplot2 Well mostly focus however on gpplot2, based on the Grammar of Graphics because it provides considerable control over your graphics while remaining fairly easily readable, as long as you buy into its grammar. ggplot2 looks at three aspects of a graph: data : where are the data coming from? geometry : what type of graph are we creating? aesthetics : what choices can we make about symbology and how do we connect symbology to data? See https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf The ggplot2 system provides plots of single and multiple variables, using various coordinate systems (including geographic). 4.2 Plotting one variable continuous histograms density plots dot plots discrete bar library(iGIScData) library(tidyverse) summary(XSptsNDVI) ## DistNtoS elevation vegetation geometry NDVIgrowing ## Min. : 0.0 Min. :1510 Length:29 Length:29 Min. :0.3255 ## 1st Qu.: 37.0 1st Qu.:1510 Class :character Class :character 1st Qu.:0.5052 ## Median :175.0 Median :1511 Mode :character Mode :character Median :0.6169 ## Mean :164.7 Mean :1511 Mean :0.5901 ## 3rd Qu.:275.5 3rd Qu.:1511 3rd Qu.:0.6768 ## Max. :298.8 Max. :1511 Max. :0.7683 ## NDVIsenescent ## Min. :0.1402 ## 1st Qu.:0.2418 ## Median :0.2817 ## Mean :0.3662 ## 3rd Qu.:0.5407 ## Max. :0.7578 ggplot(XSptsNDVI, aes(vegetation)) + geom_bar() 4.2.1 Histogram First, to prepare the data, we need to use a pivot_longer on XSptsNDVI: XSptsPheno &lt;- XSptsNDVI %&gt;% filter(vegetation != &quot;pine&quot;) %&gt;% pivot_longer(cols = starts_with(&quot;NDVI&quot;), names_to = &quot;phenology&quot;, values_to = &quot;NDVI&quot;) %&gt;% mutate(phenology = str_sub(phenology, 5, str_length(phenology))) XSptsPheno &lt;- read_csv(&quot;data/XSptsPheno.csv&quot;) ## Parsed with column specification: ## cols( ## DistNtoS = col_double(), ## elevation = col_double(), ## vegetation = col_character(), ## geometry = col_character(), ## phenology = col_character(), ## NDVI = col_double() ## ) XSptsPheno %&gt;% ggplot(aes(NDVI)) + geom_histogram(binwidth=0.05) Figure 4.3: Distribution of NDVI, Knuthson Meadow Normal histogram: easier to visualize the distribution, see modes sierraData %&gt;% ggplot(aes(TEMPERATURE)) + geom_histogram(fill=&quot;dark green&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 4.4: Distribution of Average Monthly Temperatures, Sierra Nevada Cumulative histogram with proportions: easier to see percentiles, median n &lt;- length(sierraData$TEMPERATURE) sierraData %&gt;% ggplot(aes(TEMPERATURE)) + geom_histogram(aes(y=cumsum(..count..)/n), fill=&quot;dark goldenrod&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 4.5: Cumulative Distribution of Average Monthly Temperatures, Sierra Nevada 4.2.2 Density Plot Density represents how much out of the total. The total area (sum of widths of bins times densities of that bin) adds up to 1. XSptsPheno %&gt;% ggplot(aes(NDVI)) + geom_density() Figure 4.6: Density plot of NDVI, Knuthson Meadow Note that NDVI values are &lt;1 so bins are very small numbers, so in this case densities can be &gt;1. Using alpha and mapping phenology as fill color. This illustrates two useful ggplot methods: mapping a variable (phenology) to an aesthetic property (fill color of the density polygon) setting a a property (alpha = 0.2) to all polygons of the density plot. The alpha channel of colors defines its opacity, from invisible (0) to opaque (1) so is commonly used to set as its reverse, transparency. XSptsPheno %&gt;% ggplot(aes(NDVI, fill=phenology)) + geom_density(alpha=0.2) tidy_eucoak %&gt;% ggplot(aes(log(runoff_L),fill=tree)) + geom_density(alpha=0.2) Figure 4.7: Runoff under Eucalyptus and Oak in Bay Area sites 4.2.3 boxplot ggplot(data = tidy_eucoak) + geom_boxplot(aes(x = site, y = runoff_L)) Figure 4.8: Runoff under Eucalyptus and Oak, Bay Area Sites Get color from tree within aes() ggplot(data = tidy_eucoak) + geom_boxplot(aes(x=site, y=runoff_L, color=tree)) Figure 4.9: Runoff at Bay Area Sites, colored as Eucalyptus and Oak Visualizing soil CO_2_ data with a Tukey box plot co2 &lt;- soilCO2_97 co2$SITE &lt;- factor(co2$SITE) # in order to make the numeric field a factor ggplot(data = co2, mapping = aes(x = SITE, y = `CO2%`)) + geom_boxplot() Figure 4.10: Visualizing soil CO_2_ data with a Tukey box plot 4.3 Plotting two variables 4.3.1 Two continuous variables Weve looked at this before  the scatterplot ggplot(data=sierraFeb) + geom_point(mapping = aes(TEMPERATURE, ELEVATION)) Figure 4.11: Scatter plot of February temperature vs elevation The aes (aesthetics) function specifies the variables to use as x and y coordinates geom_point creates a scatter plot of those coordinate points Set color for all (not in aes()) ggplot(data=sierraFeb) + geom_point(aes(TEMPERATURE, ELEVATION), color=&quot;blue&quot;) color is defined outside of aes, so is applies to all points. mapping is first argument of geom_point, so mapping = is not needed. 4.3.2 Two variables, one discrete ggplot(tidy_eucoak) + geom_bar(aes(site, runoff_L), stat=&quot;identity&quot;) Figure 4.12: Two variables, one discrete 4.4 Color systems You can find a lot about color systems. See these sources: http://sape.inf.usi.ch/quick-reference/ggplot2/colour http://applied-r.com/rcolorbrewer-palettes/ 4.4.1 Color from variable, in aesthetics In this graph, color is defined inside aes, so is based on COUNTY ggplot(data=sierraFeb) + geom_point(aes(TEMPERATURE, ELEVATION, color=COUNTY)) Figure 4.13: Color set within aes() Plotting lines using the same x,y in aesthetics sierraFeb %&gt;% ggplot(aes(TEMPERATURE,ELEVATION)) + geom_point(color=&quot;blue&quot;) + geom_line(color=&quot;red&quot;) Figure 4.14: Using aesthetics settings for both points and lines Note the use of pipe to start with the data then apply ggplot. River map &amp; profile x &lt;- c(1000, 1100, 1300, 1500, 1600, 1800, 1900) y &lt;- c(500, 700, 800, 1000, 1200, 1300, 1500) z &lt;- c(0, 1, 2, 5, 25, 75, 150) d &lt;- rep(NA, length(x)) longd &lt;- rep(NA, length(x)) s &lt;- rep(NA, length(x)) for(i in 1:length(x)){ if(i==1){longd[i] &lt;- 0; d[i] &lt;-0} else{ d[i] &lt;- sqrt((x[i]-x[i-1])^2 + (y[i]-y[i-1])^2) longd[i] &lt;- longd[i-1] + d[i] s[i-1] &lt;- (z[i]-z[i-1])/d[i]}} longprofile &lt;- bind_cols(x=x,y=y,z=z,d=d,longd=longd,s=s) ggplot(longprofile, aes(x,y)) + geom_line(mapping=aes(col=s), size=1.2) + geom_point(mapping=aes(col=s, size=z)) + coord_fixed(ratio=1) + scale_color_gradient(low=&quot;green&quot;, high=&quot;red&quot;) + ggtitle(&quot;Simulated river path, elevations and slopes&quot;) Figure 4.15: Longitudinal Profiles ggplot(longprofile, aes(longd,z)) + geom_line(aes(col=s), size=1.5) + geom_point() + scale_color_gradient(low=&quot;green&quot;, high=&quot;red&quot;) + ggtitle(&quot;Elevation over longitudinal distance upstream&quot;) Figure 4.16: Longitudinal Profiles ggplot(longprofile, aes(longd,s)) + geom_point(aes(col=s), size=3) + scale_color_gradient(low=&quot;green&quot;, high=&quot;red&quot;) + ggtitle(&quot;Slope over longitudinal distance upstream&quot;) ## Warning: Removed 1 rows containing missing values (geom_point). Figure 4.17: Longitudinal Profiles #summary(lm(s~longd, data=longprofile)) 4.4.2 Trend line sierraFeb %&gt;% ggplot(aes(TEMPERATURE,ELEVATION)) + geom_point(color=&quot;blue&quot;) + geom_smooth(color=&quot;red&quot;, method=&quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 20 rows containing non-finite values (stat_smooth). ## Warning: Removed 20 rows containing missing values (geom_point). Figure 4.18: Trend line using geom_smooth with a linear model 4.4.3 General symbology A useful vignette accessed by vignette(\"ggplot2-specs\") lets you see aesthetic specifications for symbols, including: Color &amp; fill Lines line type, size, ends Polygon border color, linetype, size fill Points shape size color &amp; fill stroke Text font face &amp; size justification 4.4.3.1 Categorical symbology One example of a Big Data resource is EPAs Toxic Release Inventory that tracks releases from a wide array of sources, from oil refineries on down. One way of dealing with big data in terms of exploring meaning is to use symbology to try to make sense of it. csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_2017_CA.csv&quot;, package=&quot;iGIScData&quot;) TRI &lt;- read_csv(csvPath) %&gt;% filter(`5.1_FUGITIVE_AIR` &gt; 100 &amp; `5.2_STACK_AIR` &gt; 100) ## Warning: Missing column names filled in: &#39;X110&#39; [110] ## Warning: 3807 parsing failures. ## row col expected actual file ## 1 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## 2 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## 3 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## 4 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## 5 -- 110 columns 109 columns &#39;C:/Users/900008452/Documents/R/win-library/4.0/iGIScData/extdata/TRI_2017_CA.csv&#39; ## ... ... ........... ........... .................................................................................. ## See problems(...) for more details. ggplot(data = TRI, aes(log(`5.2_STACK_AIR`), log(`5.1_FUGITIVE_AIR`), color = INDUSTRY_SECTOR)) + geom_point() Figure 4.19: EPA Toxic Release Inventory, as a big data set needing symbology clarification 4.4.3.2 Graphs from grouped data XSptsPheno %&gt;% ggplot() + geom_point(aes(elevation, NDVI, shape=vegetation, color = phenology), size = 3) + geom_smooth(aes(elevation, NDVI, color = phenology), method=&quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 4.20: NDVI symbolized by vegetation in two seasons ggplot(data = tidy_eucoak) + geom_point(mapping = aes(x = rain_mm, y = runoff_L, color = tree)) + geom_smooth(mapping = aes(x = rain_mm, y= runoff_L, color = tree), method = &quot;lm&quot;) + scale_color_manual(values = c(&quot;seagreen4&quot;, &quot;orange3&quot;)) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 4.21: Eucalyptus and Oak: rainfall and runoff 4.4.3.3 Faceted graphs This is another option to displaying groups of data, with parallel graphs ggplot(data = tidy_eucoak) + geom_point(aes(x=rain_mm,y=runoff_L)) + geom_smooth(aes(x=rain_mm,y=runoff_L), method=&quot;lm&quot;) + facet_grid(tree ~ .) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 4.22: Faceted graph alternative 4.5 Titles and subtitles ggplot(data = tidy_eucoak) + geom_point(aes(x=rain_mm,y=runoff_L, color=tree)) + geom_smooth(aes(x=rain_mm,y=runoff_L, color=tree), method=&quot;lm&quot;) + scale_color_manual(values=c(&quot;seagreen4&quot;,&quot;orange3&quot;)) + labs(title=&quot;rainfall ~ runoff&quot;, subtitle=&quot;eucalyptus &amp; oak sites, 2016&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 4.23: Titles added 4.6 Pairs Plot sierraFeb %&gt;% select(LATITUDE, ELEVATION, TEMPERATURE, PRECIPITATION) %&gt;% pairs() Figure 4.24: Pairs plot for Sierra Nevada stations variables 4.7 Exercises Create a bar graph of the counts of the species in the penguins data frame. What can you say about what it shows? Use bind_cols in dplyr to create a tibble from built-in vectors state.abb and state.region, then use ggplot with geom_bar to create a bar graph of the four regions. Convert the built-in time series treering into a tibble trusing the tibble() functions with the single variable assigned as treering = treering, then create a histogram, using that tibble and variable for the data and x settings needed. Attach a screen capture of the histogram. Start by clearing your environment with the broom icon in the Environment tab, then well create two tibbles: Create a new tibble st using bind_cols with Name=state.name, Abb=state.abb, Region=state.region, then a tibble created from state.x77 with as_tibble. Then use the save button in the Environment tab to save these two tibbles as Q4.RData, and attach that for your answer. From st, create a density plot from the variable Frost (number of days with frost for that state). Attach that plot, and answer: approximately what is the modal value? From st create a a boxplot of Area by Region. Which region has the highest and which has the lowest median Area? Do the same for Frost. From st, compare murder rate (y=Murder) to Frost (x) in a scatter plot, colored by Region. Add a trend line (smooth) with method=lm to your scatterplot, not colored by Region (but keep the points colored by Region). What can you say about what this graph is showing you? Add a title to your graph. Change your scatterplot to place labels using the Abb variable (still colored by Region) using geom_label(aes(label=Abb, col=Region)). Any observations about outliers? "],["spatial-r.html", "Chapter 5 Spatial R 5.1 Spatial Data 5.2 Raster GIS in R 5.3 ggplot2 for maps 5.4 tmap 5.5 Exercises", " Chapter 5 Spatial R Well explore the basics of simple features (sf) for building spatial datasets, then some common mapping methods, probably: ggplot2 tmap 5.1 Spatial Data To work with spatial data requires extending R to deal with it using packages. Many have been developed, but the field is starting to mature using international open GIS standards. sp (until recently, the dominant library of spatial tools) Includes functions for working with spatial data Includes spplot to create maps Also needs rgdal package for readOGR  reads spatial data frames. sf (Simple Features) ISO 19125 standard for GIS geometries Also has functions for working with spatial data, but clearer to use. Doesnt need many additional packages, though you may still need rgdal installed for some tools you want to use. Replacing sp and spplot though youll still find them in code. Well give it a try Works with ggplot2 and tmap for nice looking maps. Cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/sf.pdf 5.1.0.1 simple feature geometry sfg and simple feature column sfc 5.1.1 Examples of simple geometry building in sf sf functions have the pattern st_* st means space and time See Geocomputation with R at https://geocompr.robinlovelace.net/ or https://r-spatial.github.io/sf/ for more details, but heres an example of manual feature creation of sf geometries (sfg): library(tidyverse) library(sf) library(iGIScData) library(sf) eyes &lt;- st_multipoint(rbind(c(1,5), c(3,5))) nose &lt;- st_point(c(2,4)) mouth &lt;- st_linestring(rbind(c(1,3),c(3, 3))) border &lt;- st_polygon(list(rbind(c(0,5), c(1,2), c(2,1), c(3,2), c(4,5), c(3,7), c(1,7), c(0,5)))) face &lt;- st_sfc(eyes, nose, mouth, border) # sfc = sf column plot(face) Figure 5.1: Building simple geometries in sf The face was a simple feature column (sfc) built from the list of sfgs. An sfc just has the one column, so is not quite like a shapefile. But it can have a coordinate referencing system CRS, and so can be mapped. Kind of like a shapefile with no other attributes than shape 5.1.2 Building a mappable sfc from scratch CA_matrix &lt;- rbind(c(-124,42),c(-120,42),c(-120,39),c(-114.5,35), c(-114.1,34.3),c(-114.6,32.7),c(-117,32.5),c(-118.5,34),c(-120.5,34.5), c(-122,36.5),c(-121.8,36.8),c(-122,37),c(-122.4,37.3),c(-122.5,37.8), c(-123,38),c(-123.7,39),c(-124,40),c(-124.4,40.5),c(-124,41),c(-124,42)) NV_matrix &lt;- rbind(c(-120,42),c(-114,42),c(-114,36),c(-114.5,36), c(-114.5,35),c(-120,39),c(-120,42)) CA_list &lt;- list(CA_matrix); NV_list &lt;- list(NV_matrix) CA_poly &lt;- st_polygon(CA_list); NV_poly &lt;- st_polygon(NV_list) sfc_2states &lt;- st_sfc(CA_poly,NV_poly,crs=4326) # crs=4326 specifies GCS st_geometry_type(sfc_2states) ## [1] POLYGON POLYGON ## 18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE library(tidyverse) ggplot() + geom_sf(data = sfc_2states) Figure 5.2: A simple map built from scratch with hard-coded data as simple feature columns sf class Is like a shapefile: has attributes to which geometry is added, and can be used like a data frame. attributes &lt;- bind_rows(c(abb=&quot;CA&quot;, area=423970, pop=39.56e6), c(abb=&quot;NV&quot;, area=286382, pop=3.03e6)) twostates &lt;- st_sf(attributes, geometry = sfc_2states) ggplot(twostates) + geom_sf() + geom_sf_text(aes(label = abb)) ## Warning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not give ## correct results for longitude/latitude data Figure 5.3: Using an sf class to build a map, displaying an attribute 5.1.3 Creating features from shapefiles or tables sfs st_read reads shapefiles shapefile is an open GIS format for points, polylines, polygons You would normally have shapefiles (and all the files that go with them  .shx, etc.) stored on your computer, but well access one from the iGIScData external data folder: library(iGIScData) library(sf) shpPath &lt;- system.file(&quot;extdata&quot;,&quot;CA_counties.shp&quot;, package=&quot;iGIScData&quot;) CA_counties &lt;- st_read(shpPath) ## Reading layer `CA_counties&#39; from data source `C:\\Users\\900008452\\Documents\\R\\win-library\\4.0\\iGIScData\\extdata\\CA_counties.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 58 features and 60 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -124.4152 ymin: 32.53427 xmax: -114.1312 ymax: 42.00952 ## geographic CRS: WGS 84 plot(CA_counties) ## Warning: plotting the first 9 out of 60 attributes; use max.plot = 60 to plot all st_as_sf converts data frames using coordinates read from x and y variables, with crs set to coordinate system (4326 for GCS) sierraFebpts &lt;- st_as_sf(sierraFeb, coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs=4326) plot(sierraFebpts) library(tidyverse) library(sf) library(iGIScData) censusCentroids &lt;- st_centroid(BayAreaTracts) TRI_sp &lt;- st_as_sf(TRI_2017_CA, coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs=4326) # simple way to specify coordinate reference bnd &lt;- st_bbox(censusCentroids) ggplot() + geom_sf(data = BayAreaCounties, aes(fill = NAME)) + geom_sf(data = censusCentroids) + geom_sf(data = CAfreeways, color = &quot;grey&quot;) + geom_sf(data = TRI_sp, color = &quot;yellow&quot;) + coord_sf(xlim = c(bnd[1], bnd[3]), ylim = c(bnd[2], bnd[4])) + labs(title=&quot;Bay Area Counties, Freeways and Census Tract Centroids&quot;) Figure 5.4: ggplot map of Bay Area TRI sites, census centroids, freeways 5.1.4 Coordinate Referencing System Say you have data you need to make spatial with a spatial reference sierra &lt;- read_csv(\"sierraClimate.csv\") EPSG or CRS codes are an easy way to provide coordinate referencing. Two ways of doing the same thing. Spell it out: GCS &lt;- &quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; wsta = st_as_sf(sierra, coords = c(&quot;LONGITUDE&quot;,&quot;LATITUDE&quot;), crs=GCS) Google to find the code you need and assign it to the crs parameter: wsta &lt;- st_as_sf(sierra, coords = c(\"LONGITUDE\",\"LATITUDE\"), crs=4326) 5.1.4.1 Removing Geometry There are many instances where you want to remove geometry from a sf data frame Some R functions run into problems with geometry and produce confusing error messages, like non-numeric argument Youre wanting to work with an sf data frame in a non-spatial way One way to remove geometry: myNonSFdf &lt;- mySFdf %&gt;% st_set_geometry(NULL) 5.1.5 Spatial join st_join A spatial join with st_join joins data from census where TRI points occur TRI_sp &lt;- st_as_sf(TRI_2017_CA, coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs=4326) %&gt;% st_join(BayAreaTracts) %&gt;% filter(CNTY_FIPS %in% c(&quot;013&quot;, &quot;095&quot;)) 5.1.6 Plotting maps in the base plot system There are various programs for creating maps from spatial data, and well look at a few after weve looked at rasters. As usual, the base plot system often does something useful when you give it data. plot(BayAreaCounties) ## Warning: plotting the first 9 out of 60 attributes; use max.plot = 60 to plot all And with just one variable: plot(BayAreaCounties[&quot;POP_SQMI&quot;]) Theres a lot more we could do with the base plot system, but well mostly focus on some better options in ggplot2 and tmap. 5.2 Raster GIS in R Simple Features are feature-based, so based on the name I guess its not surprising that sf doesnt have support for rasters. But we can use the raster package for that. A bit of raster reading and map algebra with Marble Mountains elevation data library(raster) rasPath &lt;- system.file(&quot;extdata&quot;,&quot;elev.tif&quot;, package=&quot;iGIScData&quot;) elev &lt;- raster(rasPath) slope &lt;- terrain(elev, opt=&quot;slope&quot;) aspect &lt;- terrain(elev, opt=&quot;aspect&quot;) slopeclasses &lt;-matrix(c(0,0.2,1, 0.2,0.4,2, 0.4,0.6,3, 0.6,0.8,4, 0.8,1,5), ncol=3, byrow=TRUE) slopeclass &lt;- reclassify(slope, rcl = slopeclasses) plot(elev) plot(slope) plot(slopeclass) plot(aspect) 5.2.1 Raster from scratch new_raster2 &lt;- raster(nrows = 6, ncols = 6, res = 0.5, xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5, vals = 1:36) plot(new_raster2) 5.3 ggplot2 for maps The Grammar of Graphics is the gg of ggplot. Key concept is separating aesthetics from data Aesthetics can come from variables (using aes()setting) or be constant for the graph Mapping tools that follow this lead ggplot, as we have seen, and it continues to be enhanced tmap (Thematic Maps) https://github.com/mtennekes/tmap Tennekes, M., 2018, tmap: Thematic Maps in R, Journal of Statistical Software 84(6), 1-39 ggplot(CA_counties) + geom_sf() Try ?geom_sf and youll find that its first parameters is mapping with aes() by default. The data property is inherited from the ggplot call, but commonly youll want to specify data=something in your geom_sf call. Another simple ggplot, with labels ggplot(CA_counties) + geom_sf() + geom_sf_text(aes(label = NAME), size = 1.5) ## Warning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not give ## correct results for longitude/latitude data and now with fill color ggplot(CA_counties) + geom_sf(aes(fill = MED_AGE)) + geom_sf_text(aes(label = NAME), col=&quot;white&quot;, size=1.5) ## Warning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not give ## correct results for longitude/latitude data Repositioned legend, no x or y labels ggplot(CA_counties) + geom_sf(aes(fill=MED_AGE)) + geom_sf_text(aes(label = NAME), col=&quot;white&quot;, size=1.5) + theme(legend.position = c(0.8, 0.8)) + labs(x=&quot;&quot;,y=&quot;&quot;) Map in ggplot2, zoomed into two counties: library(tidyverse); library(sf); library(iGIScData) census &lt;- BayAreaTracts %&gt;% filter(CNTY_FIPS %in% c(&quot;013&quot;, &quot;095&quot;)) TRI &lt;- TRI_2017_CA %&gt;% st_as_sf(coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs=4326) %&gt;% st_join(census) %&gt;% filter(CNTY_FIPS %in% c(&quot;013&quot;, &quot;095&quot;), (`5.1_FUGITIVE_AIR` + `5.2_STACK_AIR`) &gt; 0) ## although coordinates are longitude/latitude, st_intersects assumes that they are planar ## although coordinates are longitude/latitude, st_intersects assumes that they are planar bnd = st_bbox(census) ggplot() + geom_sf(data = BayAreaCounties, aes(fill = NAME)) + geom_sf(data = census, color=&quot;grey40&quot;, fill = NA) + geom_sf(data = TRI) + coord_sf(xlim = c(bnd[1], bnd[3]), ylim = c(bnd[2], bnd[4])) + labs(title=&quot;Census Tracts and TRI air-release sites&quot;) + theme(legend.position = &quot;none&quot;) 5.3.1 Rasters in ggplot2 Raster display in ggplot2 is currently a little awkward, as are rasters in general in the feature-dominated GIS world. We can use a trick: converting rasters to a grid of points: library(tidyverse) library(sf) library(raster) rasPath &lt;- system.file(&quot;extdata&quot;,&quot;elev.tif&quot;, package=&quot;iGIScData&quot;) elev &lt;- raster(rasPath) shpPath &lt;- system.file(&quot;extdata&quot;,&quot;trails.shp&quot;, package=&quot;iGIScData&quot;) trails &lt;- st_read(shpPath) ## Reading layer `trails&#39; from data source `C:\\Users\\900008452\\Documents\\R\\win-library\\4.0\\iGIScData\\extdata\\trails.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 32 features and 8 fields ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: 481903.8 ymin: 4599196 xmax: 486901.9 ymax: 4603200 ## projected CRS: NAD83 / UTM zone 10N elevpts = as.data.frame(rasterToPoints(elev)) ggplot() + geom_raster(data = elevpts, aes(x = x, y = y, fill = elev)) + geom_sf(data = trails) 5.4 tmap Basic building block is tm_shape(data) followed by various layer elements such as tm_fill() shape can be features or raster See Geocomputation with R Chapter 8 Making Maps with R for more information. https://geocompr.robinlovelace.net/adv-map.html library(spData) ## ## Attaching package: &#39;spData&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## elev library(tmap) tm_shape(world) + tm_fill() + tm_borders() Color by variable library(sf) library(tmap) tm_shape(BayAreaTracts) + tm_fill(col = &quot;MED_AGE&quot;) tmap of sierraFeb with hillshade and point symbols library(tmap) library(sf) library(raster) library(iGIScData) tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting tmap_options(max.categories = 8) sierra &lt;- st_as_sf(sierraFeb, coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs = 4326) rasPath &lt;- system.file(&quot;extdata&quot;,&quot;ca_hillsh_WGS84.tif&quot;, package=&quot;iGIScData&quot;) hillsh &lt;- raster(rasPath) bounds &lt;- st_bbox(sierra) tm_shape(hillsh,bbox=bounds)+ tm_raster(palette=&quot;-Greys&quot;,legend.show=FALSE,n=10) + tm_shape(sierra) + tm_symbols(col=&quot;TEMPERATURE&quot;, palette=c(&quot;blue&quot;,&quot;red&quot;), style=&quot;cont&quot;,n=8) + tm_legend() + tm_layout(legend.position=c(&quot;RIGHT&quot;,&quot;TOP&quot;)) ## stars object downsampled to 1092 by 915 cells. See tm_shape manual (argument raster.downsample) Note: -Greys needed to avoid negative image, since Greys go from light to dark, and to match reflectance as with b&amp;w photography, they need to go from dark to light. 5.4.1 Interactive Maps The word static in static maps isnt something you would have heard in a cartography class 30 years ago, since essentially all maps then were static. Very important in designing maps is considering your audience, and one characteristic of the audience of those maps of yore were that they were printed and thus fixed on paper. A lot of cartographic design relates to that property: Figure-to-ground relationships assume ground is a white piece of paper (or possibly a standard white background in a pdf), so good cartographic color schemes tend to range from light for low values to dark for high values. Scale is fixed and there are no tools for changing scale, so a lot of attention must be paid to providing scale information. Similarly, without the ability to see the map at different scales, inset maps are often needed to provide context. Interactive maps change the game in having tools for changing scale, and always being printed on a computer or device where the color of the background isnt necessarily white. We are increasingly used to using interactive maps on our phones or other devices, and often get frustrated not being able to zoom into a static map. A widely used interactive mapping system is Leaflet, but were going to use tmap to access Leaflet behind the scenes and allow us to create maps with one set of commands. The key parameter needed is tmap_mode which must be set to view to create an interactive map. tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing tm_shape(BayAreaTracts) + tm_fill(col = &quot;MED_AGE&quot;, alpha = 0.5) library(tmap) library(sf) tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing tmap_options(max.categories = 8) sierra &lt;- st_as_sf(sierraFeb, coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs = 4326) bounds &lt;- st_bbox(sierra) tm_basemap(leaflet::providers$Esri.NatGeoWorldMap) + tm_shape(sierra) + tm_symbols(col=&quot;TEMPERATURE&quot;, palette=c(&quot;blue&quot;,&quot;red&quot;), style=&quot;cont&quot;,n=8,size=0.2) + tm_legend() + tm_layout(legend.position=c(&quot;RIGHT&quot;,&quot;TOP&quot;)) ## legend.postion is used for plot mode. Use view.legend.position in tm_view to set the legend position in view mode. 5.4.1.1 Leaflet Now that weve seen an app that used it, lets look briefly at Leaflet itself, and well see that even the Leaflet package in R actually uses JavaScript Leaflet is designed as An open-source JavaScript library for mobile-friendly interactive maps https://leafletjs.com The R package leaflet is an interface to the JavaScript library Leaflet to create interactive web maps. It was developed on top of the htmlwidgets framework, which means the maps can be rendered in RMarkdown (v2) documents (which is why you can see it in this document), Shiny apps, and RStudio IDE / the R console. https://blog.rstudio.com/2015/06/24/leaflet-interactive-web-maps-with-r/ https://github.com/rstudio/cheatsheets/blob/master/leaflet.pdf library(leaflet) m &lt;- leaflet() %&gt;% addTiles() %&gt;% # default OpenStreetMap tiles addMarkers(lng=174.768, lat=-36.852, popup=&quot;The birthplace of R&quot;) m 5.5 Exercises Using the method of building simple sf geometries, build a simple 1x1 square object and plot it. Remember that you have to close the polygon, so the first vertex is the same as the last (of 5) vertices. Provide your code only. Build a map in ggplot of Colorado, Wyoming, and Utah with these boundary vertices in GCS. As with the square, remember to close each figure, and assign the crs to what is needed for GCS: 4326. Submit map as exported plot, and code in the submittal text block. Colorado: (-109,41),(-102,41),(-102,37),(-109,37) Wyoming: (-111,45),(-104,45),(-104,41),(-111,41) Utah: (-114,42),(-111,42),(-111,41),(-109,41),(-109,37),(-114,37) Arizona: (-114,37),(-109,37),(-109,31.3),(-111,31.3),(-114.8,32.5), (-114.6,32.7),(-114.1,34.3),(-114.5,35),(-114.5,36),(-114,36) New Mexico: (-109,37),(-103,37),(-103,32),(-106.6,32),(-106.5,31.8), (-108.2,31.8),(-108.2,31.3),(-109,31.3) Add in the code for CA and NV and create kind of a western US map Create an sf class from the seven states adding the fields name, abb, area_sqkm, and population, and create a map labeling with the name. Colorado, CO, 269837, 5758736 Wyoming, WY, 253600, 578759 Utah, UT, 84899, 3205958 Arizona, AZ, 295234, 7278717 New Mexico, NM, 314917, 2096829 California, CA, 423970, 39368078 Nevada, NV, 286382, 3080156 Create a tibble for the highest peaks in the 7 states, with the following names, elevations in m, longitude and latitude, and add them to that map: Wheeler Peak, 4011, -105.4, 36.5 Mt. Whitney, 4421, -118.2, 36.5 Boundary Peak, 4007, -118.35, 37.9 Kings Peak, 4120, -110.3, 40.8 Gannett Peak, 4209, -109, 43.2 Mt. Elbert, 4401, -106.4, 39.1 Humphreys Peak, 3852, -111.7, 35.4 Note: the easiest way to do this is with the tribble function, starting with: peaks &lt;- tribble( ~peak, ~elev, ~longitude, ~latitude, &quot;Wheeler Peak&quot;, 4011, -105.4, 36.5, Use a spatial join to add the points to the states to provide a new attribute maximum elevation, and display that using geom_sf_text() with the state polygons. From the CA_counties and CAfreeways feature data in iGIScData, make a simple map in ggplot, with freeways colored red. After adding the raster library, create a raster from the built-in volcano matrix of elevations from Aucklands Maunga Whau Volcano, and use plot() to display it. Wed do more with that dataset but we dont know what the cell size is. Use tmap to create a simple map from the SW_States (polygons) and peaksp (points) data we created earlier. Hints: youll want to use tm_text with text set to peak to label the points, along with the parameter auto.placement=TRUE. Change the map to the view mode, but dont use the state borders since the basemap will have them. Just before adding shapes, set the basemap to leaflet::providers$Esri.NatGeoWorldMap, then continue to the peaks after the + to see the peaks on a National Geographic basemap. "],["building-a-data-package-for-github.html", "Chapter 6 Building a Data Package for GitHub 6.1 rda files 6.2 Raw data", " Chapter 6 Building a Data Package for GitHub These are just some notes on building data packages, based mostly on Chapter 14 External Packages of r-pkgs.org, which also covers code packages. For our package, iGIScData, we provided data in two ways: rda files: normal external data that are ready to use as data frames, simple feature (sf) data, and rasters. raw data as CSVs, shapefiles and TIFFs. 6.1 rda files These files need to be prepared from data in R and go in the data folder. The process is made very easy by using usethis::use_data() package: 6.1.1 usethis::use_data() usethis::use_data() is used to add data as rda files to the data folder. These data can be data frames, simple features, rasters, and Im sure other things. I used a script addData.R that I put in the data-raw folder which built the data (usually with read_csv(), st_read(), or raster() and maybe some other processing like mutate, filter, etc.) to create the data set, and then usethis::use_data() to store it in the data folder. Heres an simple example with just a csv converted directly, and it takes care of storing the result in the data folder as an .rda file: sierraFeb &lt;- read_csv(&quot;data-raw/sierraFeb.csv&quot;) usethis::use_data(sierraFeb) 6.1.2 usethis::create_package() This creates the package and uses roxygen to document it. I think you just need to run this once, then the devtools::document() does the rest, and can be run again to update it. 6.1.3 devtools::document() This creates documentation on the data sets, using the file R/data.R, which will need to have lines of code similar to the following to document each data set. Note that the name of the data set goes last, in quotes. The formatting of the field names and descriptions is a bit tricky and doesnt follow normal R rules. As a result, sometimes my field names dont exactly match the actual field names. Maybe Ill get around to changing the original field names with rename. Note that the organization is important, with the title of the data first, a blank line, then a description, etc.: #&#39; Sierra February climate data #&#39; #&#39; Selection from SierraData to only include February data #&#39; #&#39; @format A data frame with 82 entries and 7 variables selected and renamed \\describe{ #&#39; \\item{STATION_NAME}{Station name} #&#39; \\item{COUNTY}{County Name} #&#39; \\item{ELEVATION}{Elevation in meters} #&#39; \\item{LATITUDE}{Latitude in decimal degrees} #&#39; \\item{LONGITUDE}{Longitude in decimal degrees} #&#39; \\item{PRECIPITATION}{February Average Precipitation in mm} #&#39; \\item{TEMPERATURE}{Febrary Average Temperature in degrees C} #&#39; } #&#39; @source \\url{https://www.ncdc.noaa.gov/} &quot;sierraFeb&quot; Once these are on GitHub, a user can simply install the package with devtools::install_github(\"iGISc/iGIScData\")  to use the iGIScData we created. Then to access the data just like built-in data, the user just needs to load that library with library(iGIScData) But we also wanted to provide raw CSVs, shapefiles and rasters, in order to demonstrate how to read those. 6.2 Raw data Raw data (e.g. CSVs, shapefiles, and rasters) are simply stored in the inst/extdata folder. Just create those folders and put the files there. Make sure to include all the files (like the multiple files that go with a shapefile). Then, to access the data once the data package is installed, the user just needs to use the system.file() function to provide the path and then use that with the appropriate read function; e.g. for a CSV, something like: csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_2017_CA.csv&quot;) TRI_2017_CA &lt;- read_csv(csvPath) "]]
