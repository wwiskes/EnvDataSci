[["index.html", "Introduction to Environmental Data Science 1 Prerequisites 1.1 Packages to install 1.2 Data", " Introduction to Environmental Data Science Jerry Davis 2021-04-19 1 Prerequisites This book is intended to work in concert with a series of lectures and discussions among the participants in Geog 604/704 Environmental Data Science at San Francisco State University. Data packages will be created on GitHub. 1.1 Packages to install Participants need to have R and RStudio installed, and at least the following packages: tidyverse (to include ggplot2, dplyr, tidyr, stringr, etc.) ggplot2 dplyr stringr tidyr lubridate sf raster tmap To install these packages, the following code will work: install.packages(c(&quot;tidyverse&quot;, &quot;lubridate&quot;, &quot;sf&quot;, &quot;raster&quot;, &quot;tmap&quot;)) You can always add more packages as needed, do them one at a time, whatever. But generally dont reinstall the packages again with the unless you actually want to reinstall it, maybe because its been updated. So generally I dont include install.packages() in my script. Once installed, you can access the packages with the library function, e.g. library(tidyverse) which you will want to include in your script. 1.2 Data Well be using data from various sources, including data on CRAN like the code packages above which you install the same way  so use install.packages(\"palmerpenguins\"). Weve also created a repository on GitHub that includes data weve developed in the iGISc at SFSU, and youll need to install that package a slightly different way. GitHub packages require a bit more work on the users part since we need to first install remotes then use that to install the GitHub data package: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;iGISc/iGIScData&quot;) Note: you can also use devtools instead of remotes if you have that installed. They do the same thing; remotes is a subset of devtools. If you see a message about Rtools, you can ignore it since that is only needed for building tools from C++ and things like that. Then you can access it just like other built-in data by including: library(iGIScData) To see whats in it, youll see the various datasets listed in: data(package=&quot;iGIScData&quot;) Those package datasets can be used directly as sf data (if the sf library is installed) or data frames (all tibbles). Raw data can also be read from the extdata folder that is installed on your computer when you install the package, using code such as: csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_1987_BaySites.csv&quot;, package=&quot;iGIScData&quot;) TRI87 &lt;- read_csv(csvPath) or something similar for shapefiles. "],["introduction-to-r.html", "2 Introduction to R 2.1 Variables 2.2 Functions 2.3 Expressions and Statements 2.4 Data Types 2.5 Rectangular data 2.6 Data Structures in R 2.7 Programming scripts in RStudio 2.8 Exercises", " 2 Introduction to R Were assuming youre either new to R or need a refresher. Well start with some basic R operations entered directly in the console in RStudio. 2.1 Variables Variables are objects that store values. Every computer language, like in math, stores values by assigning them constants or results of expressions. x &lt;- 5 uses the R standard assignment operator &lt;- though you can also use =. Well use &lt;- because it is more common and avoids some confusion with other syntax. Variable names must start with a letter, have no spaces, and not use any names that are built into the R language or used in package libraries, such as reserved words like for or function names like log() x &lt;- 5 y &lt;- 8 longitude &lt;- -122.4 latitude &lt;- 37.8 my_name &lt;- &quot;Inigo Montoya&quot; To check the value of a variable or other object, you can just enter the name in the console, or even in the code in a code chunk. x ## [1] 5 y ## [1] 8 longitude ## [1] -122.4 latitude ## [1] 37.8 my_name ## [1] &quot;Inigo Montoya&quot; This is counter to the way printing out values work in programming, and you will need to know how this method works as well because you will want to use your code to develop tools that accomplish things, and there are also limitations to what you can see by just naming variables. To see the values of variables in programming mode, use the print() function, or to concatenate character string output, use paste(): print(x) ## [1] 5 print(y) ## [1] 8 print(latitude) ## [1] 37.8 paste(&quot;The location is latitude&quot;, latitude, &quot;longitude&quot;, longitude) ## [1] &quot;The location is latitude 37.8 longitude -122.4&quot; paste(&quot;My name is&quot;, my_name, &quot;-- Prepare to die.&quot;) ## [1] &quot;My name is Inigo Montoya -- Prepare to die.&quot; 2.2 Functions Once you have variables or other objects to work with, most of your work involves functions such as the well-known math functions log10(100) log(exp(5)) cos(pi) sin(90 * pi/180) Most of your work will involve functions and there are too many to name, even in the base functions, not to mention all the packages we will want to use. You will likely have already used the install.packages() and library() functions that add in an array of other functions. Later well also learn how to write our own functions, a capability that is easy to accomplish and also gives you a sense of what developing your own package might be like. Arithmetic operators There are of course all the normal arithmetic operators (that are actually functions) like plus + and minus - or the key-stroke approximations of multiply * and divide / operators. Youre probably familiar with these approximations from using equations in Excel if not in some other programming language you may have learned. These operators look a bit different from how theyd look when creating a nicely formatted equation. For example, \\(\\frac{NIR - R}{NIR + R}\\) instead has to look like (NIR-R)/(NIR+R). Similarly * must be used to multiply; theres no implied multiplication that we expect in a math equation like \\(x(2+y)\\) which would need to be written x*(2+y). In contrast to those four well-known operators, the symbol used to exponentiate  raise to a power  varies among programming languages. R uses ** so the the Pythagorean theorem \\(c^2=a^2+b^2\\) would be written c**2 = a**2 + b**2 except for the fact that it wouldnt make sense as a statement to R. Well need to talk about expressions and statements. 2.3 Expressions and Statements The concepts of expressions and statements are very important to understand in any programming language. An expression in R (or any programming language) has a value just like a variable has a value. An expression will commonly combine variables and functions to be evaluated to derive the value of the expression. Here are some examples of expressions: 5 x x*2 sin(x) sqrt(a**2 + b**2) (-b+sqrt(b**2-4*a*c))/2*a paste(&quot;My name is&quot;, aname) Note that some of those expressions used previously assigned variables  x, a, b, c, aname. An expression can be entered in the console to display its current value, and this is commonly done in R for objects of many types and complexity. cos(pi) ## [1] -1 print(cos(pi)) ## [1] -1 state.x77 ## Population Income Illiteracy Life Exp Murder HS Grad Frost Area ## Alabama 3615 3624 2.1 69.05 15.1 41.3 20 50708 ## Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 ## Arizona 2212 4530 1.8 70.55 7.8 58.1 15 113417 ## Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 51945 ## California 21198 5114 1.1 71.71 10.3 62.6 20 156361 ## Colorado 2541 4884 0.7 72.06 6.8 63.9 166 103766 ## Connecticut 3100 5348 1.1 72.48 3.1 56.0 139 4862 ## Delaware 579 4809 0.9 70.06 6.2 54.6 103 1982 ## Florida 8277 4815 1.3 70.66 10.7 52.6 11 54090 ## Georgia 4931 4091 2.0 68.54 13.9 40.6 60 58073 ## Hawaii 868 4963 1.9 73.60 6.2 61.9 0 6425 ## Idaho 813 4119 0.6 71.87 5.3 59.5 126 82677 ## Illinois 11197 5107 0.9 70.14 10.3 52.6 127 55748 ## Indiana 5313 4458 0.7 70.88 7.1 52.9 122 36097 ## Iowa 2861 4628 0.5 72.56 2.3 59.0 140 55941 ## Kansas 2280 4669 0.6 72.58 4.5 59.9 114 81787 ## Kentucky 3387 3712 1.6 70.10 10.6 38.5 95 39650 ## Louisiana 3806 3545 2.8 68.76 13.2 42.2 12 44930 ## Maine 1058 3694 0.7 70.39 2.7 54.7 161 30920 ## Maryland 4122 5299 0.9 70.22 8.5 52.3 101 9891 ## Massachusetts 5814 4755 1.1 71.83 3.3 58.5 103 7826 ## Michigan 9111 4751 0.9 70.63 11.1 52.8 125 56817 ## Minnesota 3921 4675 0.6 72.96 2.3 57.6 160 79289 ## Mississippi 2341 3098 2.4 68.09 12.5 41.0 50 47296 ## Missouri 4767 4254 0.8 70.69 9.3 48.8 108 68995 ## Montana 746 4347 0.6 70.56 5.0 59.2 155 145587 ## Nebraska 1544 4508 0.6 72.60 2.9 59.3 139 76483 ## Nevada 590 5149 0.5 69.03 11.5 65.2 188 109889 ## New Hampshire 812 4281 0.7 71.23 3.3 57.6 174 9027 ## New Jersey 7333 5237 1.1 70.93 5.2 52.5 115 7521 ## New Mexico 1144 3601 2.2 70.32 9.7 55.2 120 121412 ## New York 18076 4903 1.4 70.55 10.9 52.7 82 47831 ## North Carolina 5441 3875 1.8 69.21 11.1 38.5 80 48798 ## North Dakota 637 5087 0.8 72.78 1.4 50.3 186 69273 ## Ohio 10735 4561 0.8 70.82 7.4 53.2 124 40975 ## Oklahoma 2715 3983 1.1 71.42 6.4 51.6 82 68782 ## Oregon 2284 4660 0.6 72.13 4.2 60.0 44 96184 ## Pennsylvania 11860 4449 1.0 70.43 6.1 50.2 126 44966 ## Rhode Island 931 4558 1.3 71.90 2.4 46.4 127 1049 ## South Carolina 2816 3635 2.3 67.96 11.6 37.8 65 30225 ## South Dakota 681 4167 0.5 72.08 1.7 53.3 172 75955 ## Tennessee 4173 3821 1.7 70.11 11.0 41.8 70 41328 ## Texas 12237 4188 2.2 70.90 12.2 47.4 35 262134 ## Utah 1203 4022 0.6 72.90 4.5 67.3 137 82096 ## Vermont 472 3907 0.6 71.64 5.5 57.1 168 9267 ## Virginia 4981 4701 1.4 70.08 9.5 47.8 85 39780 ## Washington 3559 4864 0.6 71.72 4.3 63.5 32 66570 ## West Virginia 1799 3617 1.4 69.48 6.7 41.6 100 24070 ## Wisconsin 4589 4468 0.7 72.48 3.0 54.5 149 54464 ## Wyoming 376 4566 0.6 70.29 6.9 62.9 173 97203 A statement in R does something. It represents a directive were assigning to the computer, or maybe the environment were running on the computer (like RStudio, which then runs R). A simple print() statement seems a lot like what we just did when we entered an expression in the console, but recognize that it does something: print(&quot;Hello, World&quot;) ## [1] &quot;Hello, World&quot; Which is the same as just typing Hello, World, but thats just because the job of the console is to display what we are looking for [where we are the ones doing something], or if our statement includes something to display. Statements in R are usually put on one line, but you can use a semicolon to have multiple statements on one line, if desired: x &lt;- 5; print(x); print(x**2) ## [1] 5 ## [1] 25 Many (perhaps most) statements dont actually display anything. For instance: x &lt;- 5 doesnt display anything, but it does assign the value 5 to the variable x, so it simply does something. Its an assignment statement and uses that special assignment operator &lt;- . Most languages just use = which the designers of R didnt want to use, to avoid confusing it with the equal sign meaning is equal to. An assignment statement assigns an expression to a variable. If that variable already exists, it is reused with the new value. For instance its completely legit (and commonly done in coding) to update the variable in an assignment statement. This is very common when using a counter variable: i = i + 1 Youre simply updating the index variable with the next value. This also illustrates why its not an equation: \\(i=i+1\\) doesnt work as an equation (unless i is actually \\(\\infty\\) but thats just really weird.) And c**2 = a**2 + b**2 doesnt make sense as an R statement because c**2 isnt a variable to be created. The ** part is interpreted as raise to a power. What is to the left of the assignment operator = must be a variable to be assigned the value of the expression. 2.4 Data Types Variables, constants and other data elements in R have data types. Common types are numeric and character. x &lt;- 5 class(x) ## [1] &quot;numeric&quot; class(4.5) ## [1] &quot;numeric&quot; class(&quot;Fred&quot;) ## [1] &quot;character&quot; 2.4.1 Integers By default, R creates double-precision floating-point numeric variables To create integer variables: - append an L to a constant, e.g. 5L is an integer 5 - convert with as.integer Were going to be looking at various as. functions in R, more on that later, but we should look at as.integer() now. Most other languages use int() for this, and what it does is converts any number into an integer, truncating it to an integer, not rounding it. as.integer(5) ## [1] 5 as.integer(4.5) ## [1] 4 To round a number, theres a round() function or you can easily use as.integer adding 0.5: x &lt;- 4.8 y &lt;- 4.2 as.integer(x + 0.5) ## [1] 5 round(x) ## [1] 5 as.integer(y + 0.5) ## [1] 4 round(y) ## [1] 4 Integer divison: 5 %/% 2 ## [1] 2 Integer remainder from division (the modulus, using a %% to represent the modulo): 5 %% 2 ## [1] 1 Surprisingly, the values returned by integer division or the remainder are not stored as integers. R seems to prefer floating point 2.5 Rectangular data A common data format used in most types of research is rectangular data such as in a spreadsheet, with rows and columns, where rows might be observations and columns might be variables. Well read this type of data in from spreadsheets or even more commonly from comma-separated-variable (CSV) text files that spreadsheet programs like Excel commonly read in just like their native format. library(iGIScData) sierraFeb ## # A tibble: 62 x 9 ## STATION_NAME COUNTY ELEVATION LATITUDE LONGITUDE PRECIPITATION TEMPERATURE resid predict ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GROVELAND 2, CA~ Tuolu~ 853. 37.8 -120. 176. 6.1 -0.574 6.67 ## 2 CANYON DAM, CA ~ Plumas 1390. 40.2 -121. 164. 1.4 -2.00 3.40 ## 3 KERN RIVER PH 3~ Kern 824. 35.8 -118. 67.1 8.9 2.05 6.85 ## 4 DONNER MEMORIAL~ Nevada 1810. 39.3 -120. 167. -0.9 -1.74 0.840 ## 5 BOWMAN DAM, CA ~ Nevada 1641. 39.5 -121. 277. 2.9 1.03 1.87 ## 6 GRANT GROVE, CA~ Tulare 2012. 36.7 -119. 186. 1.7 2.09 -0.394 ## 7 LEE VINING, CA ~ Mono 2072. 38.0 -119. 71.9 0.4 1.16 -0.760 ## 8 OROVILLE MUNICI~ Butte 57.9 39.5 -122. 138. 10.3 -1.23 11.5 ## 9 LEMON COVE, CA ~ Tulare 156. 36.4 -119. 62.7 11.3 0.373 10.9 ## 10 CALAVERAS BIG T~ Calav~ 1431 38.3 -120. 254 2.7 -0.450 3.15 ## # ... with 52 more rows 2.6 Data Structures in R We looked briefly at numeric and character string (well abbreviate simply as string from here on). Well also look at factors and dates/times later on. 2.6.1 Vectors A vector is an ordered collection of numbers, strings, vectors, data frames, etc. What we mostly refer to as vectors are formally called atomic vectors which requires that they be homogeneous sets of whatever type were referring to, such as a vector of numbers, or a vector of strings, or a vector of dates/times. You can create a simple vector with the c() function: lats &lt;- c(37.5,47.4,29.4,33.4) lats ## [1] 37.5 47.4 29.4 33.4 states = c(&quot;VA&quot;, &quot;WA&quot;, &quot;TX&quot;, &quot;AZ&quot;) states ## [1] &quot;VA&quot; &quot;WA&quot; &quot;TX&quot; &quot;AZ&quot; zips = c(23173, 98801, 78006, 85001) zips ## [1] 23173 98801 78006 85001 The class of a vector is the type of data it holds temp &lt;- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7) class(temp) ## [1] &quot;numeric&quot; Vectors can only have one data class, and if mixed with character types, numeric elements will become character: mixed &lt;- c(1, &quot;fred&quot;, 7) class(mixed) ## [1] &quot;character&quot; mixed[3] # gets a subset, example of coercion ## [1] &quot;7&quot; 2.6.1.1 NA Data science requires dealing with missing data by storing some sort of null value, called various things: - null - nodata - NA not available or not applicable as.numeric(c(&quot;1&quot;,&quot;Fred&quot;,&quot;5&quot;)) # note NA introduced by coercion ## Warning: NAs introduced by coercion ## [1] 1 NA 5 Ignoring NA in statistical summaries is commonly used. Where normally the summary statistic can only return NA mean(as.numeric(c(&quot;1&quot;, &quot;Fred&quot;, &quot;5&quot;))) ## Warning in mean(as.numeric(c(&quot;1&quot;, &quot;Fred&quot;, &quot;5&quot;))): NAs introduced by coercion ## [1] NA  with na.rm=T you can still get the result for all actual data: mean(as.numeric(c(&quot;1&quot;, &quot;Fred&quot;, &quot;5&quot;)), na.rm=T) ## Warning in mean(as.numeric(c(&quot;1&quot;, &quot;Fred&quot;, &quot;5&quot;)), na.rm = T): NAs introduced by coercion ## [1] 3 Dont confuse with nan (not a number) which is used for things like imaginary numbers (explore the help for more on this) is.na(NA) ## [1] TRUE is.nan(NA) ## [1] FALSE is.na(as.numeric(&#39;&#39;)) ## [1] TRUE is.nan(as.numeric(&#39;&#39;)) ## [1] FALSE i &lt;- sqrt(-1) ## Warning in sqrt(-1): NaNs produced is.na(i) # interestingly nan is also na ## [1] TRUE is.nan(i) ## [1] TRUE 2.6.1.2 Sequences An easy way to make a vector from a sequence of values. The following 3 examples are equivalent: seq(1,10) c(1:10) c(1,2,3,4,5,6,7,8,9,10) The seq() function has special uses like using a step parameter: seq(2,10,2) ## [1] 2 4 6 8 10 2.6.1.3 Vectorization and vector arithmetic Arithmetic on vectors operates element-wise elev &lt;- c(52,394,510,564,725,848,1042,1225,1486,1775,1899,2551) elevft &lt;- elev / 0.3048 elevft ## [1] 170.6037 1292.6509 1673.2283 1850.3937 2378.6089 2782.1522 3418.6352 4019.0289 4875.3281 ## [10] 5823.4908 6230.3150 8369.4226 Another example, with 2 vectors: temp03 &lt;- c(13.1,11.4,9.4,10.9,8.9,8.4,6.7,7.6,2.8,1.6,1.2,-2.1) temp02 &lt;- c(10.7,9.7,7.7,9.2,7.3,6.7,4.0,5.0,0.9,-1.1,-0.8,-4.4) tempdiff &lt;- temp03 - temp02 tempdiff ## [1] 2.4 1.7 1.7 1.7 1.6 1.7 2.7 2.6 1.9 2.7 2.0 2.3 2.6.1.4 Plotting vectors Vectors of Feb temperature, elevation and latitude at stations in the Sierra: temp &lt;- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7, 4.0, 5.0, 0.9, -1.1, -0.8, -4.4) elev &lt;- c(52, 394, 510, 564, 725, 848, 1042, 1225, 1486, 1775, 1899, 2551) lat &lt;- c(39.52, 38.91, 37.97, 38.70, 39.09, 39.25, 39.94, 37.75, 40.35, 39.33, 39.17, 38.21) Plot individually plot(temp) Figure 2.1: Temperature plot(elev) Figure 2.2: Elevation plot(lat) Figure 2.3: Latitude Then plot as a scatterplot plot(elev,temp) Figure 2.4: Temperature~Elevation 2.6.1.5 Named indices Vector indices can be named. codes &lt;- c(380, 124, 818) codes ## [1] 380 124 818 codes &lt;- c(italy = 380, canada = 124, egypt = 818) codes ## italy canada egypt ## 380 124 818 str(codes) ## Named num [1:3] 380 124 818 ## - attr(*, &quot;names&quot;)= chr [1:3] &quot;italy&quot; &quot;canada&quot; &quot;egypt&quot; Why? I guess so you can refer to observations by name instead of index. The following are equivalent: codes[1] ## italy ## 380 codes[&quot;italy&quot;] ## italy ## 380 2.6.2 Lists Lists can be heterogeneous, with multiple class types. Lists are actually used a lot in R, but we wont see them for a while. 2.6.3 Matrices Vectors are commonly used as a column in a matrix (or as well see, a data frame), like a variable temp &lt;- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7, 4.0, 5.0, 0.9, -1.1, -0.8, -4.4) elev &lt;- c(52, 394, 510, 564, 725, 848, 1042, 1225, 1486, 1775, 1899, 2551) lat &lt;- c(39.52, 38.91, 37.97, 38.70, 39.09, 39.25, 39.94, 37.75, 40.35, 39.33, 39.17, 38.21) Building a matrix from vectors as columns sierradata &lt;- cbind(temp, elev, lat) class(sierradata) ## [1] &quot;matrix&quot; &quot;array&quot; 2.6.3.1 Dimensions for arrays and matrices Note: a matrix is just a 2D array. Arrays have 1, 3, or more dimensions. dim(sierradata) ## [1] 12 3 a &lt;- 1:12 dim(a) &lt;- c(3, 4) # matrix class(a) ## [1] &quot;matrix&quot; &quot;array&quot; dim(a) &lt;- c(2,3,2) # 3D array class(a) ## [1] &quot;array&quot; dim(a) &lt;- 12 # 1D array class(a) ## [1] &quot;array&quot; b &lt;- matrix(1:12, ncol=1) # 1 column matrix is allowed 2.6.4 Data frames A data frame is a database with fields (as vectors) with records (rows), so is very important for data analysis and GIS. Theyre kind of like a spreadsheet with rules (first row is field names, fields all one type). So even though theyre more complex than a list, we use them so frequently they become quite familiar [whereas I continue to find lists confusing, especially when discovering them as what a particular function returns.] library(palmerpenguins) head(penguins) ## # A tibble: 6 x 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 ## 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 ## 3 Adelie Torgersen 40.3 18 195 3250 female 2007 ## 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 ## 5 Adelie Torgersen 36.7 19.3 193 3450 female 2007 ## 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 Creating a data frame out of a matrix mydata &lt;- as.data.frame(sierradata) plot(data = mydata, x = elev, y = temp) Figure 2.5: Temperature~Elevation Read a data frame from a CSV Well be looking at this more in the next chapter, but a common need is to read data from a spreadsheet stored in the CSV format. Normally, youd have that stored with your project and can just specify the file name, but well access CSVs from the iGIScData package. Since you have this installed, it will already be on your computer, but not in your project folder. The path to it can be derived using the system.file() function. Reading a csv in readr (part of the tidyverse that well be looking at in the next chapter) is done with read_csv(): library(readr) csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_1987_BaySites.csv&quot;, package=&quot;iGIScData&quot;) TRI87 &lt;- read_csv(csvPath) TRI87 ## # A tibble: 335 x 9 ## TRI_FACILITY_ID count FACILITY_NAME COUNTY air_releases fugitive_air stack_air LATITUDE ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 91002FRMND585BR 2 FORUM IND SAN M~ 1423 1423 0 37.5 ## 2 92052ZPMNF2970C 1 ZEP MFG CO SANTA~ 337 337 0 37.4 ## 3 93117TLDYN3165P 2 TELEDYNE MEC SANTA~ 12600 12600 0 37.4 ## 4 94002GTWSG477HA 2 MORGAN ADVAN~ SAN M~ 18700 18700 0 37.5 ## 5 94002SMPRD120SE 2 SEM PRODS INC SAN M~ 1500 500 1000 37.5 ## 6 94025HBLNN151CO 2 HEUBLEIN INC SAN M~ 500 0 500 37.5 ## 7 94025RYCHM300CO 10 TE CONNECTIV~ SAN M~ 144871 47562 97309 37.5 ## 8 94025SNFRD990OB 1 SANFORD META~ SAN M~ 9675 9675 0 37.5 ## 9 94026BYPCK3575H 2 BAY PACKAGIN~ SAN M~ 80000 32000 48000 37.5 ## 10 94026CDRSY3475E 2 CDR SYS CORP SAN M~ 126800 0 126800 37.5 ## # ... with 325 more rows, and 1 more variable: LONGITUDE &lt;dbl&gt; Sort, Index, &amp; Max/Min head(sort(TRI87$air_releases)) ## [1] 2 5 5 7 9 10 index &lt;- order(TRI87$air_releases) head(TRI87$FACILITY_NAME[index]) # displays facilities in order of their air releases ## [1] &quot;AIR PRODUCTS MANUFACTURING CORP&quot; &quot;UNITED FIBERS&quot; ## [3] &quot;CLOROX MANUFACTURING CO&quot; &quot;ICI AMERICAS INC WESTERN RESEARCH CENTER&quot; ## [5] &quot;UNION CARBIDE CORP&quot; &quot;SCOTTS-SIERRA HORTICULTURAL PRODS CO INC&quot; i_max &lt;- which.max(TRI87$air_releases) TRI87$FACILITY_NAME[i_max] # was NUMMI at the time ## [1] &quot;TESLA INC&quot; 2.6.5 Factors Factors are vectors with predefined values - Normally used for categorical data. - Built on an integer vector - Levels are the set of predefined values. fruit &lt;- factor(c(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;, &quot;banana&quot;)) fruit # note that levels will be in alphabetical order ## [1] apple banana orange banana ## Levels: apple banana orange class(fruit) ## [1] &quot;factor&quot; typeof(fruit) ## [1] &quot;integer&quot; An equivalent conversion: fruitint &lt;- c(1, 2, 3, 2) # equivalent conversion fruit &lt;- factor(fruitint, labels = c(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;)) str(fruit) ## Factor w/ 3 levels &quot;apple&quot;,&quot;banana&quot;,..: 1 2 3 2 2.6.5.1 Categorical Data and Factors While character data might be seen as categorical (e.g. urban, agricultural, forest land covers), to be used as categorical variables they must be made into factors. grain_order &lt;- c(&quot;clay&quot;, &quot;silt&quot;, &quot;sand&quot;) grain_char &lt;- sample(grain_order, 36, replace = TRUE) grain_fact &lt;- factor(grain_char, levels = grain_order) grain_char ## [1] &quot;sand&quot; &quot;silt&quot; &quot;clay&quot; &quot;sand&quot; &quot;clay&quot; &quot;clay&quot; &quot;sand&quot; &quot;sand&quot; &quot;sand&quot; &quot;clay&quot; &quot;silt&quot; &quot;sand&quot; &quot;clay&quot; ## [14] &quot;clay&quot; &quot;clay&quot; &quot;sand&quot; &quot;clay&quot; &quot;clay&quot; &quot;clay&quot; &quot;sand&quot; &quot;silt&quot; &quot;silt&quot; &quot;clay&quot; &quot;silt&quot; &quot;sand&quot; &quot;sand&quot; ## [27] &quot;clay&quot; &quot;sand&quot; &quot;silt&quot; &quot;sand&quot; &quot;sand&quot; &quot;silt&quot; &quot;sand&quot; &quot;sand&quot; &quot;silt&quot; &quot;silt&quot; grain_fact ## [1] sand silt clay sand clay clay sand sand sand clay silt sand clay clay clay sand clay clay ## [19] clay sand silt silt clay silt sand sand clay sand silt sand sand silt sand sand silt silt ## Levels: clay silt sand To make a categorical variable a factor: fruit &lt;- c(&quot;apples&quot;, &quot;oranges&quot;, &quot;bananas&quot;, &quot;oranges&quot;) farm &lt;- c(&quot;organic&quot;, &quot;conventional&quot;, &quot;organic&quot;, &quot;organic&quot;) ag &lt;- as.data.frame(cbind(fruit, farm)) ag$fruit &lt;- factor(ag$fruit) ag$fruit ## [1] apples oranges bananas oranges ## Levels: apples bananas oranges Factor example sierraFeb$COUNTY &lt;- factor(sierraFeb$COUNTY) str(sierraFeb$COUNTY) ## Factor w/ 20 levels &quot;Amador&quot;,&quot;Butte&quot;,..: 19 14 7 12 12 18 11 2 18 3 ... 2.7 Programming scripts in RStudio Given the exploratory nature of the R language, we sometimes forget that it provides significant capabilities as a programming language where we can solve more complex problems by coding procedures and using logic to control the process and handle a range of possible scenarios. Programming languages are used for a wide range of purposes, from developing operating systems built from low-level code to high-level scripting used to run existing functions in libraries. R and Python are commonly used for scripting, and you may be familiar with using arcpy to script ArcGIS geoprocessing tools. But whether low- or high-level, some common operational structures are used in all computer programming languages: Conditional operations: If a condition is true, do this, and maybe otherwise do something else. if x!=0 {print(1/x)} else {print(\"Can't divide by 0\")} Loops for(i in 1:10) print(paste(i, 1/i)) Functions (defining your own then using it in your main script) turnright &lt;- function(ang){(ang + 90) %% 360} turnright(c(260, 270, 280)) ## [1] 350 0 10 Free-standing scripts and RStudio projects As we move forward, well be wanting to develop complete, free-standing scripts that have all of the needed libraries and data. Your scripts should stand on their own. One example of this that may seem insignificant is using print() statements instead of just naming the object or variable in the console. While that is common in exploratory work, we need to learn to create free-standing scripts. However, free standing still allows for loading libraries of functions well be using. Were still talking about high-level (scripting), not low-level programming, so we can depend on those libraries that any user can access by installing those packages. If we develop our own packages, we just need to provide the user the ability to install those packages. RStudio projects are going to be the way well want to work for the rest of this book, so each time we start looking at a new data set, or even create one from scratch, you need to create a project to go with it, using File/New Project and specify in a new directory (unless you already have data in an existing one), and specifying a location. Most likely the default location to place it in will work but you can change that. In this book, well be making a lot of use of data provided for you from various data packages such as built-in data, palmerpenguins or iGIScData, but they correspond to specific research projects, such as Sierra Climate to which several data frames and spatial data apply. You should create a sierra project for those problems and return to it every time it applies. Well try to include a reminder about using a particular project. In that project, youll build a series of scripts, many of which youll re-use to develop new methods. When youre working on your own project with your own data, which you should store in a data folder inside the project folder. All paths are local, and the default working directory is the project folder, so you can specify \"data/mydata.csv\" as the path to a csv of that name. Its very important to get used to working this way, so start now. 2.7.1 Subsetting with logic Well use a package that includes data from Irizarry, Rafael (2020) Introduction to Data Science section 2.13.1. Identify all states with murder rates  that of Italy. [Start by creating a new murders project.] library(dslabs) data(murders) murder_rate &lt;- murders$total / murders$population * 100000 i &lt;- murder_rate &lt;= 0.71 murders$abb[i] ## [1] &quot;HI&quot; &quot;IA&quot; &quot;NH&quot; &quot;ND&quot; &quot;VT&quot; which [in a new air_quality project] library(readr) TRI87 &lt;- read_csv(&quot;data/TRI_1987_BaySites.csv&quot;) i &lt;- which(TRI87$air_releases &gt; 1e6) TRI87$FACILITY_NAME[i] ## [1] &quot;VALERO REFINING CO-CALI FORNIA BENICIA REFINERY&quot; ## [2] &quot;TESLA INC&quot; ## [3] &quot;TESORO REFINING &amp; MARKETING CO LLC&quot; ## [4] &quot;HGST INC&quot; %in% library(readr) csvPath = system.file(&quot;extdata&quot;,&quot;TRI_1987_BaySites.csv&quot;, package=&quot;iGIScData&quot;) TRI87 &lt;- read_csv(csvPath) i &lt;- TRI87$COUNTY %in% c(&quot;NAPA&quot;,&quot;SONOMA&quot;) TRI87$FACILITY_NAME[i] ## [1] &quot;SAWYER OF NAPA&quot; ## [2] &quot;BERINGER VINEYARDS&quot; ## [3] &quot;CAL-WOOD DOOR INC&quot; ## [4] &quot;SOLA OPTICAL USA INC&quot; ## [5] &quot;KEYSIGHT TECHNOLOGIES INC&quot; ## [6] &quot;SANTA ROSA STAINLESS STEEL&quot; ## [7] &quot;OPTICAL COATING LABORATORY INC&quot; ## [8] &quot;MGM BRAKES&quot; ## [9] &quot;SEBASTIANI VINEYARDS INC, SONOMA CASK CELLARS&quot; 2.7.2 Apply functions There are many apply functions in R, and they largely obviate the need for looping. For instance: apply derives values at margins of rows and columns, e.g. to sum across rows or down columns [create the following in a new generic_methods project which youll use for a variety of generic methods] # matrix apply  the same would apply to data frames matrix12 &lt;- 1:12 dim(matrix12) &lt;- c(3,4) rowsums &lt;- apply(matrix12, 1, sum) colsums &lt;- apply(matrix12, 2, sum) sum(rowsums) ## [1] 78 sum(colsums) ## [1] 78 zero &lt;- sum(rowsums) - sum(colsums) matrix12 ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Apply functions satisfy one of the needs that spreadsheets are used for. Consider how of ten you use sum, mean or similar functions in Excel. sapply sapply applies functions to either: all elements of a vector  unary functions only sapply(1:12, sqrt) ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 3.000000 3.162278 ## [11] 3.316625 3.464102 or all variables of a data frame (not a matrix), where it works much like a column-based apply (since variables are columns) but more easily interpreted without the need of specifying columns with 2: sapply(cars,mean) # same as apply(cars,2,mean) ## speed dist ## 15.40 42.98 temp02 &lt;- c(10.7,9.7,7.7,9.2,7.3,6.7,4.0,5.0,0.9,-1.1,-0.8,-4.4) temp03 &lt;- c(13.1,11.4,9.4,10.9,8.9,8.4,6.7,7.6,2.8,1.6,1.2,-2.1) sapply(as.data.frame(cbind(temp02,temp03)),mean) # has to be a data frame ## temp02 temp03 ## 4.575000 6.658333 While various apply functions are in base R, the purrr package takes these further. See: purrr cheat sheet 2.8 Exercises Assign variables for your name, city, state and zip code, and use paste() to combine them, and assign them to the variable me. What is the class of me? [Do this and the next several problems in your generic_methods project.] Knowing that trigonometric functions require angles (including azimuth directions) to be provided in radians, and that degrees can be converted into radians by dividing by 180 and multiplying that by pi, derive the sine of 30 degrees with an R expression. (Base R knows what pi is, so you can just use pi) If two sides of a right triangle on a map can be represented as \\(dX\\) and \\(dY\\) and the direct line path between them \\(c\\), and the coordinates of 2 points on a map might be given as \\((x1,y1)\\) and \\((x2,y2)\\), with \\(dX=x2-x1\\) and \\(dY=y2-y1\\), use the Pythagorean theorem to derive the distance between them and assign that expression to \\(c\\). You can create a vector uniform random numbers from 0 to 1 using runif(n=30) where n=30 says to make 30 of them. Use the round() function to round each of the values, and provide what you created and explain what happened. Create two vectors of 10 numbers each with the c() function, then assigning to x and y. Then plot(x,y), and provide the three lines of code you used to do the assignment and plot. Change your code from #5 so that one value is NA (entered simply as NA, no quotation marks), and derive the mean value for x. Then add ,na.rm=T to the parameters for mean(). Also do this for y. Describe your results and explain what happens. Create two sequences, a and b, with a all odd numbers from 1 to 99, b all even numbers from 2 to 100. Then derive c through vector division of b/a. Plot a and c together as a scatterplot. Build the sierradata data frame [in a sierra project] from the data at the top of the Matrices section, also given here: temp &lt;- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7, 4.0, 5.0, 0.9, -1.1, -0.8, -4.4) elev &lt;- c(52, 394, 510, 564, 725, 848, 1042, 1225, 1486, 1775, 1899, 2551) lat &lt;- c(39.52, 38.91, 37.97, 38.70, 39.09, 39.25, 39.94, 37.75, 40.35, 39.33, 39.17, 38.21) Create a data frame from it using the same steps, and plot temp against latitude. From the sierradata matrix built with cbind(), derive colmeans using the mean parameter on the columns 2 for apply(). Do the same thing with the sierra data data frame with sapply(). "],["data-abstraction.html", "3 Data Abstraction 3.1 Background: Exploratory Data Analysis 3.2 The Tidyverse and what well explore in this chapter 3.3 Tibbles 3.4 Statistical summary of variables 3.5 Visualizing data with a Tukey box plot 3.6 Database operations with dplyr 3.7 The dot operator 3.8 Exercises 3.9 String Abstraction 3.10 Dates and times with lubridate", " 3 Data Abstraction At this point, weve learned the basics of working with the R language. From here well want to explore how to analyze data, both statistically and spatially. One part of this is abstracting information from existing data sets by selecting variables and observations and summarizing their statistics. Some useful methods for data abstraction can be found in the various packages of the tidyverse which can be included all at once with the tidyverse package. Well start dplyr, which includes an array of data manipulation tools, including select for selecting variables, filter for subsetting observations, summarize for reducing variables to summary statistics, typically stratified by groups, and mutate for creating new variables from mathematical expressions from existing variables. Some dplyr tools such as data joins well look at later in the data transformation chapter. 3.1 Background: Exploratory Data Analysis In 1961, John Tukey proposed a new approach to data analysis, defining it as Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. He followed this up in 1977 with Exploratory Data Analysis. Exploratory data analysis (EDA) in part as an approach to analyzing data via summaries, tables and graphics. The key word is exploratory, in contrast with confirmatory statistics. Both are important, but ignoring exploration is ignoring enlightenment. Some purposes of EDA are: to suggest hypotheses to assess assumptions on which inference will be based to select appropriate inferential statistical tools to guide further data collection These concepts led to the development of S at Bell Labs (John Chambers, 1976), then R, built on clear design and extensive, clear graphics. 3.2 The Tidyverse and what well explore in this chapter The Tidyverse refers to a suite of R packages developed at RStudio (see R Studio and R for Data Science) for facilitating data processing and analysis. While R itself is designed around EDA, the Tidyverse takes it further. Some of the packages in the Tidyverse that are widely used are: dplyr : data manipulation like a database readr : better methods for reading and writing rectangular data tidyr : reorganization methods that extend dplyrs database capabilities purrr : expanded programming toolkit including enhanced apply methods tibble : improved data frame stringr : string manipulation library ggplot2 : graphing system based on the grammar of graphics In this chapter, well be mostly exploring dplyr, with a few other things thrown in like reading data frames with readr. For simplicity, we can just include library(tidyverse) to get everything. 3.3 Tibbles Tibbles are an improved type of data frame part of the Tidyverse serve the same purpose as a data frame, and all data frame operations work Advantages display better can be composed of more complex objects like lists, etc. can be grouped How created Reading from a CSV, using one of a variety of Tidyverse functions similarly named to base functions: read_csv creates a tibble (in general, underscores are used in the Tidyverse) read.csv creates a regular data frame [air_quality project] library(tidyverse) # includes readr, ggplot2, and dplyr which we&#39;ll use in this chapter library(iGIScData) csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_1987_BaySites.csv&quot;, package=&quot;iGIScData&quot;) TRI87 &lt;- read_csv(csvPath) You can also use the tibble() function a &lt;- rnorm(10) b &lt;- runif(10) ab &lt;- tibble(a,b) ab ## # A tibble: 10 x 2 ## a b ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.05 0.902 ## 2 -0.467 0.727 ## 3 -0.817 0.755 ## 4 0.719 0.0337 ## 5 1.40 0.347 ## 6 1.74 0.116 ## 7 -0.668 0.538 ## 8 0.424 0.0276 ## 9 -0.597 0.0523 ## 10 -0.604 0.396 3.3.1 read_csv vs. read.csv You might be tempted to use read.csv from base R They look a lot alike, so you might confuse them You dont need to load library(readr) read.csv fixes some things and that might be desired: problematic field names like MLY-TAVG-NORMAL become MLY.TAVG.NORMAL numbers stored as characters are converted to numbers 01 becomes 1, 02 becomes 2, etc. However, there are potential problems You may not want some of those changes, and want to specify those changes separately There are known problems that read_csv avoids Recommendation: Use read_csv and write_csv. 3.4 Statistical summary of variables A simple statistical summary is very easy to do [eucoak]: summary(eucoakrainfallrunoffTDR) ## site site # date month rain_mm ## Length:90 Min. :1.000 Length:90 Length:90 Min. : 1.00 ## Class :character 1st Qu.:2.000 Class :character Class :character 1st Qu.:16.00 ## Mode :character Median :4.000 Mode :character Mode :character Median :28.50 ## Mean :4.422 Mean :37.99 ## 3rd Qu.:6.000 3rd Qu.:63.25 ## Max. :8.000 Max. :99.00 ## NA&#39;s :18 ## rain_oak rain_euc runoffL_oak runoffL_euc slope_oak ## Min. : 1.00 Min. : 1.00 Min. : 0.000 Min. : 0.00 Min. : 9.00 ## 1st Qu.:16.00 1st Qu.:14.75 1st Qu.: 0.000 1st Qu.: 0.07 1st Qu.:12.00 ## Median :30.50 Median :30.00 Median : 0.450 Median : 1.20 Median :24.50 ## Mean :35.08 Mean :34.60 Mean : 2.032 Mean : 2.45 Mean :21.62 ## 3rd Qu.:50.50 3rd Qu.:50.00 3rd Qu.: 2.800 3rd Qu.: 3.30 3rd Qu.:30.50 ## Max. :98.00 Max. :96.00 Max. :14.000 Max. :16.00 Max. :32.00 ## NA&#39;s :2 NA&#39;s :2 NA&#39;s :5 NA&#39;s :3 ## slope_euc aspect_oak aspect_euc surface_tension_oak surface_tension_euc ## Min. : 9.00 Min. :100.0 Min. :106.0 Min. :37.40 Min. :28.51 ## 1st Qu.:12.00 1st Qu.:143.0 1st Qu.:175.0 1st Qu.:72.75 1st Qu.:32.79 ## Median :23.00 Median :189.0 Median :196.5 Median :72.75 Median :37.40 ## Mean :19.34 Mean :181.9 Mean :191.2 Mean :68.35 Mean :43.11 ## 3rd Qu.:25.00 3rd Qu.:220.0 3rd Qu.:224.0 3rd Qu.:72.75 3rd Qu.:56.41 ## Max. :31.00 Max. :264.0 Max. :296.0 Max. :72.75 Max. :72.75 ## NA&#39;s :22 NA&#39;s :22 ## runoff_rainfall_ratio_oak runoff_rainfall_ratio_euc ## Min. :0.00000 Min. :0.000000 ## 1st Qu.:0.00000 1st Qu.:0.003027 ## Median :0.02046 Median :0.047619 ## Mean :0.05357 Mean :0.065902 ## 3rd Qu.:0.08485 3rd Qu.:0.083603 ## Max. :0.42000 Max. :0.335652 ## NA&#39;s :5 NA&#39;s :3 3.5 Visualizing data with a Tukey box plot ggplot(data = eucoakrainfallrunoffTDR) + geom_boxplot(mapping = aes(x=site, y=runoffL_euc)) 3.6 Database operations with dplyr As part of exploring our data, well typically simplify or reduce it for our purposes. The following methods are quickly discovered to be essential as part of exploring and analyzing data. select rows using logic, such as population &gt; 10000, with filter select variable columns you want to retain with select add new variables and assign their values with mutate sort rows based on a a field with arrange summarize by group 3.6.1 Select, mutate, and the pipe The pipe %&gt;%: Read %&gt;% as and then This is bigger than it sounds and opens up a lot of possibilities. See example below, and observe how the expression becomes several lines long. In the process, well see examples of new variables with mutate and selecting (and in the process ordering) variables. [If you havent already created it, this should be in a eucoak project] runoff &lt;- eucoakrainfallrunoffTDR %&gt;% mutate(Date = as.Date(date,&quot;%m/%d/%Y&quot;), rain_subcanopy = (rain_oak + rain_euc)/2) %&gt;% dplyr::select(site, Date, rain_mm, rain_subcanopy, runoffL_oak, runoffL_euc, slope_oak, slope_euc) runoff ## # A tibble: 90 x 8 ## site Date rain_mm rain_subcanopy runoffL_oak runoffL_euc slope_oak slope_euc ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AB1 2006-11-08 29 29 4.79 6.7 32 31 ## 2 AB1 2006-11-12 22 18.5 3.2 4.3 32 31 ## 3 AB1 2006-11-29 85 65 9.7 16 32 31 ## 4 AB1 2006-12-12 82 87.5 14 14.2 32 31 ## 5 AB1 2006-12-28 43 54 9.75 4.33 32 31 ## 6 AB1 2007-01-29 7 54 1.4 0 32 31 ## 7 AB1 2007-02-09 56 44 10.1 0 32 31 ## 8 AB1 2007-02-13 63 42.5 3.90 1.40 32 31 ## 9 AB1 2007-02-28 NA 56 4.75 8.65 32 31 ## 10 AB1 2007-03-22 NA 2 NA 0.11 32 31 ## # ... with 80 more rows Note: to just rename a variable, use rename instead of mutate. It will stay in position. Helper functions for select() In the select() example above, we listed all of the variables, but there are a variety of helper functions for using logic to specify which variables to select: contains(\"_\") or any substring of interest in the variable name `starts_with(runoff) ends_with(\"euc\") everything() matches() a regular expression num_range(\"x\",1:5) for the common situation where a series of variable names combine a string and a number one_of(myList) for when you have a group of variable names range of variable: e.g. runoffL_oak:slope_euc could have followed rain_subcanopy above all but (-): preface a variable or a set of variabe names with - to select all others 3.6.2 filter filter lets you select observations that meet criteria, similar to an SQL WHERE clause. runoff2007 &lt;- runoff %&gt;% filter(Date &gt;= as.Date(&quot;01/01/2007&quot;, &quot;%m/%d/%Y&quot;)) runoff2007 ## # A tibble: 51 x 8 ## site Date rain_mm rain_subcanopy runoffL_oak runoffL_euc slope_oak slope_euc ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AB1 2007-01-29 7 54 1.4 0 32 31 ## 2 AB1 2007-02-09 56 44 10.1 0 32 31 ## 3 AB1 2007-02-13 63 42.5 3.90 1.40 32 31 ## 4 AB1 2007-02-28 NA 56 4.75 8.65 32 31 ## 5 AB1 2007-03-22 NA 2 NA 0.11 32 31 ## 6 AB1 2007-04-23 NA 33.5 6.94 9.20 32 31 ## 7 AB1 2007-05-05 NA 31 6.34 7.43 32 31 ## 8 AB2 2007-01-29 4 3.5 1.26 0.05 24 25 ## 9 AB2 2007-02-09 37 41.5 6.3 3.3 24 25 ## 10 AB2 2007-02-13 43 49.5 6.78 1.14 24 25 ## # ... with 41 more rows Filtering out NA with !is.na Heres an important one. There are many times you need to avoid NAs. We commonly see summary statistics using na.rm = TRUE in order to ignore NAs when calculating a statistic like mean. To simply filter out NAs from a vector or a variable use a filter: feb_filt &lt;- feb_s %&gt;% filter(!is.na(TEMP)) 3.6.3 Writing a data frame to a csv Lets say you have created a data frame, maybe with read_csv runoff20062007 &lt;- read_csv(csvPath) Then you do some processing to change it, maybe adding variables, reorganizing, etc., and you want to write out your new eucoak, so you just need to use write_csv write_csv(eucoak, \"data/tidy_eucoak.csv\") 3.6.4 Summarize by group Youll find that you need to use this all the time with real data. You have a bunch of data where some categorical variable is defining a grouping, like our site field in the eucoak data. Wed like to just create average slope, rainfall, and runoff for each site. Note that it involves two steps, first defining which field defines the group, then the various summary statistics wed like to store. In this case all of the slopes under oak remain the same for a given site  its a site characteristic  and the same applies to the euc site, so we can just grab the first value (mean would have also worked of course). eucoakSiteAvg &lt;- runoff %&gt;% group_by(site) %&gt;% summarize( rain = mean(rain_mm, na.rm = TRUE), rain_subcanopy = mean(rain_subcanopy, na.rm = TRUE), runoffL_oak = mean(runoffL_oak, na.rm = TRUE), runoffL_euc = mean(runoffL_euc, na.rm = TRUE), slope_oak = first(slope_oak), slope_euc = first(slope_euc) ) eucoakSiteAvg ## # A tibble: 8 x 7 ## site rain rain_subcanopy runoffL_oak runoffL_euc slope_oak slope_euc ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AB1 48.4 43.1 6.80 6.03 32 31 ## 2 AB2 34.1 35.4 4.91 3.65 24 25 ## 3 KM1 48 36.1 1.94 0.592 30.5 25 ## 4 PR1 56.5 37.6 0.459 2.31 27 23 ## 5 TP1 38.4 30.0 0.877 1.66 9 9 ## 6 TP2 34.3 32.9 0.0955 1.53 12 10 ## 7 TP3 32.1 27.8 0.381 0.815 25 18 ## 8 TP4 32.5 35.7 0.231 2.83 12 12 Summarizing by group with TRI data [air_quality project] csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_2017_CA.csv&quot;, package=&quot;iGIScData&quot;) TRI_BySite &lt;- read_csv(csvPath) %&gt;% mutate(all_air = `5.1_FUGITIVE_AIR` + `5.2_STACK_AIR`) %&gt;% filter(all_air &gt; 0) %&gt;% group_by(FACILITY_NAME) %&gt;% summarize( FACILITY_NAME = first(FACILITY_NAME), air_releases = sum(all_air, na.rm = TRUE), mean_fugitive = mean(`5.1_FUGITIVE_AIR`, na.rm = TRUE), LATITUDE = first(LATITUDE), LONGITUDE = first(LONGITUDE)) 3.6.5 Count Count is a simple variant on summarize by group, since the only statistic is the count of events. [eucoak project] tidy_eucoak %&gt;% count(tree) ## # A tibble: 2 x 2 ## tree n ## &lt;chr&gt; &lt;int&gt; ## 1 euc 90 ## 2 oak 90 Another way is to use n(): tidy_eucoak %&gt;% group_by(tree) %&gt;% summarize(n = n()) ## # A tibble: 2 x 2 ## tree n ## &lt;chr&gt; &lt;int&gt; ## 1 euc 90 ## 2 oak 90 3.6.6 Sorting after summarizing Using the marine debris data from NOAA Marine Debris Programs Marine Debris Monitoring and Assessment Project [in a new litter project] shorelineLatLong &lt;- ConcentrationReport %&gt;% group_by(`Shoreline Name`) %&gt;% summarize( latitude = mean((`Latitude Start`+`Latitude End`)/2), longitude = mean((`Longitude Start`+`Longitude End`)/2) ) %&gt;% arrange(latitude) shorelineLatLong ## # A tibble: 38 x 3 ## `Shoreline Name` latitude longitude ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aimee Arvidson 33.6 -118. ## 2 Balboa Pier #2 33.6 -118. ## 3 Bolsa Chica 33.7 -118. ## 4 Junipero Beach 33.8 -118. ## 5 Malaga Cove 33.8 -118. ## 6 Zuma Beach, Malibu 34.0 -119. ## 7 Zuma Beach 34.0 -119. ## 8 Will Rodgers 34.0 -119. ## 9 Carbon Beach 34.0 -119. ## 10 Nicholas Canyon 34.0 -119. ## # ... with 28 more rows 3.7 The dot operator The dot . operator derives from UNIX syntax, and refers to here. For accessing files in the current folder, the path is ./filename A similar specification is used in piped sequences The advantage of the pipe is you dont have to keep referencing the data frame. The dot is then used to connect to items inside the data frame [in air_quality] csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_1987_BaySites.csv&quot;, package=&quot;iGIScData&quot;) TRI87 &lt;- read_csv(csvPath) stackrate &lt;- TRI87 %&gt;% mutate(stackrate = stack_air/air_releases) %&gt;% .$stackrate head(stackrate) ## [1] 0.0000000 0.0000000 0.0000000 0.0000000 0.6666667 1.0000000 3.8 Exercises Create a tibble with 20 rows of two variables norm and unif with norm created with rnorm() and unif created with runif(). [generic_methods] Read in TRI_2017_CA.csv [air_quality] in two ways, as a normal data frame assigned to df and as a tibble assigned to tb. What field names result for whats listed in the CSV as 5.1_FUGITIVE_AIR? Use the summary function to investigate the variables in either the data.frame or tibble you just created. What type of field and what values are assigned to BIA_CODE? Create a boxplot of body_mass_g by species from the penguins data frame in the palmerpenguins package [in a penguins project]. Access the data with data(package = palmerpenguins), and also remember library(ggplot2) or library(tidyverse). Use select, mutate, and the pipe to create a penguinMass tibble where the only original variable retained is species, but with body_mass_kg created as \\(\\frac{1}{1000}\\) the body_mass_g. The statement should start with penguinMass &lt;- penguins and use a pipe plus the other functions after that. Now, also with penguins, create FemaleChinstaps to include only the female Chinstrap penguins. Start with FemaleChinstraps &lt;- penguins %&gt;% Now, summarize by species groups to create mean and standard deviation variables from bill_length_mm, bill_depth_mm, flipper_length_mm, and body_mass_g. Preface the variable names with either avg. or sd. Include na.rm=T with all statistics function calls. Sort the penguins by body_mass_g. 3.9 String Abstraction Character string manipulation is surprisingly critical to data analysis, and so the stringr package was developed to provide a wider array of string processing tools than what is in base R, including fucntions for detecting matches, subsetting strings, managing lengths, replacing substrings with other text, and joining, splitting, and sorting strings. Well look at some of the stringr functions, but a good way to learn about the wide array of functions is through the cheat sheet that can be downloaded from https://www.rstudio.com/resources/cheatsheets/. 3.9.1 Detecting matches These functions look for patterns within existing strings which can then be used subset observations based on those patterns. [These can be investigated in your generic_methods project] str_detect detects patterns in a string, returns true or false if detected str_locate detects patterns in a string, returns start and end position if detected, or NA if not str_which returns the indices of strings that match a pattern str_count counts the number of matches in each string str_detect(fruit,&quot;qu&quot;) ## [1] FALSE FALSE FALSE FALSE fruit[str_detect(fruit,&quot;qu&quot;)] ## character(0) tail(str_locate(fruit, &quot;qu&quot;),15) ## start end ## [1,] NA NA ## [2,] NA NA ## [3,] NA NA ## [4,] NA NA str_which(fruit, &quot;qu&quot;) ## integer(0) fruit[str_which(fruit,&quot;qu&quot;)] ## character(0) str_count(fruit,&quot;qu&quot;) ## [1] 0 0 0 0 3.9.2 Subsetting Strings Subsetting in this case includes its normal use of abstracting the observations specified by a match (similar to a filter for data frames), or just a specified part of a string specified by start and end character positions, or the part of the string that matches an expression. str_sub extracts a part of a string from a start to and end character position str_subset returns the strings that contain a pattern match str_extract returns the first (or if str_extract_all then all matches) pattern matches str_match returns the first (or _all) pattern match as a matrix qfruit &lt;- str_subset(fruit, &quot;q&quot;) qfruit ## character(0) str_sub(qfruit,1,2) ## character(0) str_sub(&quot;94132&quot;,1,2) ## [1] &quot;94&quot; str_extract(qfruit,&quot;[aeiou]&quot;) ## character(0) 3.9.3 String Length The length of strings is often useful in an analysis process, either just knowing the length as an integer, or purposefully increasing or reducing it. str_length simply returns the length of the string as an integer str_pad adds a specified character (typically a space \" \") to either end of a string str_trim removes whitespace from the either end of a string qfruit &lt;- str_subset(fruit,&quot;q&quot;) qfruit ## character(0) str_length(qfruit) ## integer(0) name &lt;- &quot;Inigo Montoya&quot; str_length(name) ## [1] 13 firstname &lt;- str_sub(name,1,str_locate(name,&quot; &quot;)[1]-1) firstname ## [1] &quot;Inigo&quot; lastname &lt;- str_sub(name,str_locate(name,&quot; &quot;)[1]+1,str_length(name)) lastname ## [1] &quot;Montoya&quot; str_pad(qfruit,10,&quot;both&quot;) ## character(0) str_trim(str_pad(qfruit,10,&quot;both&quot;),&quot;both&quot;) ## character(0) 3.9.4 Replacing substrings with other text (mutating strings) These methods range from converting case to replace substrings. str_to_lower converts strings to lower case str_to_upper converts strings to upper case str_to_title capitalizes strings (makes the first character of each word upper case) str_sub a special use of this function to replace substrings with a specified string str_replace replaces the first matched pattern (or all with str_replace_all) with a specified string str_to_lower(name) ## [1] &quot;inigo montoya&quot; str_to_upper(name) ## [1] &quot;INIGO MONTOYA&quot; str_to_title(&quot;for whom the bell tolls&quot;) ## [1] &quot;For Whom The Bell Tolls&quot; str_sub(name,1,str_locate(name,&quot; &quot;)-1) &lt;- &quot;Diego&quot; str_replace(qfruit,&quot;q&quot;,&quot;z&quot;) ## character(0) 3.9.5 Concatenating and splitting One very common string function is that to concatenate strings, and somewhat less common though useful is splitting them using a key separator like space, comma, or line end. One use of using str_c in the example below is to create a comparable join field based on a numeric character string that might need a zero or something at the left or right. str_c The paste() function in base R will work but you might want the default separator setting to be \"\" instead of \" , so str_c is just paste with a default\" separator, but you can also use \" \". str_split splits a string into parts based upon the detection of a specified separator like space, comma, or line end str_split(&quot;for whom the bell tolls&quot;, &quot; &quot;) ## [[1]] ## [1] &quot;for&quot; &quot;whom&quot; &quot;the&quot; &quot;bell&quot; &quot;tolls&quot; str_c(&quot;for&quot;,&quot;whom&quot;,&quot;the&quot;,&quot;bell&quot;,&quot;tolls&quot;,sep=&quot; &quot;) ## [1] &quot;for whom the bell tolls&quot; csvPath &lt;- system.file(&quot;extdata&quot;,&quot;CA_MdInc.csv&quot;,package=&quot;iGIScData&quot;) CA_MdInc &lt;- read_csv(csvPath) join_id &lt;- str_c(&quot;0&quot;,CA_MdInc$NAME) # could also use str_pad(CA_MdInc$NAME,1,side=&quot;left&quot;,pad=&quot;0&quot;) head(CA_MdInc) ## # A tibble: 6 x 3 ## trID NAME HHinc2016 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6001400100 60014001 177417 ## 2 6001400200 60014002 153125 ## 3 6001400300 60014003 85313 ## 4 6001400400 60014004 99539 ## 5 6001400500 60014005 83650 ## 6 6001400600 60014006 61597 head(join_id) ## [1] &quot;060014001&quot; &quot;060014002&quot; &quot;060014003&quot; &quot;060014004&quot; &quot;060014005&quot; &quot;060014006&quot; 3.10 Dates and times with lubridate Makes it easy to work with dates and times. Can parse many forms Well look at more with time series See the cheat sheet for more information, but the following examples may demonstrate that its pretty easy to use, and does a good job of making your job easier. library(lubridate) dmy(&quot;20 September 2020&quot;) ## [1] &quot;2020-09-20&quot; dmy_hm(&quot;20 September 2020 10:45&quot;) ## [1] &quot;2020-09-20 10:45:00 UTC&quot; mdy_hms(&quot;September 20, 2020 10:48&quot;) ## [1] &quot;2020-09-20 20:10:48 UTC&quot; mdy_hm(&quot;9/20/20 10:50&quot;) ## [1] &quot;2020-09-20 10:50:00 UTC&quot; mdy(&quot;9.20.20&quot;) ## [1] &quot;2020-09-20&quot; start704 &lt;- dmy_hm(&quot;24 August 2020 16:00&quot;) end704 &lt;- mdy_hm(&quot;12/18/2020 4:45 pm&quot;) year(start704) ## [1] 2020 month(start704) ## [1] 8 day(end704) ## [1] 18 hour(end704) ## [1] 16 end704-start704 ## Time difference of 116.0312 days as_date(end704) ## [1] &quot;2020-12-18&quot; hms::as_hms(end704) ## 16:45:00 Note the use of :: after the package Sometimes you need to specify the package and function name this way, for instance if more than one package has a function of the same name. dplyr::select(...) "],["visualization.html", "4 Visualization 4.1 ggplot2 4.2 Plotting one variable 4.3 Plotting two variables 4.4 Color systems 4.5 Titles and subtitles 4.6 Pairs Plot 4.7 Exercises", " 4 Visualization In this section well explore visualization methods in R. Visualization has been a key element of R since its inception, since visualization is central to the exploratory philosophy of the language. The base plot system generally does a good job in coming up with the most likely graphical output based on the data you provide. plot(penguins$body_mass_g, penguins$flipper_length_mm) Figure 4.1: Flipper length by species plot(penguins$species, penguins$flipper_length_mm) Figure 4.2: Flipper length by species 4.1 ggplot2 Well mostly focus however on gpplot2, based on the Grammar of Graphics because it provides considerable control over your graphics while remaining fairly easily readable, as long as you buy into its grammar. ggplot2 looks at three aspects of a graph: data : where are the data coming from? geometry : what type of graph are we creating? aesthetics : what choices can we make about symbology and how do we connect symbology to data? See https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf The ggplot2 system provides plots of single and multiple variables, using various coordinate systems (including geographic). 4.2 Plotting one variable continuous histograms density plots dot plots discrete bar [Create a new NDVI project] library(iGIScData) library(tidyverse) summary(XSptsNDVI) ## DistNtoS elevation vegetation geometry NDVIgrowing ## Min. : 0.0 Min. :1510 Length:29 Length:29 Min. :0.3255 ## 1st Qu.: 37.0 1st Qu.:1510 Class :character Class :character 1st Qu.:0.5052 ## Median :175.0 Median :1511 Mode :character Mode :character Median :0.6169 ## Mean :164.7 Mean :1511 Mean :0.5901 ## 3rd Qu.:275.5 3rd Qu.:1511 3rd Qu.:0.6768 ## Max. :298.8 Max. :1511 Max. :0.7683 ## NDVIsenescent ## Min. :0.1402 ## 1st Qu.:0.2418 ## Median :0.2817 ## Mean :0.3662 ## 3rd Qu.:0.5407 ## Max. :0.7578 ggplot(XSptsNDVI, aes(vegetation)) + geom_bar() 4.2.1 Histogram First, to prepare the data, we need to use a pivot_longer on XSptsNDVI: XSptsPheno &lt;- XSptsNDVI %&gt;% filter(vegetation != &quot;pine&quot;) %&gt;% pivot_longer(cols = starts_with(&quot;NDVI&quot;), names_to = &quot;phenology&quot;, values_to = &quot;NDVI&quot;) %&gt;% mutate(phenology = str_sub(phenology, 5, str_length(phenology))) XSptsPheno &lt;- read_csv(&quot;data/XSptsPheno.csv&quot;) XSptsPheno %&gt;% ggplot(aes(NDVI)) + geom_histogram(binwidth=0.05) Figure 4.3: Distribution of NDVI, Knuthson Meadow Normal histogram: easier to visualize the distribution, see modes [sierra] sierraData %&gt;% ggplot(aes(TEMPERATURE)) + geom_histogram(fill=&quot;dark green&quot;) Figure 4.4: Distribution of Average Monthly Temperatures, Sierra Nevada Cumulative histogram with proportions: easier to see percentiles, median n &lt;- length(sierraData$TEMPERATURE) sierraData %&gt;% ggplot(aes(TEMPERATURE)) + geom_histogram(aes(y=cumsum(..count..)/n), fill=&quot;dark goldenrod&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 4.5: Cumulative Distribution of Average Monthly Temperatures, Sierra Nevada 4.2.2 Density Plot Density represents how much out of the total. The total area (sum of widths of bins times densities of that bin) adds up to 1. [NDVI] XSptsPheno %&gt;% ggplot(aes(NDVI)) + geom_density() Figure 4.6: Density plot of NDVI, Knuthson Meadow Note that NDVI values are &lt;1 so bins are very small numbers, so in this case densities can be &gt;1. Using alpha and mapping phenology as fill color. This illustrates two useful ggplot methods: mapping a variable (phenology) to an aesthetic property (fill color of the density polygon) setting a a property (alpha = 0.2) to all polygons of the density plot. The alpha channel of colors defines its opacity, from invisible (0) to opaque (1) so is commonly used to set as its reverse, transparency. XSptsPheno %&gt;% ggplot(aes(NDVI, fill=phenology)) + geom_density(alpha=0.2) [eucoak] tidy_eucoak %&gt;% ggplot(aes(log(runoff_L),fill=tree)) + geom_density(alpha=0.2) Figure 4.7: Runoff under Eucalyptus and Oak in Bay Area sites 4.2.3 boxplot ggplot(data = tidy_eucoak) + geom_boxplot(aes(x = site, y = runoff_L)) Figure 4.8: Runoff under Eucalyptus and Oak, Bay Area Sites Get color from tree within aes() ggplot(data = tidy_eucoak) + geom_boxplot(aes(x=site, y=runoff_L, color=tree)) Figure 4.9: Runoff at Bay Area Sites, colored as Eucalyptus and Oak Visualizing soil CO_2_ data with a Tukey box plot [soilCO2] soilCO2 &lt;- soilCO2_97 soilCO2$SITE &lt;- factor(soilCO2$SITE) # in order to make the numeric field a factor ggplot(data = soilCO2, mapping = aes(x = SITE, y = `CO2%`)) + geom_boxplot() Figure 4.10: Visualizing soil CO_2_ data with a Tukey box plot 4.3 Plotting two variables 4.3.1 Two continuous variables [sierra] Weve looked at this before  the scatterplot ggplot(data=sierraFeb) + geom_point(mapping = aes(TEMPERATURE, ELEVATION)) Figure 4.11: Scatter plot of February temperature vs elevation The aes (aesthetics) function specifies the variables to use as x and y coordinates geom_point creates a scatter plot of those coordinate points Set color for all (not in aes()) ggplot(data=sierraFeb) + geom_point(aes(TEMPERATURE, ELEVATION), color=&quot;blue&quot;) color is defined outside of aes, so is applies to all points. mapping is first argument of geom_point, so mapping = is not needed. 4.3.2 Two variables, one discrete [eucoak] ggplot(tidy_eucoak) + geom_bar(aes(site, runoff_L), stat=&quot;identity&quot;) Figure 4.12: Two variables, one discrete 4.4 Color systems You can find a lot about color systems. See these sources: http://sape.inf.usi.ch/quick-reference/ggplot2/colour http://applied-r.com/rcolorbrewer-palettes/ 4.4.1 Color from variable, in aesthetics In this graph, color is defined inside aes, so is based on COUNTY [sierra] ggplot(data=sierraFeb) + geom_point(aes(TEMPERATURE, ELEVATION, color=COUNTY)) Figure 4.13: Color set within aes() Plotting lines using the same x,y in aesthetics sierraFeb %&gt;% ggplot(aes(TEMPERATURE,ELEVATION)) + geom_point(color=&quot;blue&quot;) + geom_line(color=&quot;red&quot;) Figure 4.14: Using aesthetics settings for both points and lines Note the use of pipe to start with the data then apply ggplot. [generic_methods] River map &amp; profile x &lt;- c(1000, 1100, 1300, 1500, 1600, 1800, 1900) y &lt;- c(500, 700, 800, 1000, 1200, 1300, 1500) z &lt;- c(0, 1, 2, 5, 25, 75, 150) d &lt;- rep(NA, length(x)) longd &lt;- rep(NA, length(x)) s &lt;- rep(NA, length(x)) for(i in 1:length(x)){ if(i==1){longd[i] &lt;- 0; d[i] &lt;-0} else{ d[i] &lt;- sqrt((x[i]-x[i-1])^2 + (y[i]-y[i-1])^2) longd[i] &lt;- longd[i-1] + d[i] s[i-1] &lt;- (z[i]-z[i-1])/d[i]}} longprofile &lt;- bind_cols(x=x,y=y,z=z,d=d,longd=longd,s=s) ggplot(longprofile, aes(x,y)) + geom_line(mapping=aes(col=s), size=1.2) + geom_point(mapping=aes(col=s, size=z)) + coord_fixed(ratio=1) + scale_color_gradient(low=&quot;green&quot;, high=&quot;red&quot;) + ggtitle(&quot;Simulated river path, elevations and slopes&quot;) Figure 4.15: Longitudinal Profiles ggplot(longprofile, aes(longd,z)) + geom_line(aes(col=s), size=1.5) + geom_point() + scale_color_gradient(low=&quot;green&quot;, high=&quot;red&quot;) + ggtitle(&quot;Elevation over longitudinal distance upstream&quot;) Figure 4.16: Longitudinal Profiles ggplot(longprofile, aes(longd,s)) + geom_point(aes(col=s), size=3) + scale_color_gradient(low=&quot;green&quot;, high=&quot;red&quot;) + ggtitle(&quot;Slope over longitudinal distance upstream&quot;) Figure 4.17: Slope by longitudinal distance 4.4.2 Trend line [sierra] sierraFeb %&gt;% ggplot(aes(TEMPERATURE,ELEVATION)) + geom_point(color=&quot;blue&quot;) + geom_smooth(color=&quot;red&quot;, method=&quot;lm&quot;) Figure 4.18: Trend line using geom_smooth with a linear model 4.4.3 General symbology A useful vignette accessed by vignette(\"ggplot2-specs\") lets you see aesthetic specifications for symbols, including: Color &amp; fill Lines line type, size, ends Polygon border color, linetype, size fill Points shape size color &amp; fill stroke Text font face &amp; size justification 4.4.3.1 Categorical symbology One example of a Big Data resource is EPAs Toxic Release Inventory [air_quality] that tracks releases from a wide array of sources, from oil refineries on down. One way of dealing with big data in terms of exploring meaning is to use symbology to try to make sense of it. csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_2017_CA.csv&quot;, package=&quot;iGIScData&quot;) TRI &lt;- read_csv(csvPath) %&gt;% filter(`5.1_FUGITIVE_AIR` &gt; 100 &amp; `5.2_STACK_AIR` &gt; 100) ggplot(data = TRI, aes(log(`5.2_STACK_AIR`), log(`5.1_FUGITIVE_AIR`), color = INDUSTRY_SECTOR)) + geom_point() Figure 4.19: EPA Toxic Release Inventory, as a big data set needing symbology clarification 4.4.3.2 Graphs from grouped data [NDVI] XSptsPheno %&gt;% ggplot() + geom_point(aes(elevation, NDVI, shape=vegetation, color = phenology), size = 3) + geom_smooth(aes(elevation, NDVI, color = phenology), method=&quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 4.20: NDVI symbolized by vegetation in two seasons [eucoak] ggplot(data = tidy_eucoak) + geom_point(mapping = aes(x = rain_mm, y = runoff_L, color = tree)) + geom_smooth(mapping = aes(x = rain_mm, y= runoff_L, color = tree), method = &quot;lm&quot;) + scale_color_manual(values = c(&quot;seagreen4&quot;, &quot;orange3&quot;)) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 4.21: Eucalyptus and Oak: rainfall and runoff 4.4.3.3 Faceted graphs This is another option to displaying groups of data, with parallel graphs ggplot(data = tidy_eucoak) + geom_point(aes(x=rain_mm,y=runoff_L)) + geom_smooth(aes(x=rain_mm,y=runoff_L), method=&quot;lm&quot;) + facet_grid(tree ~ .) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 4.22: Faceted graph alternative 4.5 Titles and subtitles ggplot(data = tidy_eucoak) + geom_point(aes(x=rain_mm,y=runoff_L, color=tree)) + geom_smooth(aes(x=rain_mm,y=runoff_L, color=tree), method=&quot;lm&quot;) + scale_color_manual(values=c(&quot;seagreen4&quot;,&quot;orange3&quot;)) + labs(title=&quot;rainfall ~ runoff&quot;, subtitle=&quot;eucalyptus &amp; oak sites, 2016&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 4.23: Titles added 4.6 Pairs Plot [sierra] Pairs plots are an excellent exploratory tool to see which variables are correlated. Since only continuous data are useful for this, and since pairs plots can quickly get overly complex, its good to use dplyr::select to select the continuous variables, or maybe use a helper function like is.numeric with dplyr::select_if: sierraFeb %&gt;% dplyr::select_if(is.numeric) %&gt;% # dplyr::select(LATITUDE, ELEVATION, TEMPERATURE, PRECIPITATION) %&gt;% pairs() Figure 4.24: Pairs plot for Sierra Nevada stations variables 4.7 Exercises Create a bar graph of the counts of the species in the penguins data frame. What can you say about what it shows? Use bind_cols in dplyr to create a tibble from built-in vectors state.abb and state.region, then use ggplot with geom_bar to create a bar graph of the four regions. [generic_methods] Convert the built-in time series treering into a tibble trusing the tibble() functions with the single variable assigned as treering = treering, then create a histogram, using that tibble and variable for the data and x settings needed. Attach a screen capture of the histogram. Start by clearing your environment with the broom icon in the Environment tab, then well create two tibbles: Create a new tibble st using bind_cols with Name=state.name, Abb=state.abb, Region=state.region, and as_tibble(state.x77). Note that this works since all of the parts are sorted by state. Then use the save button in the Environment tab to save st as Q4.RData, and attach that for your answer. From st, create a density plot from the variable Frost (number of days with frost for that state). Attach that plot, and answer: approximately what is the modal value? From st create a a boxplot of Area by Region. Which region has the highest and which has the lowest median Area? Do the same for Frost. From st, compare murder rate (y=Murder) to Frost (x) in a scatter plot, colored by Region. Add a trend line (smooth) with method=lm to your scatterplot, not colored by Region (but keep the points colored by Region). What can you say about what this graph is showing you? Add a title to your graph. Change your scatterplot to place labels using the Abb variable (still colored by Region) using geom_label(aes(label=Abb, col=Region)). Any observations about outliers? "],["data-transformation.html", "5 Data Transformation 5.1 Data joins 5.2 Set Operations 5.3 Binding Rows and Columns 5.4 Pivotting data frames", " 5 Data Transformation The goal of this section is to continue where we started in the earlier chapter on data abstraction with dplyr to look at more transformational functions, and tidyr adds other tools like pivot tables. dplyr tools: joins: left_join, right_join, inner_join, full_join, semi_join, anti_join set operations: intersect, union, setdiff binding rows and columns: bind_cols, bind_rows tidyr tools: pivot tables: pivot_longer, pivot_wider The term data wrangling has been used for what were doing with these tools, and the relevant cheat sheet is actually called Data Wrangling https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf 5.1 Data joins To bring in variables from another data frame based on a common join field. There are multiple types of joins. Probably the most common is left_join since it starts from the data frame (or sf) you want to continue working with and bring in data from an additional source. Youll retain all records of the first data set. For any non-matches, NA is assigned. [air_quality] library(tidyverse) library(iGIScData) library(sf) csvPath &lt;- system.file(&quot;extdata&quot;, &quot;CA_MdInc.csv&quot;, package = &quot;iGIScData&quot;) income &lt;- read_csv(csvPath) %&gt;% dplyr::select(trID, HHinc2016) %&gt;% mutate(HHinc2016 = as.numeric(HHinc2016), joinid = str_c(&quot;0&quot;, trID)) %&gt;% dplyr::select(joinid, HHinc2016) census &lt;- BayAreaTracts %&gt;% left_join(income, by = c(&quot;FIPS&quot; = &quot;joinid&quot;)) %&gt;% dplyr::select(FIPS, POP12_SQMI, POP2012, HHinc2016) head(census %&gt;% st_set_geometry(NULL)) ## FIPS POP12_SQMI POP2012 HHinc2016 ## 1 06001400100 1118.797 2976 177417 ## 2 06001400200 9130.435 2100 153125 ## 3 06001400300 11440.476 4805 85313 ## 4 06001400400 14573.077 3789 99539 ## 5 06001400500 15582.609 3584 83650 ## 6 06001400600 13516.667 1622 61597 Other joins are: right_join where you end up retaining all the rows of the second data set and NA is assigned to non-matches inner_join where you only retain records for matches full_join where records are retained for both sides, and NAs assigned to non-matches Right join example We need to join NCDC monthly climate data for all California weather stations to a selection of 82 stations that are in the Sierra. The monthly data has 12 rows (1/month) for each station The right_join gets all months for all stations, so we weed out the non-Sierra stations by removing NAs from a field only with Sierra station data [sierra] sierra &lt;- right_join(sierraStations, CA_ClimateNormals, by=&quot;STATION&quot;) %&gt;% filter(!is.na(STATION_NA)) %&gt;% dplyr::select(-STATION_NA) head(sierra %&gt;% filter(DATE == &quot;01&quot;) %&gt;% dplyr::select(NAME, ELEVATION, `MLY-TAVG-NORMAL`), n=10) ## # A tibble: 10 x 3 ## NAME ELEVATION `MLY-TAVG-NORMAL` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GROVELAND 2, CA US 853. 5.6 ## 2 CANYON DAM, CA US 1390. 0.2 ## 3 KERN RIVER PH 3, CA US 824. 7.6 ## 4 DONNER MEMORIAL ST PARK, CA US 1810. -1.9 ## 5 BOWMAN DAM, CA US 1641. 3 ## 6 BRUSH CREEK RANGER STATION, CA US 1085. NA ## 7 GRANT GROVE, CA US 2012. 1.9 ## 8 LEE VINING, CA US 2072. -1.2 ## 9 OROVILLE MUNICIPAL AIRPORT, CA US 57.9 7.7 ## 10 LEMON COVE, CA US 156. 8.6 The exact same thing however could be accomplished with an inner_join and it doesnt required removing the NAs: sierraAlso &lt;- inner_join(sierraStations, CA_ClimateNormals, by=&quot;STATION&quot;) %&gt;% dplyr::select(-STATION_NA) 5.2 Set Operations Set operations compare two data frames (or vectors) to handle observations or rows that are the same for each, or not the same. The three set methods are: dplyr::intersect(x,y) retains rows that appear in both x and y dplyr::union(x,y) retains rows that appear in either or both of x and y dplyr::setdiff(x,y) retains rows that appear in x but not in y [generic_methods] squares &lt;- (1:10)^2 evens &lt;- seq(0,100,2) squares ## [1] 1 4 9 16 25 36 49 64 81 100 evens ## [1] 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 ## [24] 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 ## [47] 92 94 96 98 100 intersect(squares,evens) ## [1] 4 16 36 64 100 sort(union(squares,evens)) ## [1] 0 1 2 4 6 8 9 10 12 14 16 18 20 22 24 25 26 28 30 32 34 36 38 ## [24] 40 42 44 46 48 49 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 81 ## [47] 82 84 86 88 90 92 94 96 98 100 sort(setdiff(squares,evens)) ## [1] 1 9 25 49 81 5.3 Binding Rows and Columns These dplyr functions are similar to cbind and rbind in base R, but always creates data frames. For instance, cbind usually creates matrices, and make all vectors the same class. Note that in bind_cols, the order of data in rows must be the same. states &lt;- bind_cols(abb=state.abb, name=state.name, region=state.region, state.x77) head(states) ## # A tibble: 6 x 11 ## abb name region Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost Area ## &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AL Alabama South 3615 3624 2.1 69.0 15.1 41.3 20 50708 ## 2 AK Alaska West 365 6315 1.5 69.3 11.3 66.7 152 566432 ## 3 AZ Arizona West 2212 4530 1.8 70.6 7.8 58.1 15 113417 ## 4 AR Arkansas South 2110 3378 1.9 70.7 10.1 39.9 65 51945 ## 5 CA California West 21198 5114 1.1 71.7 10.3 62.6 20 156361 ## 6 CO Colorado West 2541 4884 0.7 72.1 6.8 63.9 166 103766 To compare, note that cbind converts numeric fields to character when any other field is character, and character fields are converted to character integers where there are any repeats, which would require manipulating them into factors: states &lt;- as_tibble(cbind(abb=state.abb, name=state.name, region=state.region, division=state.division, state.x77)) head(states) ## # A tibble: 6 x 12 ## abb name region division Population Income Illiteracy `Life Exp` Murder `HS Grad` Frost ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AL Alab~ 2 4 3615 3624 2.1 69.05 15.1 41.3 20 ## 2 AK Alas~ 4 9 365 6315 1.5 69.31 11.3 66.7 152 ## 3 AZ Ariz~ 4 8 2212 4530 1.8 70.55 7.8 58.1 15 ## 4 AR Arka~ 2 5 2110 3378 1.9 70.66 10.1 39.9 65 ## 5 CA Cali~ 4 9 21198 5114 1.1 71.71 10.3 62.6 20 ## 6 CO Colo~ 4 8 2541 4884 0.7 72.06 6.8 63.9 166 ## # ... with 1 more variable: Area &lt;chr&gt; 5.4 Pivotting data frames Pivot tables are a popular tool in Excel, allowing you to transform your data to be more useful in a particular analysis. A common need to pivot is 2+ variables with the same data where the variable name should be a factor. Tidyr has pivot_wider and pivot_longer. pivot_wider pivots rows into variables. pivot_longer pivots variables into rows, creating factors. In our meadows study cross-section created by intersecting normalized difference vegetation index (NDVI) values from multispectral drone imagery with surveyed elevation and vegetation types (xeric, mesic, and hydric), we have fields NDVIgrowing from a July 2019 growing season and NDVIsenescent from a September 2020 dry season, but would like growing and senescent to be factors with a single NDVI variable. This is how we used pivot_longer to accomplish this, using data from the iGIScData data package [NDVI]: XSptsPheno &lt;- XSptsNDVI %&gt;% pivot_longer(cols = starts_with(&quot;NDVI&quot;), names_to = &quot;phenology&quot;, values_to = &quot;NDVI&quot;) %&gt;% mutate(phenology = str_sub(phenology, 5, str_length(phenology))) Then to do the opposite use pivot_wider: XSptsPheno %&gt;% pivot_wider(names_from = phenology, names_prefix = &quot;NDVI&quot;, values_from = NDVI) ## # A tibble: 29 x 6 ## DistNtoS elevation vegetation geometry NDVIgrowing NDVIsenescent ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1510. Artemisia c(718649.456, 4397466.714) 0.326 0.140 ## 2 16.7 1510. mixed graminoid c(718649.4309, 4397450.077) 0.627 0.259 ## 3 28.6 1510. mixed graminoid c(718649.413, 4397438.222) 0.686 0.329 ## 4 30.5 1510. mixed graminoid c(718649.4101, 4397436.33) 0.668 0.282 ## 5 31.1 1510. mixed graminoid c(718649.4092, 4397435.732) 0.655 0.266 ## 6 33.4 1510. mixed graminoid c(718649.4058, 4397433.441) 0.617 0.274 ## 7 35.6 1510. mixed graminoid c(718649.4025, 4397431.249) 0.623 0.275 ## 8 37 1510. mixed graminoid c(718649.4004, 4397429.854) 0.589 0.242 ## 9 74 1510. mixed graminoid c(718649.3448, 4397392.994) 0.641 0.325 ## 10 101 1510. mixed graminoid c(718649.3042, 4397366.097) 0.558 0.312 ## # ... with 19 more rows XSptsPheno ## # A tibble: 58 x 6 ## DistNtoS elevation vegetation geometry phenology NDVI ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0 1510. Artemisia c(718649.456, 4397466.714) growing 0.326 ## 2 0 1510. Artemisia c(718649.456, 4397466.714) senescent 0.140 ## 3 16.7 1510. mixed graminoid c(718649.4309, 4397450.077) growing 0.627 ## 4 16.7 1510. mixed graminoid c(718649.4309, 4397450.077) senescent 0.259 ## 5 28.6 1510. mixed graminoid c(718649.413, 4397438.222) growing 0.686 ## 6 28.6 1510. mixed graminoid c(718649.413, 4397438.222) senescent 0.329 ## 7 30.5 1510. mixed graminoid c(718649.4101, 4397436.33) growing 0.668 ## 8 30.5 1510. mixed graminoid c(718649.4101, 4397436.33) senescent 0.282 ## 9 31.1 1510. mixed graminoid c(718649.4092, 4397435.732) growing 0.655 ## 10 31.1 1510. mixed graminoid c(718649.4092, 4397435.732) senescent 0.266 ## # ... with 48 more rows XSptsPheno %&gt;% ggplot() + geom_point(aes(elevation, NDVI, shape=vegetation, color = phenology), size = 5) + geom_smooth(aes(elevation, NDVI, color = phenology), method=&quot;lm&quot;) Pivots turn out to be commonly useful. Runoff graphing from the Eucalyptus/Oak study also benefitted from a pivot_longer [eucoak]: eucoakrainfallrunoffTDR %&gt;% pivot_longer(cols = starts_with(&quot;runoffL&quot;), names_to = &quot;tree&quot;, values_to = &quot;runoffL&quot;) %&gt;% mutate(tree = str_sub(tree, str_length(tree)-2, str_length(tree))) %&gt;% ggplot() + geom_boxplot(aes(site, runoffL)) + facet_grid(tree ~ .) Combining a pivot with bind_rows to create a runoff/rainfall scatterplot colored by tree runoffPivot &lt;- eucoakrainfallrunoffTDR %&gt;% pivot_longer(cols = starts_with(&quot;runoffL&quot;), names_to = &quot;tree&quot;, values_to = &quot;runoffL&quot;) %&gt;% mutate(tree = str_sub(tree, str_length(tree)-2, str_length(tree)), Date = as.Date(date, &quot;%m/%d/%Y&quot;)) euc &lt;- runoffPivot %&gt;% filter(tree == &quot;euc&quot;) %&gt;% mutate(rain_subcanopy = rain_euc, slope = slope_euc, aspect = aspect_euc, surface_tension = surface_tension_euc, runoff_rainfall_ratio = runoff_rainfall_ratio_euc) %&gt;% dplyr::select(site, `site #`, tree, Date, month, rain_mm, rain_subcanopy, slope, aspect, runoffL, surface_tension, runoff_rainfall_ratio) oak &lt;- runoffPivot %&gt;% filter(tree == &quot;oak&quot;) %&gt;% mutate(rain_subcanopy = rain_oak, slope = slope_oak, aspect = aspect_oak, surface_tension = surface_tension_oak, runoff_rainfall_ratio = runoff_rainfall_ratio_oak) %&gt;% dplyr::select(site, `site #`, tree, Date, month, rain_mm, rain_subcanopy, slope, aspect, runoffL, surface_tension, runoff_rainfall_ratio) bind_rows(euc, oak) %&gt;% ggplot() + geom_point(mapping = aes(x = rain_mm, y = runoffL, color = tree)) + geom_smooth(mapping = aes(x = rain_mm, y= runoffL, color = tree), method = &quot;lm&quot;) + scale_color_manual(values = c(&quot;seagreen4&quot;, &quot;orange3&quot;)) Using pivot_wider with traffic stop data (from Claudia Engel (2021) Data Wrangling) library(lubridate) csvPath &lt;- system.file(&quot;extdata&quot;,&quot;MS_trafficstops_bw_age.csv&quot;, package=&quot;iGIScData&quot;) trafficStops &lt;- read_csv(csvPath) %&gt;% mutate(year = year(stop_date)) trafficCounts &lt;- trafficStops %&gt;% count(year,violation_raw) trafficCounts %&gt;% pivot_wider(names_from = year, values_from = n) ## # A tibble: 19 x 5 ## violation_raw `2013` `2014` `2015` `2016` ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 ?? 5018 4368 2935 682 ## 2 Careless driving 607 561 709 270 ## 3 Child or youth restraint not used properly as required 1313 873 709 286 ## 4 Driving while license suspended 3258 2306 2455 516 ## 5 Driving wrong way 40 33 38 14 ## 6 Expired or no non-commercial driver license or permit 1433 984 1166 358 ## 7 Failure to comply with financial responsibility law 1 1 NA NA ## 8 Failure to maintain required liability insurance 7096 5193 7889 2874 ## 9 Failure to obey sign or traffic control device 607 356 388 122 ## 10 Failure to yield right of way (FTY ROW) 292 206 203 65 ## 11 Following too closely 179 117 153 41 ## 12 Improper passing 178 155 178 71 ## 13 Improper turn 65 54 67 22 ## 14 Operating without equipment as required by law 967 744 964 425 ## 15 Other (non-mapped) 421 289 365 129 ## 16 Reckless driving 233 229 284 125 ## 17 Seat belt not used properly as required 7027 5768 4912 2547 ## 18 Speeding 713 948 978 167 ## 19 Speeding - Regulated or posted speed limit and actual speed 39818 31888 38721 15044 Note that this table is not tidy, but provides a useful table for a report. "],["spatial-data-and-maps.html", "6 Spatial Data and Maps 6.1 Spatial Data 6.2 Raster GIS in R 6.3 ggplot2 for maps 6.4 tmap 6.5 Interactive Maps 6.6 Exercises", " 6 Spatial Data and Maps Well explore the basics of simple features (sf) for building spatial datasets, then some common mapping methods: ggplot2 tmap leaflet the base plot system occasionally 6.1 Spatial Data To work with spatial data requires extending R to deal with it using packages. Many have been developed, but the field is starting to mature using international open GIS standards. sp (until recently, the dominant library of spatial tools) Includes functions for working with spatial data Includes spplot to create maps Also needs rgdal package for readOGR  reads spatial data frames. sf (Simple Features) ISO 19125 standard for GIS geometries Also has functions for working with spatial data, but clearer to use. Doesnt need many additional packages, though you may still need rgdal installed for some tools you want to use. Replacing sp and spplot though youll still find them in code. Well give it a try Works with ggplot2 and tmap for nice looking maps. Cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/sf.pdf 6.1.0.1 simple feature geometry sfg and simple feature column sfc 6.1.1 Examples of simple geometry building in sf sf functions have the pattern st_* st means space and time See Geocomputation with R at https://geocompr.robinlovelace.net/ or https://r-spatial.github.io/sf/ for more details, but heres an example of manual feature creation of sf geometries (sfg): library(tidyverse) library(sf) library(iGIScData) [As usual, go to the relevant project, in this case generic_methods] library(sf) eyes &lt;- st_multipoint(rbind(c(1,5), c(3,5))) nose &lt;- st_point(c(2,4)) mouth &lt;- st_linestring(rbind(c(1,3),c(3, 3))) border &lt;- st_polygon(list(rbind(c(0,5), c(1,2), c(2,1), c(3,2), c(4,5), c(3,7), c(1,7), c(0,5)))) face &lt;- st_sfc(eyes, nose, mouth, border) # sfc = sf column plot(face) Figure 6.1: Building simple geometries in sf The face was a simple feature column (sfc) built from the list of sfgs. An sfc just has the one column, so is not quite like a shapefile. But it can have a coordinate referencing system CRS, and so can be mapped. Kind of like a shapefile with no other attributes than shape [westUS] 6.1.2 Building a mappable sfc from scratch CA_matrix &lt;- rbind(c(-124,42),c(-120,42),c(-120,39),c(-114.5,35), c(-114.1,34.3),c(-114.6,32.7),c(-117,32.5),c(-118.5,34),c(-120.5,34.5), c(-122,36.5),c(-121.8,36.8),c(-122,37),c(-122.4,37.3),c(-122.5,37.8), c(-123,38),c(-123.7,39),c(-124,40),c(-124.4,40.5),c(-124,41),c(-124,42)) NV_matrix &lt;- rbind(c(-120,42),c(-114,42),c(-114,36),c(-114.5,36), c(-114.5,35),c(-120,39),c(-120,42)) CA_list &lt;- list(CA_matrix); NV_list &lt;- list(NV_matrix) CA_poly &lt;- st_polygon(CA_list); NV_poly &lt;- st_polygon(NV_list) sfc_2states &lt;- st_sfc(CA_poly,NV_poly,crs=4326) # crs=4326 specifies GCS st_geometry_type(sfc_2states) ## [1] POLYGON POLYGON ## 18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT MULTILINESTRING ... TRIANGLE library(tidyverse) ggplot() + geom_sf(data = sfc_2states) Figure 6.2: A simple map built from scratch with hard-coded data as simple feature columns sf class Is like a shapefile: has attributes to which geometry is added, and can be used like a data frame. attributes &lt;- bind_rows(c(abb=&quot;CA&quot;, area=423970, pop=39.56e6), c(abb=&quot;NV&quot;, area=286382, pop=3.03e6)) twostates &lt;- st_sf(attributes, geometry = sfc_2states) ggplot(twostates) + geom_sf() + geom_sf_text(aes(label = abb)) ## Warning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not give correct ## results for longitude/latitude data Figure 6.3: Using an sf class to build a map, displaying an attribute 6.1.3 Creating features from shapefiles or tables sfs st_read reads shapefiles shapefile is an open GIS format for points, polylines, polygons You would normally have shapefiles (and all the files that go with them  .shx, etc.) stored on your computer, but well access one from the iGIScData external data folder [sierra]: library(iGIScData) library(sf) shpPath &lt;- system.file(&quot;extdata&quot;,&quot;CA_counties.shp&quot;, package=&quot;iGIScData&quot;) CA_counties &lt;- st_read(shpPath) ## Reading layer `CA_counties&#39; from data source `C:\\Users\\900008452\\Documents\\R\\win-library\\4.0\\iGIScData\\extdata\\CA_counties.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 58 features and 60 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -124.4152 ymin: 32.53427 xmax: -114.1312 ymax: 42.00952 ## Geodetic CRS: WGS 84 plot(CA_counties) ## Warning: plotting the first 9 out of 60 attributes; use max.plot = 60 to plot all st_as_sf converts data frames using coordinates read from x and y variables, with crs set to coordinate system (4326 for GCS) sierraFebpts &lt;- st_as_sf(sierraFeb, coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs=4326) plot(sierraFebpts) [air_quality] library(tidyverse) library(sf) library(iGIScData) censusCentroids &lt;- st_centroid(BayAreaTracts) TRI_sp &lt;- st_as_sf(TRI_2017_CA, coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs=4326) # simple way to specify coordinate reference bnd &lt;- st_bbox(censusCentroids) ggplot() + geom_sf(data = BayAreaCounties, aes(fill = NAME)) + geom_sf(data = censusCentroids) + geom_sf(data = CAfreeways, color = &quot;grey&quot;) + geom_sf(data = TRI_sp, color = &quot;yellow&quot;) + coord_sf(xlim = c(bnd[1], bnd[3]), ylim = c(bnd[2], bnd[4])) + labs(title=&quot;Bay Area Counties, Freeways and Census Tract Centroids&quot;) Figure 6.4: ggplot map of Bay Area TRI sites, census centroids, freeways 6.1.4 Coordinate Referencing System Say you have data you need to make spatial with a spatial reference sierra &lt;- read_csv(\"sierraClimate.csv\") EPSG or CRS codes are an easy way to provide coordinate referencing. Two ways of doing the same thing. Spell it out: GCS &lt;- &quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; wsta = st_as_sf(sierra, coords = c(&quot;LONGITUDE&quot;,&quot;LATITUDE&quot;), crs=GCS) Google to find the code you need and assign it to the crs parameter: wsta &lt;- st_as_sf(sierra, coords = c(\"LONGITUDE\",\"LATITUDE\"), crs=4326) 6.1.4.1 Removing Geometry There are many instances where you want to remove geometry from a sf data frame Some R functions run into problems with geometry and produce confusing error messages, like non-numeric argument Youre wanting to work with an sf data frame in a non-spatial way One way to remove geometry: myNonSFdf &lt;- mySFdf %&gt;% st_set_geometry(NULL) 6.1.5 Spatial join st_join A spatial join with st_join joins data from census where TRI points occur [air_quality] TRI_sp &lt;- st_as_sf(TRI_2017_CA, coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs=4326) %&gt;% st_join(BayAreaTracts) %&gt;% filter(CNTY_FIPS %in% c(&quot;013&quot;, &quot;095&quot;)) 6.1.6 Plotting maps in the base plot system There are various programs for creating maps from spatial data, and well look at a few after weve looked at rasters. As usual, the base plot system often does something useful when you give it data. plot(BayAreaCounties) ## Warning: plotting the first 9 out of 60 attributes; use max.plot = 60 to plot all And with just one variable: plot(BayAreaCounties[&quot;POP_SQMI&quot;]) Theres a lot more we could do with the base plot system, but well mostly focus on some better options in ggplot2 and tmap. 6.2 Raster GIS in R Simple Features are feature-based, so based on the name I guess its not surprising that sf doesnt have support for rasters. But we can use the raster package for that. A bit of raster reading and map algebra with Marble Mountains elevation data [marbles] library(raster) rasPath &lt;- system.file(&quot;extdata&quot;,&quot;elev.tif&quot;, package=&quot;iGIScData&quot;) elev &lt;- raster(rasPath) slope &lt;- terrain(elev, opt=&quot;slope&quot;) aspect &lt;- terrain(elev, opt=&quot;aspect&quot;) slopeclasses &lt;-matrix(c(0,0.2,1, 0.2,0.4,2, 0.4,0.6,3, 0.6,0.8,4, 0.8,1,5), ncol=3, byrow=TRUE) slopeclass &lt;- reclassify(slope, rcl = slopeclasses) plot(elev) plot(slope) plot(slopeclass) plot(aspect) 6.2.1 Raster from scratch new_raster2 &lt;- raster(nrows = 6, ncols = 6, res = 0.5, xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5, vals = 1:36) plot(new_raster2) 6.3 ggplot2 for maps The Grammar of Graphics is the gg of ggplot. Key concept is separating aesthetics from data Aesthetics can come from variables (using aes()setting) or be constant for the graph Mapping tools that follow this lead ggplot, as we have seen, and it continues to be enhanced tmap (Thematic Maps) https://github.com/mtennekes/tmap Tennekes, M., 2018, tmap: Thematic Maps in R, Journal of Statistical Software 84(6), 1-39 ggplot(CA_counties) + geom_sf() Try ?geom_sf and youll find that its first parameters is mapping with aes() by default. The data property is inherited from the ggplot call, but commonly youll want to specify data=something in your geom_sf call. Another simple ggplot, with labels ggplot(CA_counties) + geom_sf() + geom_sf_text(aes(label = NAME), size = 1.5) ## Warning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not give correct ## results for longitude/latitude data and now with fill color ggplot(CA_counties) + geom_sf(aes(fill = MED_AGE)) + geom_sf_text(aes(label = NAME), col=&quot;white&quot;, size=1.5) ## Warning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not give correct ## results for longitude/latitude data Repositioned legend, no x or y labels ggplot(CA_counties) + geom_sf(aes(fill=MED_AGE)) + geom_sf_text(aes(label = NAME), col=&quot;white&quot;, size=1.5) + theme(legend.position = c(0.8, 0.8)) + labs(x=&quot;&quot;,y=&quot;&quot;) Map in ggplot2, zoomed into two counties [air_quality]: library(tidyverse); library(sf); library(iGIScData) census &lt;- BayAreaTracts %&gt;% filter(CNTY_FIPS %in% c(&quot;013&quot;, &quot;095&quot;)) TRI &lt;- TRI_2017_CA %&gt;% st_as_sf(coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs=4326) %&gt;% st_join(census) %&gt;% filter(CNTY_FIPS %in% c(&quot;013&quot;, &quot;095&quot;), (`5.1_FUGITIVE_AIR` + `5.2_STACK_AIR`) &gt; 0) ## although coordinates are longitude/latitude, st_intersects assumes that they are planar ## although coordinates are longitude/latitude, st_intersects assumes that they are planar bnd = st_bbox(census) ggplot() + geom_sf(data = BayAreaCounties, aes(fill = NAME)) + geom_sf(data = census, color=&quot;grey40&quot;, fill = NA) + geom_sf(data = TRI) + coord_sf(xlim = c(bnd[1], bnd[3]), ylim = c(bnd[2], bnd[4])) + labs(title=&quot;Census Tracts and TRI air-release sites&quot;) + theme(legend.position = &quot;none&quot;) 6.3.1 Rasters in ggplot2 Raster display in ggplot2 is currently a little awkward, as are rasters in general in the feature-dominated GIS world. We can use a trick: converting rasters to a grid of points [marbles: library(tidyverse) library(sf) library(raster) rasPath &lt;- system.file(&quot;extdata&quot;,&quot;elev.tif&quot;, package=&quot;iGIScData&quot;) elev &lt;- raster(rasPath) shpPath &lt;- system.file(&quot;extdata&quot;,&quot;trails.shp&quot;, package=&quot;iGIScData&quot;) trails &lt;- st_read(shpPath) ## Reading layer `trails&#39; from data source `C:\\Users\\900008452\\Documents\\R\\win-library\\4.0\\iGIScData\\extdata\\trails.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 32 features and 8 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: 481903.8 ymin: 4599196 xmax: 486901.9 ymax: 4603200 ## Projected CRS: NAD83 / UTM zone 10N elevpts = as.data.frame(rasterToPoints(elev)) ggplot() + geom_raster(data = elevpts, aes(x = x, y = y, fill = elev)) + geom_sf(data = trails) 6.4 tmap Basic building block is tm_shape(data) followed by various layer elements such as tm_fill() shape can be features or raster See Geocomputation with R Chapter 8 Making Maps with R for more information. https://geocompr.robinlovelace.net/adv-map.html library(spData) library(tmap) tm_shape(world) + tm_fill() + tm_borders() Color by variable [air_quality] library(sf) library(tmap) tm_shape(BayAreaTracts) + tm_fill(col = &quot;MED_AGE&quot;) tmap of sierraFeb with hillshade and point symbols [sierra] library(tmap) library(sf) library(raster) library(iGIScData) tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting tmap_options(max.categories = 8) sierra &lt;- st_as_sf(sierraFeb, coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs = 4326) rasPath &lt;- system.file(&quot;extdata&quot;,&quot;ca_hillsh_WGS84.tif&quot;, package=&quot;iGIScData&quot;) hillsh &lt;- raster(rasPath) bounds &lt;- st_bbox(sierra) tm_shape(hillsh,bbox=bounds)+ tm_raster(palette=&quot;-Greys&quot;,legend.show=FALSE,n=10) + tm_shape(sierra) + tm_symbols(col=&quot;TEMPERATURE&quot;, palette=c(&quot;blue&quot;,&quot;red&quot;), style=&quot;cont&quot;,n=8) + tm_legend() + tm_layout(legend.position=c(&quot;RIGHT&quot;,&quot;TOP&quot;)) ## stars object downsampled to 1092 by 915 cells. See tm_shape manual (argument raster.downsample) Note: -Greys needed to avoid negative image, since Greys go from light to dark, and to match reflectance as with b&amp;w photography, they need to go from dark to light. 6.5 Interactive Maps The word static in static maps isnt something you would have heard in a cartography class 30 years ago, since essentially all maps then were static. Very important in designing maps is considering your audience, and one characteristic of the audience of those maps of yore were that they were printed and thus fixed on paper. A lot of cartographic design relates to that property: Figure-to-ground relationships assume ground is a white piece of paper (or possibly a standard white background in a pdf), so good cartographic color schemes tend to range from light for low values to dark for high values. Scale is fixed and there are no tools for changing scale, so a lot of attention must be paid to providing scale information. Similarly, without the ability to see the map at different scales, inset maps are often needed to provide context. Interactive maps change the game in having tools for changing scale, and always being printed on a computer or device where the color of the background isnt necessarily white. We are increasingly used to using interactive maps on our phones or other devices, and often get frustrated not being able to zoom into a static map. A widely used interactive mapping system is Leaflet, but were going to use tmap to access Leaflet behind the scenes and allow us to create maps with one set of commands. The key parameter needed is tmap_mode which must be set to view to create an interactive map. [air_quality] tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing tm_shape(BayAreaTracts) + tm_fill(col = &quot;MED_AGE&quot;, alpha = 0.5) [sierra] library(tmap) library(sf) tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing tmap_options(max.categories = 8) sierra &lt;- st_as_sf(sierraFeb, coords = c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs = 4326) bounds &lt;- st_bbox(sierra) tm_basemap(leaflet::providers$Esri.NatGeoWorldMap) + tm_shape(sierra) + tm_symbols(col=&quot;TEMPERATURE&quot;, palette=c(&quot;blue&quot;,&quot;red&quot;), style=&quot;cont&quot;,n=8,size=0.2) + tm_legend() + tm_layout(legend.position=c(&quot;RIGHT&quot;,&quot;TOP&quot;)) ## legend.postion is used for plot mode. Use view.legend.position in tm_view to set the legend position in view mode. 6.5.0.1 Leaflet Now that weve seen an app that used it, lets look briefly at Leaflet itself, and well see that even the Leaflet package in R actually uses JavaScript Leaflet is designed as An open-source JavaScript library for mobile-friendly interactive maps https://leafletjs.com The R package leaflet is an interface to the JavaScript library Leaflet to create interactive web maps. It was developed on top of the htmlwidgets framework, which means the maps can be rendered in RMarkdown (v2) documents (which is why you can see it in this document), Shiny apps, and RStudio IDE / the R console. https://blog.rstudio.com/2015/06/24/leaflet-interactive-web-maps-with-r/ https://github.com/rstudio/cheatsheets/blob/master/leaflet.pdf library(leaflet) m &lt;- leaflet() %&gt;% addTiles() %&gt;% # default OpenStreetMap tiles addMarkers(lng=174.768, lat=-36.852, popup=&quot;The birthplace of R&quot;) m 6.6 Exercises Using the method of building simple sf geometries, build a simple 1x1 square object and plot it. Remember that you have to close the polygon, so the first vertex is the same as the last (of 5) vertices. Provide your code only. Build a map in ggplot of Colorado, Wyoming, and Utah with these boundary vertices in GCS. As with the square, remember to close each figure, and assign the crs to what is needed for GCS: 4326. Submit map as exported plot, and code in the submittal text block. [westUS] Colorado: (-109,41),(-102,41),(-102,37),(-109,37) Wyoming: (-111,45),(-104,45),(-104,41),(-111,41) Utah: (-114,42),(-111,42),(-111,41),(-109,41),(-109,37),(-114,37) Arizona: (-114,37),(-109,37),(-109,31.3),(-111,31.3),(-114.8,32.5), (-114.6,32.7),(-114.1,34.3),(-114.5,35),(-114.5,36),(-114,36) New Mexico: (-109,37),(-103,37),(-103,32),(-106.6,32),(-106.5,31.8), (-108.2,31.8),(-108.2,31.3),(-109,31.3) Add in the code for CA and NV and create kind of a western US map Create an sf class from the seven states adding the fields name, abb, area_sqkm, and population, and create a map labeling with the name. Colorado, CO, 269837, 5758736 Wyoming, WY, 253600, 578759 Utah, UT, 84899, 3205958 Arizona, AZ, 295234, 7278717 New Mexico, NM, 314917, 2096829 California, CA, 423970, 39368078 Nevada, NV, 286382, 3080156 Create a tibble for the highest peaks in the 7 states, with the following names, elevations in m, longitude and latitude, and add them to that map: Wheeler Peak, 4011, -105.4, 36.5 Mt. Whitney, 4421, -118.2, 36.5 Boundary Peak, 4007, -118.35, 37.9 Kings Peak, 4120, -110.3, 40.8 Gannett Peak, 4209, -109, 43.2 Mt. Elbert, 4401, -106.4, 39.1 Humphreys Peak, 3852, -111.7, 35.4 Note: the easiest way to do this is with the tribble function, starting with: peaks &lt;- tribble( ~peak, ~elev, ~longitude, ~latitude, &quot;Wheeler Peak&quot;, 4011, -105.4, 36.5, Use a spatial join to add the points to the states to provide a new attribute maximum elevation, and display that using geom_sf_text() with the state polygons. From the CA_counties and CAfreeways feature data in iGIScData, make a simple map in ggplot, with freeways colored red. After adding the raster library, create a raster from the built-in volcano matrix of elevations from Aucklands Maunga Whau Volcano, and use plot() to display it. Wed do more with that dataset but we dont know what the cell size is. Use tmap to create a simple map from the SW_States (polygons) and peaksp (points) data we created earlier. Hints: youll want to use tm_text with text set to peak to label the points, along with the parameter auto.placement=TRUE. [westUS] Change the map to the view mode, but dont use the state borders since the basemap will have them. Just before adding shapes, set the basemap to leaflet::providers$Esri.NatGeoWorldMap, then continue to the peaks after the + to see the peaks on a National Geographic basemap. "],["statistics-and-modeling.html", "7 Statistics and Modeling 7.1 Goals of statistical analysis 7.2 Summary Statistics 7.3 Statistical tests 7.4 Modeling in R 7.5 Spatial Statistical Analysis", " 7 Statistics and Modeling 7.1 Goals of statistical analysis To frame how we might approach statistical analysis and modeling, there are various goals that are commonly involved: To understand our data nature of our data, through summary statistics and various graphics like histograms spatial statistical analysis time series analysis To group or classify things based on their properties using factors to define groups, and deriving grouped summaries comparing observed vs expected counts or probabilities To understand how variables relate to one another or maybe even explain variations in other variables, through correlation analysis To model behavior and maybe predict it various linear models To confirm our observations from exploration (field/lab/vis) inferential statistics e.g. difference of means tests, ANOVA, X^2 To have the confidence to draw conclusions, make informed decisions To help communicate our work These goals can be seen in the context of a typical research paper or thesis outline in environmental science: Introduction Literature Review Methodology Results field, lab, geospatial data Analysis statistical analysis qualitative analysis visualization Discussion making sense of analysis possibly recursive, with visualization Conclusion conclusion about what the above shows new questions for further research possible policy recommendation The scope and theory of statistical analysis and models is extensive, and there are many good books on the subject that employ the R language. This chapter is a short review of some of these methods and how they apply to environmental data science. 7.2 Summary Statistics Summary statistics such as mean, standard deviation, variance, minimum, maximum, and range are derived in quite a few R functions, commonly as a parameter or a sub-function (see mutate). A an overall simple statistical summary is very easy to do in base R: summary(tidy_eucoak) ## site site # tree Date month ## Length:180 Min. :1.000 Length:180 Min. :2006-11-08 Length:180 ## Class :character 1st Qu.:2.000 Class :character 1st Qu.:2006-12-07 Class :character ## Mode :character Median :4.000 Mode :character Median :2007-01-30 Mode :character ## Mean :4.422 Mean :2007-01-29 ## 3rd Qu.:6.000 3rd Qu.:2007-03-22 ## Max. :8.000 Max. :2007-05-07 ## ## rain_mm rain_subcanopy slope aspect runoff_L ## Min. : 1.00 Min. : 1.00 Min. : 9.00 Min. :100.0 Min. : 0.000 ## 1st Qu.:16.00 1st Qu.:16.00 1st Qu.:12.00 1st Qu.:143.0 1st Qu.: 0.000 ## Median :28.50 Median :30.00 Median :24.00 Median :196.0 Median : 0.825 ## Mean :37.99 Mean :34.84 Mean :20.48 Mean :186.6 Mean : 2.244 ## 3rd Qu.:63.25 3rd Qu.:50.00 3rd Qu.:27.00 3rd Qu.:221.8 3rd Qu.: 3.200 ## Max. :99.00 Max. :98.00 Max. :32.00 Max. :296.0 Max. :16.000 ## NA&#39;s :36 NA&#39;s :4 NA&#39;s :8 ## surface_tension runoff_rainfall_ratio ## Min. :28.51 Min. :0.00000 ## 1st Qu.:37.40 1st Qu.:0.00000 ## Median :62.60 Median :0.03347 ## Mean :55.73 Mean :0.05981 ## 3rd Qu.:72.75 3rd Qu.:0.08474 ## Max. :72.75 Max. :0.42000 ## NA&#39;s :44 NA&#39;s :8 7.2.1 Summarize by group: stratifying a summary eucoakrainfallrunoffTDR %&gt;% group_by(site) %&gt;% summarize( rain = mean(rain_mm, na.rm = TRUE), rainSD = sd(rain_mm, na.rm = TRUE), runoffL_oak = mean(runoffL_oak, na.rm = TRUE), runoffL_euc = mean(runoffL_euc, na.rm = TRUE), runoffL_oakMax = max(runoffL_oak, na.rm = TRUE), runoffL_eucMax = max(runoffL_oak, na.rm = TRUE), ) ## # A tibble: 8 x 7 ## site rain rainSD runoffL_oak runoffL_euc runoffL_oakMax runoffL_eucMax ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AB1 48.4 28.2 6.80 6.03 6.80 6.80 ## 2 AB2 34.1 27.9 4.91 3.65 4.91 4.91 ## 3 KM1 48 32.0 1.94 0.592 1.94 1.94 ## 4 PR1 56.5 19.1 0.459 2.31 0.459 0.459 ## 5 TP1 38.4 29.5 0.877 1.66 0.877 0.877 ## 6 TP2 34.3 29.2 0.0955 1.53 0.0955 0.0955 ## 7 TP3 32.1 28.4 0.381 0.815 0.381 0.381 ## 8 TP4 32.5 28.2 0.231 2.83 0.231 0.231 7.2.2 Boxplot for visualizing distributions by group A Tukey boxplot is a good way to visualize distributions by group. In this soil CO_2 study of the Marble Mountains, some sites had much greater variance, and some sites tended to be low vs high: soilCO2_97$SITE &lt;- factor(soilCO2_97$SITE) ggplot(data = soilCO2_97, mapping = aes(x = SITE, y = `CO2%`)) + geom_boxplot() 7.2.3 Generating pseudorandom numbers Functions commonly used in R books for quickly creating a lot of numbers to display are those that generate pseudorandom numbers. These are also useful in statistical methods that need a lot of these, such as in Monte Carlo simulation. The two most commonly used are: runif() generates a vector of n pseudorandom numbers ranging by default from min=0 to max=1. rnorm() generates a vector of n normally distributed pseudorandom numbers with a default mean=0 and sd=0. To see both in action as x and y: x &lt;- as_tibble(runif(n=1000, min=10, max=20)) names(x) &lt;- &#39;x&#39; ggplot(x, aes(x=x)) + geom_histogram() y &lt;- as_tibble(rnorm(n=1000, mean=100, sd=10)) names(y) &lt;- &#39;y&#39; ggplot(y, aes(x=y)) + geom_histogram() ggplot(y, aes(x=y)) + geom_density() xy &lt;- bind_cols(x,y) ggplot(xy, aes(x=x,y=y)) + geom_point() 7.3 Statistical tests Tests that compare our data to other data or look at relationships among variables are important statistical methods, and you should refer to statistical references to best understand how to apply the appropriate methods for your research. 7.3.1 Comparing samples and groupings A common need in environmental research is to compare samples of a phenomenon or compare samples with an assumed standard population. The simplest application of this is the t-test, which can only involve comparing two samples or one sample with a population. Analysis of Variance extends this to allow for more than two groups, and can be seen as a linear model where the categorical grouping (as a factor in R) is one of the variables. 7.3.1.1 t.test and a non-parametric alternative, the Kruskal-Wallis Rank Sum test XSptsPheno %&gt;% ggplot(aes(NDVI, fill=phenology)) + geom_density(alpha=0.2) Figure 7.1: NDVI by phenology t.test(NDVI~phenology, data=XSptsPheno) ## ## Welch Two Sample t-test ## ## data: NDVI by phenology ## t = 5.4952, df = 52.03, p-value = 1.19e-06 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.1421785 0.3057412 ## sample estimates: ## mean in group growing mean in group senescent ## 0.5901186 0.3661588 While these data sets appear reasonably normal, the Shapiro-Wilk test (which uses a null hypothesis of normal) has a p value &lt; 0.05 for the senescent group, so the data cant be assumed to be normal. shapiro.test(XSptsPheno$NDVI[XSptsPheno$phenology==&quot;growing&quot;]) ## ## Shapiro-Wilk normality test ## ## data: XSptsPheno$NDVI[XSptsPheno$phenology == &quot;growing&quot;] ## W = 0.93608, p-value = 0.07918 shapiro.test(XSptsPheno$NDVI[XSptsPheno$phenology==&quot;senescent&quot;]) ## ## Shapiro-Wilk normality test ## ## data: XSptsPheno$NDVI[XSptsPheno$phenology == &quot;senescent&quot;] ## W = 0.88728, p-value = 0.004925 Therefore we should use a non-parametric alternative such as the Kruskal-Wallis Rank Sum test: kruskal.test(NDVI~phenology, data=XSptsPheno) ## ## Kruskal-Wallis rank sum test ## ## data: NDVI by phenology ## Kruskal-Wallis chi-squared = 19.164, df = 1, p-value = 1.199e-05 [eucoak] For the question Is the runoff under Eucalyptus canopy significantly different from that under oaks? well then start by test for normality of each of the two samples (euc and oak) shapiro.test(tidy_eucoak$runoff_L[tidy_eucoak$tree == &quot;euc&quot;]) ## ## Shapiro-Wilk normality test ## ## data: tidy_eucoak$runoff_L[tidy_eucoak$tree == &quot;euc&quot;] ## W = 0.74241, p-value = 4.724e-11 shapiro.test(tidy_eucoak$runoff_L[tidy_eucoak$tree == &quot;oak&quot;]) ## ## Shapiro-Wilk normality test ## ## data: tidy_eucoak$runoff_L[tidy_eucoak$tree == &quot;oak&quot;] ## W = 0.71744, p-value = 1.698e-11 which shows clearly that both samples are non-normal. So we might apply the non-parametric Kruskal-Wallis test: kruskal.test(runoff_L~tree, data=tidy_eucoak) ## ## Kruskal-Wallis rank sum test ## ## data: runoff_L by tree ## Kruskal-Wallis chi-squared = 2.2991, df = 1, p-value = 0.1294 and no significant difference can be seen. If we look at the data graphically, this makes sense: tidy_eucoak %&gt;% ggplot(aes(log(runoff_L),fill=tree)) + geom_density(alpha=0.2) Figure 7.2: Runoff under Eucalyptus and Oak in Bay Area sites However, some of this may result from major variations among sites, which is apparent in this boxplot: ggplot(data = tidy_eucoak) + geom_boxplot(aes(x=site, y=runoff_L, color=tree)) Figure 7.3: runoff at various sites contrasting euc and oak We might restrict our analysis to Tilden Park sites in the East Bay. tilden &lt;- tidy_eucoak %&gt;% filter(str_detect(tidy_eucoak$site,&quot;TP&quot;)) tilden %&gt;% ggplot(aes(log(runoff_L),fill=tree)) + geom_density(alpha=0.2) shapiro.test(tilden$runoff_L[tilden$tree == &quot;euc&quot;]) ## ## Shapiro-Wilk normality test ## ## data: tilden$runoff_L[tilden$tree == &quot;euc&quot;] ## W = 0.73933, p-value = 1.764e-07 shapiro.test(tilden$runoff_L[tilden$tree == &quot;oak&quot;]) ## ## Shapiro-Wilk normality test ## ## data: tilden$runoff_L[tilden$tree == &quot;oak&quot;] ## W = 0.59535, p-value = 8.529e-10 So once again, as is common with small sample sets, we need a non-parametric test. kruskal.test(runoff_L~tree, data=tilden) ## ## Kruskal-Wallis rank sum test ## ## data: runoff_L by tree ## Kruskal-Wallis chi-squared = 14.527, df = 1, p-value = 0.0001382 Analysis process from exploration to testing [eucoak] In the year runoff was studied, there were no runoff events sufficient to mobilize sediments. The next year, January had a big event, so we collected sediments and processed them in the lab. Questions: Is there a difference between eucs and oaks in terms of fine sediment yield? Is there a difference between eucs and oaks in terms of total sediment yield? (includes litter) csvPath &lt;- system.file(&quot;extdata&quot;, &quot;eucoaksediment.csv&quot;, package=&quot;iGIScData&quot;) eucoaksed &lt;- read_csv(csvPath) summary(eucoaksed) ## id site trtype slope bulkDensity ## Length:14 Length:14 Length:14 Min. : 9.00 Min. :0.960 ## Class :character Class :character Class :character 1st Qu.:12.00 1st Qu.:1.060 ## Mode :character Mode :character Mode :character Median :21.00 Median :1.125 ## Mean :20.04 Mean :1.156 ## 3rd Qu.:25.00 3rd Qu.:1.245 ## Max. :32.00 Max. :1.490 ## ## litter Jan08rain mean_runoff_ratio med_runoff_ratio std_runoff_ratio ## Min. : 25.00 Min. :228.1 Min. :0.00450 Min. :0.00000 Min. :0.01070 ## 1st Qu.: 51.25 1st Qu.:290.8 1st Qu.:0.02285 1st Qu.:0.01105 1st Qu.:0.01642 ## Median : 77.00 Median :301.1 Median :0.04835 Median :0.04950 Median :0.02740 ## Mean : 76.64 Mean :298.5 Mean :0.06679 Mean :0.06179 Mean :0.04355 ## 3rd Qu.: 95.75 3rd Qu.:317.0 3rd Qu.:0.12172 3rd Qu.:0.09492 3rd Qu.:0.05735 ## Max. :135.00 Max. :328.5 Max. :0.16480 Max. :0.16430 Max. :0.11480 ## ## fines_g litter_g total_g fineTotalRatio fineRainRatio ## Min. : 7.50 Min. :14.00 Min. : 23.50 Min. :0.1300 Min. :0.025 ## 1st Qu.:13.30 1st Qu.:18.80 1st Qu.: 35.00 1st Qu.:0.2700 1st Qu.:0.044 ## Median :18.10 Median :40.40 Median : 60.50 Median :0.3900 Median :0.064 ## Mean :27.77 Mean :41.32 Mean : 69.12 Mean :0.3715 Mean :0.097 ## 3rd Qu.:45.00 3rd Qu.:57.50 3rd Qu.: 95.50 3rd Qu.:0.4900 3rd Qu.:0.141 ## Max. :66.70 Max. :97.80 Max. :125.90 Max. :0.5500 Max. :0.293 ## NA&#39;s :1 NA&#39;s :1 NA&#39;s :1 NA&#39;s :1 NA&#39;s :1 eucoaksed %&gt;% group_by(trtype) %&gt;% summarize(meanfines = mean(fines_g, na.rm=T), sdfines = sd(fines_g, na.rm=T), meantotal = mean(total_g, na.rm=T), sdtotal = sd(total_g, na.rm=T)) ## # A tibble: 2 x 5 ## trtype meanfines sdfines meantotal sdtotal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 euc 14.2 3.50 48.6 35.0 ## 2 oak 39.4 20.4 86.7 26.2 eucoakLong &lt;- eucoaksed %&gt;% pivot_longer(col=c(fines_g,litter_g), names_to = &quot;sed_type&quot;, values_to = &quot;sed_g&quot;) eucoakLong %&gt;% ggplot(aes(trtype, sed_g, col=sed_type)) + geom_boxplot() eucoakLong %&gt;% ggplot(aes(sed_g, col=sed_type)) + geom_density() + facet_grid(trtype ~ .) shapiro.test(eucoaksed$fines_g[eucoaksed$trtype == &quot;euc&quot;]) ## ## Shapiro-Wilk normality test ## ## data: eucoaksed$fines_g[eucoaksed$trtype == &quot;euc&quot;] ## W = 0.9374, p-value = 0.6383 shapiro.test(eucoaksed$fines_g[eucoaksed$trtype == &quot;oak&quot;]) ## ## Shapiro-Wilk normality test ## ## data: eucoaksed$fines_g[eucoaksed$trtype == &quot;oak&quot;] ## W = 0.96659, p-value = 0.8729 t.test(fines_g~trtype, data=eucoaksed) ## ## Welch Two Sample t-test ## ## data: fines_g by trtype ## t = -3.2102, df = 6.4104, p-value = 0.01675 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -44.059797 -6.278299 ## sample estimates: ## mean in group euc mean in group oak ## 14.21667 39.38571 shapiro.test(eucoaksed$total_g[eucoaksed$trtype == &quot;euc&quot;]) ## ## Shapiro-Wilk normality test ## ## data: eucoaksed$total_g[eucoaksed$trtype == &quot;euc&quot;] ## W = 0.76405, p-value = 0.02725 shapiro.test(eucoaksed$total_g[eucoaksed$trtype == &quot;oak&quot;]) ## ## Shapiro-Wilk normality test ## ## data: eucoaksed$total_g[eucoaksed$trtype == &quot;oak&quot;] ## W = 0.94988, p-value = 0.7286 kruskal.test(total_g~trtype, data=eucoaksed) ## ## Kruskal-Wallis rank sum test ## ## data: total_g by trtype ## Kruskal-Wallis chi-squared = 3.449, df = 1, p-value = 0.06329 So we used a t test for the fines_g, and the test suggests that theres a significant difference in sediment yield for fines, but the Kruskal-Wallis test on total sediment (including litter) did not show a significant difference. Both results support the conclusion that oaks in this study produced more soil erosion, largely because the Eucalyptus stands generate so much litter cover, and that litter also made the total sediment yield not significantly different. 7.3.1.2 Analysis of Variance Purpose is to compare groups based upon continuous variables. Can be thought of as an extension of a t test where you have more than two groups, or as a linear model where one variable is a factor. Response variable is a continuous variable Explanatory variable is the grouping  categorical (a factor in R) Are water samples from streams draining sandstone, limestone, and shale different based on pH? 7.3.2 Correlation r = Pearsons product-moment correlation  negative or positive r2 = amount of variance in one variable explained by the other  always positive. Can show with a pairs plot: pairs(dataframe), but is tricky Heres an easier method from the psych package [sierra] library(psych) pairs.panels(sierraFeb %&gt;% dplyr::select(LATITUDE, LONGITUDE, ELEVATION, PRECIPITATION, TEMPERATURE)) 7.4 Modeling in R lm(y ~ x) linear regression lm(y ~ x1 + x2 + x3) multiple regression glm(y ~ x, family = poisson) generalized linear model, poisson distribution; see ?family to see those supported, including binomial, gaussian, poisson, etc. aov(y ~ x) analysis of variance (same as lm except in the summary) gam(y ~ x) generalized additive models tree(y ~ x) or rpart(y ~ x) regression/classification trees model1 &lt;- lm(TEMPERATURE ~ ELEVATION, data = sierraFeb) summary(model1) ## ## Call: ## lm(formula = TEMPERATURE ~ ELEVATION, data = sierraFeb) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.9126 -1.0466 -0.0027 0.7940 4.5327 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.8813804 0.3825302 31.06 &lt;2e-16 *** ## ELEVATION -0.0061018 0.0002968 -20.56 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.533 on 60 degrees of freedom ## Multiple R-squared: 0.8757, Adjusted R-squared: 0.8736 ## F-statistic: 422.6 on 1 and 60 DF, p-value: &lt; 2.2e-16 Probably the most important statistic is the p value for the predictor variable ELEVATION, which in this case is very small &lt;2e-16. ## ## Call: ## lm(formula = TEMPERATURE ~ ELEVATION, data = sierraFeb) ## ## Coefficients: ## (Intercept) ELEVATION ## 11.881380 -0.006102 Making Predictions eqn ## [1] &quot;temperature = 11.88 + -0.006*elevation + e&quot; a &lt;- model1$coefficients[1] b &lt;- model1$coefficients[2] elevations &lt;- c(500, 1000, 1500, 2000) elevations ## [1] 500 1000 1500 2000 tempEstimate &lt;- a + b * elevations tempEstimate ## [1] 8.8304692 5.7795580 2.7286468 -0.3222645 7.4.1 Analysis of Covariance Same purpose as Analysis of Variance, but also takes into account the influence of other variables called covariates. In a way, combines a linear model with an analysis of variance. Are water samples from streams draining sandstone, limestone, and shale different based on pH, while taking into account elevation? Response variable is modeled from the factor (ANOVA) plus the covariate (regression) ANOVA: pH ~ rocktype Regression: pH ~ elevation ANCOVA: pH ~ rocktype + elevation Yet shouldnt involve interaction between rocktype and elevation Example: stream types distinguished by discharge and slope Three common river types are meandering, braided and anastomosed. For each, their slope varies by bankfull discharge in a relationship that looks something like: No interaction between covariate and factor No relationship between discharge and channel type. Another interpretation: the slope of the relationship between the covariate and response variable is about the same for each group; only the intercept differs. Assumes parallel slopes. log10(S) ~ strtype * log10(Q)  interaction between covariate and factor log10(S) ~ strtype + log10(Q)  no interaction, parallel slopes If models are not significantly different, remove interaction term due to parsimony, and satisfies this ANCOVA requirement. library(tidyverse) csvPath &lt;- system.file(&quot;extdata&quot;,&quot;streams.csv&quot;, package=&quot;iGIScData&quot;) streams &lt;- read_csv(csvPath) streams$strtype &lt;- factor(streams$type, labels=c(&quot;Anastomosing&quot;,&quot;Braided&quot;,&quot;Meandering&quot;)) summary(streams) ## type Q S strtype ## Length:41 Min. : 6 Min. :0.000011 Anastomosing: 8 ## Class :character 1st Qu.: 15 1st Qu.:0.000100 Braided :12 ## Mode :character Median : 40 Median :0.000700 Meandering :21 ## Mean : 4159 Mean :0.001737 ## 3rd Qu.: 550 3rd Qu.:0.002800 ## Max. :100000 Max. :0.009500 ggplot(streams, aes(Q, S, color=strtype)) + geom_point() library(scales) # needed for the trans_format function below ggplot(streams, aes(Q, S, color=strtype)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se = FALSE) + scale_x_continuous(trans=log10_trans(), labels = trans_format(&quot;log10&quot;, math_format(10^.x))) + scale_y_continuous(trans=log10_trans(), labels = trans_format(&quot;log10&quot;, math_format(10^.x))) ancova = lm(log10(S)~strtype*log10(Q), data=streams) summary(ancova) ## ## Call: ## lm(formula = log10(S) ~ strtype * log10(Q), data = streams) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.63636 -0.13903 -0.00032 0.12652 0.60750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.91819 0.31094 -12.601 1.45e-14 *** ## strtypeBraided 2.20085 0.35383 6.220 3.96e-07 *** ## strtypeMeandering 1.63479 0.33153 4.931 1.98e-05 *** ## log10(Q) -0.43537 0.18073 -2.409 0.0214 * ## strtypeBraided:log10(Q) -0.01488 0.19102 -0.078 0.9384 ## strtypeMeandering:log10(Q) 0.05183 0.18748 0.276 0.7838 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2656 on 35 degrees of freedom ## Multiple R-squared: 0.9154, Adjusted R-squared: 0.9033 ## F-statistic: 75.73 on 5 and 35 DF, p-value: &lt; 2.2e-16 anova(ancova) ## Analysis of Variance Table ## ## Response: log10(S) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## strtype 2 18.3914 9.1957 130.3650 &lt; 2.2e-16 *** ## log10(Q) 1 8.2658 8.2658 117.1821 1.023e-12 *** ## strtype:log10(Q) 2 0.0511 0.0255 0.3619 0.6989 ## Residuals 35 2.4688 0.0705 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Now an additive model, which does not have that interaction ancova2 = lm(log10(S)~strtype+log10(Q), data=streams) anova(ancova2) ## Analysis of Variance Table ## ## Response: log10(S) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## strtype 2 18.3914 9.1957 135.02 &lt; 2.2e-16 *** ## log10(Q) 1 8.2658 8.2658 121.37 3.07e-13 *** ## Residuals 37 2.5199 0.0681 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(ancova,ancova2) ## Analysis of Variance Table ## ## Model 1: log10(S) ~ strtype * log10(Q) ## Model 2: log10(S) ~ strtype + log10(Q) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 35 2.4688 ## 2 37 2.5199 -2 -0.051051 0.3619 0.6989 # not significantly different, so model simplification is justified # Now we remove the strtype term ancova3 = update(ancova2, ~ . - strtype) anova(ancova2,ancova3) ## Analysis of Variance Table ## ## Model 1: log10(S) ~ strtype + log10(Q) ## Model 2: log10(S) ~ log10(Q) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 37 2.5199 ## 2 39 25.5099 -2 -22.99 168.78 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Goes too far. Removing the strtype creates a significantly different model step(ancova) ## Start: AIC=-103.2 ## log10(S) ~ strtype * log10(Q) ## ## Df Sum of Sq RSS AIC ## - strtype:log10(Q) 2 0.051051 2.5199 -106.36 ## &lt;none&gt; 2.4688 -103.20 ## ## Step: AIC=-106.36 ## log10(S) ~ strtype + log10(Q) ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 2.5199 -106.364 ## - log10(Q) 1 8.2658 10.7857 -48.750 ## - strtype 2 22.9901 25.5099 -15.455 ## ## Call: ## lm(formula = log10(S) ~ strtype + log10(Q), data = streams) ## ## Coefficients: ## (Intercept) strtypeBraided strtypeMeandering log10(Q) ## -3.9583 2.1453 1.7294 -0.4109 Part of general linear model (lm) ANOVA &amp; ANCOVA are applications of a general linear model. Uses lm in R Response variable is continuous, assumed normally distributed Not the same as Generalized Linear Model (GLM) With GLM, response variable may be from count data (e.g. Poisson), probabilities of occurrence (logistic regression) or other non-normal distributions. mymodel = lm(log10(s) ~ strtype + log10(Q)) The linear model, with categorical explanatory variable + covariate anova(mymodel) Displays the Analysis of Variance table from the linear model 7.4.2 Generalized Linear Model (GLM) The glm in R allows you to work with various types of data using various distributions, described as families such as: gaussian : normal distribution  what is used with lm binomial : logit  used with probabilities. Used for logistic regression poisson : for counts. Commonly used for species counts. see help(glm) for other examples Great explanation of poisson distribution using meteor showers at: https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459 7.5 Spatial Statistical Analysis Spatial statistical analysis brings in the spatial dimension to a statistical analysis, ranging from visual analysis of patterns to specialized spatial statistical methods. There are many applications for these methods in environmental research, since spatial patterns are generally highly relevant. We might ask: What patterns can we see? What is the effect of scale? Relationships among variables  do they vary spatially? 7.5.1 Spatial Autocorrelation [Need to add a Morans I section here] 7.5.2 Mapping Residuals If the residuals from regression are spatially autocorrelated, look for patterns in the residuals to find other explanatory variables. library(tidyverse) library(iGIScData) library(sf); library(raster) rasPath &lt;- system.file(&quot;extdata&quot;, &quot;ca_hillsh_WGS84.tif&quot;, package=&quot;iGIScData&quot;) hillsh &lt;- raster(rasPath) hillshpts &lt;- as.data.frame(rasterToPoints(hillsh)) CAbasemap &lt;- ggplot() + geom_raster(aes(x=x, y=y, fill=ca_hillsh_WGS84), hillshpts) + guides(fill=F) + geom_sf(data=CA_counties, fill=NA) + scale_fill_gradient(low = &quot;#606060&quot;, high = &quot;#FFFFFF&quot;) + labs(x=&#39;&#39;, y=&#39;&#39;) sierra &lt;- st_as_sf(filter(sierraFeb, !is.na(TEMPERATURE)), coords=c(&quot;LONGITUDE&quot;, &quot;LATITUDE&quot;), crs=4326) model1 &lt;- lm(TEMPERATURE ~ ELEVATION, data = sierra) cc &lt;- model1$coefficients sierra$resid &lt;- resid(model1) sierra$predict &lt;- predict(model1) eqn = paste(&quot;temperature =&quot;, paste(round(cc[1],2), paste(round(cc[-1], digits=3), sep=&quot;*&quot;, collapse=&quot; + &quot;, paste(&quot;elevation&quot;)), sep=&quot; + &quot;), &quot;+ e&quot;) ct &lt;- st_read(system.file(&quot;extdata&quot;,&quot;CA_places.shp&quot;,package=&quot;iGIScData&quot;)) ct$AREANAME_pad &lt;- paste0(str_replace_all(ct$AREANAME, &#39;[A-Za-z]&#39;,&#39; &#39;), ct$AREANAME) bounds &lt;- st_bbox(sierra) sierrabasemap &lt;- CAbasemap + geom_sf(data=ct) + geom_sf_text(mapping = aes(label=AREANAME_pad), data=ct, size = 2, nudge_x = 0.1, nudge_y = 0.1) + coord_sf(xlim = c(bounds[1], bounds[3]), ylim = c(bounds[2],bounds[4])) sierrabasemap + geom_sf(mapping = aes(color = resid), alpha=0.7, data=sierra, size=2.5) + scale_color_gradient2(low=&quot;blue&quot;, mid=&quot;ivory2&quot;, high=&quot;red&quot;, midpoint=mean(sierra$resid)) + coord_sf(xlim = c(bounds[1], bounds[3]), ylim = c(bounds[2],bounds[4])) + labs(title=&quot;Residuals&quot;, subtitle=eqn) + theme(legend.position = c(0.8, 0.85)) + theme(legend.key.size = unit(0.4, &#39;cm&#39;), legend.title = element_text(size=8)) "],["time-series.html", "8 Time Series 8.1 Creation of time series (ts) data 8.2 Data smoothing methods 8.3 Decomposing time series 8.4 Learning more about time series in R", " 8 Time Series plot(stl(co2, s.window = &quot;periodic&quot;)) # co2 is already a time series Time series data are widely used in the financial sector for obvious reasons, and these methods can be very useful environmental data science especially in studies such as looking at long-term climate change variables such as the Mauna Loa CO2 data above, or at the much finer temporal scale, environmental data from data loggers such as eddy covariance flux towers. 8.1 Creation of time series (ts) data A time series (ts) is created with the ts() function. the time unit can be anything  not actually saved with the ts observations must be a regularly spaced series library(tidyverse) SFhighF &lt;- c(58,61,62,63,64,67,67,68,71,70,64,58) SFlowF &lt;- c(47,48,49,50,51,53,54,55,56,55,51,47) SFhighC &lt;- (SFhighF-32)*5/9 SFlowC &lt;- (SFlowF-32)*5/9 SFtempC &lt;- bind_cols(high=SFhighC,low=SFlowC) plot(SFtempC); plot(ts(SFtempC)) 8.1.1 frequency setting Frequency setting is a key parameter for ts() sets how many observations per time unit ts() mostly doesnt seem to care what the time unit is, however some functions figure it out, at least for an annual time unit, e.g. that 1-12 means months when theres a frequency of 12 plot(ts(SFtempC, frequency=1), main=&quot;monthly time unit&quot;) plot(ts(SFtempC, frequency=12), main=&quot;yearly time unit&quot;) frequency &lt; 1 If you have data of lower frequency than 1 per unit e.g. greenhouse gas data values every 20 years, starting in year 20, frequency 1/20 = 0.05 library(dslabs) data(&quot;greenhouse_gases&quot;) GHGwide &lt;- pivot_wider(greenhouse_gases, names_from = gas, values_from = concentration) GHG &lt;- ts(GHGwide, frequency=0.05, start=20) plot(GHG) start and end parameters the time of the first (start) and last (end) observations. Either a single number or a vector of two numbers (the second of which is an integer), which specify a natural time unit and a (1-based) number of samples into the time unit. Example with year as the time unit and monthly data, starting July 2019 and ending June 2020: frequency=12, start=c(2019,7), end=c(2020,6) 8.1.2 Data logger data from Marble Mountains resurgence [Marbles] For a study of chemical water quality and hydrology of a karst system in the Marble Mountains of California, a spring resurgence was instrumented to measure water level, temperature, and specific conductance (a surrogate for total dissolved solids) over a 4-year period. library(tidyverse); library(lubridate) resurg &lt;- read_csv(system.file(&quot;extdata&quot;,&quot;resurgenceData.csv&quot;,package=&quot;iGIScData&quot;)) %&gt;% mutate(date_time = mdy_hms(paste(date, time))) %&gt;% dplyr::select(date_time, ATemp, BTemp, wlevelm, EC, InputV) resurgTS &lt;- ts(resurg, frequency = 12*365, start = c(1994, 266*12+7)) plot(resurgTS) 8.2 Data smoothing methods 8.2.1 moving average (ma) Simple generalization of sequential data The order parameter is how many values are averaged in the moving window should be an odd number library(forecast) ts(SFtempC, frequency=12) ## high low ## Jan 1 14.44444 8.333333 ## Feb 1 16.11111 8.888889 ## Mar 1 16.66667 9.444444 ## Apr 1 17.22222 10.000000 ## May 1 17.77778 10.555556 ## Jun 1 19.44444 11.666667 ## Jul 1 19.44444 12.222222 ## Aug 1 20.00000 12.777778 ## Sep 1 21.66667 13.333333 ## Oct 1 21.11111 12.777778 ## Nov 1 17.77778 10.555556 ## Dec 1 14.44444 8.333333 ma(ts(SFtempC,frequency=12),order=3) ## [,1] [,2] ## Jan 1 NA NA ## Feb 1 15.74074 8.888889 ## Mar 1 16.66667 9.444444 ## Apr 1 17.22222 10.000000 ## May 1 18.14815 10.740741 ## Jun 1 18.88889 11.481481 ## Jul 1 19.62963 12.222222 ## Aug 1 20.37037 12.777778 ## Sep 1 20.92593 12.962963 ## Oct 1 20.18519 12.222222 ## Nov 1 17.77778 10.555556 ## Dec 1 NA NA moving average of CO2 data Difference shows time-local fluctuations, a component of the data library(dslabs) data(&quot;greenhouse_gases&quot;) GHGwide &lt;- pivot_wider(greenhouse_gases, names_from = gas, values_from = concentration) CO2 &lt;- ts(GHGwide$CO2, frequency = 0.05) library(forecast) CO2ma &lt;- ma(CO2, order=7) plot(CO2) plot(CO2ma) plot(CO2-CO2ma) 8.2.2 loess (local regression) smoothing [place holder  need to work out] From: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/loess Local Polynomial Regression Fitting Fit a polynomial surface determined by one or more numerical predictors, using local fitting. From: http://r-statistics.co/Loess-Regression-With-R.html a non-parametric approach that fits regressions within local neighborhoods if X variables are bound within a range [?] 8.3 Decomposing time series : separating a time series into its constituent components. original data trend component, removes seasonal and remainder if seasonal, also a seasonal component. Note that season relates to the time unit. If 1 year, seasonality refers to the normal usage of seasons over a year. But if 1 day, seasons refers to different parts of a day, etc. irregular random remainder (time-local variation) There are two common decomposition methods, the classic method decompose using moving averages, which well look at first, then the loess method used by stl. Mauna Loa CO2 data with seasonality A good place to see the effect of seasonality is to look at the Mauna Loa CO2 data, which shows regular annual cycles, yet with a regularly increasing trend over the years. The decomposition shows the original observations, followed by a trend line that removes the seasonal and local (short-term) random irregularities, a detrended seasonal picture which removes that trend to just show the seasonal cycles, followed by the random irregularities. Note the vertical scale: the units are all the same  parts per million  so the actual amplitude of the seasonal cycle should be the same as the annual amplitude of the observations, its just scaled to the chart height, which tends to exaggerate the seasonal cycles and random irregularities. plot(decompose(co2)) Figure 8.1: Decomposition of Mauna Loa monthly co2 time series (in base) extending from 1959 to 1981 Seasonal decomposition using loess The stl function stands for Seasonal decomposition of Time series by Loess. Loess is a smoothing algorithm, that wor. If s.window = periodic the mean is used for smoothing; the seasonal values are removed and the remainder smoothed to find the trend. plot(stl(co2, s.window=&quot;periodic&quot;)) Marble Mountains karst resurgence study [Marbles] In the Marble Mountains karst resurgence study, water level water level has an annual seasonality that might be best understand by decomposition. library(tidyverse); library(lubridate) resurg &lt;- read_csv(system.file(&quot;extdata&quot;,&quot;resurgenceData.csv&quot;,package=&quot;iGIScData&quot;)) %&gt;% mutate(date_time = mdy_hms(paste(date, time))) %&gt;% dplyr::select(date_time, ATemp, BTemp, wlevelm, EC, InputV) wlevelm &lt;- ts(resurg$wlevelm, frequency = 12*365, start = c(1994, 266*12+7)) fit &lt;- stl(wlevelm, s.window=&quot;periodic&quot;) plot(fit) 8.3.1 lag regression [SolarRad_Temp] library(readxl) BugacSolstice &lt;- read_xls(system.file(&quot;extdata&quot;, &quot;SolarRad_Temp.xls&quot;, package=&quot;iGIScData&quot;), sheet=&quot;BugacHungary&quot;, col_types = &quot;numeric&quot;) %&gt;% filter(Year != &quot;YYYY&quot; &amp; `Day of Yr` &lt; 177 &amp; `Day of Yr` &gt; 168) %&gt;% dplyr::select(SolarRad, Tair) BugacSolsticeTS &lt;- ts(BugacSolstice, frequency = 48) plot(BugacSolstice, main=&quot;a simple scatter plot that illustrates hysteresis&quot;); plot(BugacSolsticeTS) mod &lt;- function(i) {sqrt(sum(lm(Tair~lag(SolarRad,i), data=BugacSolsticeTS)$resid**2))} for(i in 0:5){print(paste(i,&quot;:&quot;,mod(i),sep=&quot;&quot;))} ## [1] &quot;0:64.0135215060866&quot; ## [1] &quot;1:55.8980329398225&quot; ## [1] &quot;2:50.4934737943067&quot; ## [1] &quot;3:48.1321353382658&quot; ## [1] &quot;4:49.6489246846601&quot; ## [1] &quot;5:54.3634439433188&quot; SolarRad_comp &lt;- decompose(ts(BugacSolstice$SolarRad, frequency = 48)) Tair_comp &lt;- decompose(ts(BugacSolstice$Tair, frequency = 48)) seasonals &lt;- bind_cols(Rad = SolarRad_comp$seasonal, Temp = Tair_comp$seasonal) ggplot(seasonals) + geom_line(aes(x=seq_along(Rad), y=Rad/50), col=&quot;red&quot;, size=1) + geom_line(aes(x=seq_along(Temp), y=Temp), col=&quot;darkgreen&quot;,size=1) + scale_x_continuous(breaks = seq(0,480,12)) which.max(seasonals$Rad) ## [1] 23 which.max(seasonals$Temp) ## [1] 28 library(readxl) ManausSolstice &lt;- read_xls(system.file(&quot;extdata&quot;, &quot;SolarRad_Temp.xls&quot;, package=&quot;iGIScData&quot;), sheet=&quot;ManausBrazil&quot;, col_types = &quot;numeric&quot;) %&gt;% filter(Year != &quot;YYYY&quot; &amp; `Day of Yr` &lt; 177 &amp; `Day of Yr` &gt; 168) %&gt;% dplyr::select(SolarRad, Tair) ManausSolsticeTS &lt;- ts(ManausSolstice, frequency = 48) plot(ManausSolstice, main=&quot;Manaus Brazil&quot;); plot(ManausSolsticeTS) mod &lt;- function(i) {sqrt(sum(lm(Tair~lag(SolarRad,i), data=ManausSolsticeTS)$resid**2))} for(i in 0:5){print(paste(i,&quot;:&quot;,mod(i),sep=&quot;&quot;))} ## [1] &quot;0:36.6765871805181&quot; ## [1] &quot;1:33.572327106606&quot; ## [1] &quot;2:31.9789648893589&quot; ## [1] &quot;3:31.2029493549955&quot; ## [1] &quot;4:31.6171985286944&quot; ## [1] &quot;5:32.6933416047147&quot; SolarRad_comp &lt;- decompose(ts(ManausSolstice$SolarRad, frequency = 48)) Tair_comp &lt;- decompose(ts(ManausSolstice$Tair, frequency = 48)) seasonals &lt;- bind_cols(Rad = SolarRad_comp$seasonal, Temp = Tair_comp$seasonal) ggplot(seasonals) + geom_line(aes(x=seq_along(Rad), y=Rad/50), col=&quot;red&quot;, size=1) + geom_line(aes(x=seq_along(Temp), y=Temp), col=&quot;darkgreen&quot;,size=1) + scale_x_continuous(breaks = seq(0,480,12)) which.max(seasonals$Rad) ## [1] 30 which.max(seasonals$Temp) ## [1] 30 8.3.2 Ensemble Average For any time series data with meaningful time units like days or years (or even weeks when the data varies by days of the week  this was the case with COVID-19 data), ensemble averages are a good way to visualize changes in a variable over that time unit. As you could see from the decomposition and sequential graphics we created for the Bugac and Manaus solar radiation and temperature data, looking at what happens over one day as an ensemble of all of the days  where the mean value is displayed along with error bars based on standard deviations of the distribution  might be a useful figure. library(cowplot) Manaus &lt;- read_xls(system.file(&quot;extdata&quot;,&quot;SolarRad_Temp.xls&quot;,package=&quot;iGIScData&quot;), sheet=&quot;ManausBrazil&quot;, col_types = &quot;numeric&quot;) %&gt;% dplyr::select(Year:Tair) %&gt;% filter(Year != &quot;YYYY&quot;) ManausSum &lt;- Manaus %&gt;% group_by(Hr) %&gt;% summarize(meanRad = mean(SolarRad), meanTemp = mean(Tair), sdRad = sd(SolarRad), sdTemp = sd(Tair)) px &lt;- ManausSum %&gt;% ggplot(aes(x=Hr)) + scale_x_continuous(breaks=seq(0,24,3)) p1 &lt;- px + geom_line(aes(y=meanRad), col=&quot;blue&quot;) + geom_errorbar(aes(ymax = meanRad + sdRad, ymin = meanRad - sdRad)) + ggtitle(&quot;Manaus 2005 ensemble solar radiation&quot;) p2 &lt;- px + geom_line(aes(y=meanTemp),col=&quot;red&quot;) + geom_errorbar(aes(ymax=meanTemp+sdTemp, ymin=meanTemp-sdTemp)) + ggtitle(&quot;Manaus 2005 ensemble air temperature&quot;) plot_grid(plotlist=list(p1,p2), ncol=1, align=&#39;v&#39;) # from cowplot Bugac &lt;- read_xls(system.file(&quot;extdata&quot;,&quot;SolarRad_Temp.xls&quot;,package=&quot;iGIScData&quot;), sheet=&quot;BugacHungary&quot;, col_types = &quot;numeric&quot;) %&gt;% dplyr::select(Year:Tair) %&gt;% filter(Year != &quot;YYYY&quot;) BugacSum &lt;- Bugac %&gt;% group_by(Hr) %&gt;% summarize(meanRad = mean(SolarRad), meanTemp = mean(Tair), sdRad = sd(SolarRad), sdTemp = sd(Tair)) px &lt;- BugacSum %&gt;% ggplot(aes(x=Hr)) + scale_x_continuous(breaks=seq(0,24,3)) p1 &lt;- px + geom_line(aes(y=meanRad), col=&quot;blue&quot;) + geom_errorbar(aes(ymax = meanRad + sdRad, ymin = meanRad - sdRad)) + ggtitle(&quot;Bugac 2005 ensemble solar radiation&quot;) p2 &lt;- px + geom_line(aes(y=meanTemp),col=&quot;red&quot;) + geom_errorbar(aes(ymax=meanTemp+sdTemp, ymin=meanTemp-sdTemp)) + ggtitle(&quot;Bugac 2005 ensemble air temperature&quot;) plot_grid(plotlist=list(p1,p2), ncol=1, align=&#39;v&#39;) # from cowplot 8.3.3 Julian Dates Julian dates are the number of days since some start date The origin (Julian date 0) can be arbitrary Default for julian() is 1970-01-01 For climatological work where the year is very important, it usually means the number of days of any given year lubridates yday() gives you this. same thing as setting the origin for julian() as the 12-31 of the previous year. Useful in time series when the year is the time unit and observations are by days of the year (e.g. for climate data) library(lubridate) julian(ymd(&quot;2020-01-01&quot;)) ## [1] 18262 ## attr(,&quot;origin&quot;) ## [1] &quot;1970-01-01&quot; julian(ymd(&quot;1970-01-01&quot;)) ## [1] 0 ## attr(,&quot;origin&quot;) ## [1] &quot;1970-01-01&quot; yday(ymd(&quot;2020-01-01&quot;)) ## [1] 1 julian(ymd(&quot;2020-01-01&quot;), origin=ymd(&quot;2019-12-31&quot;)) ## [1] 1 ## attr(,&quot;origin&quot;) ## [1] &quot;2019-12-31&quot; To create a decimal yday including fractional days, heres a function: ydayDec &lt;- function(d) { yday(d)-1 + hour(d)/24 + minute(d)/1440 + second(d)/86400} 8.4 Learning more about time series in R It can be confusing, partly because of the major use of time series in the financial sector, so a search gets you quickly into the monetizing world One good source is actually at CRAN: https://cran.r-project.org/web/views/TimeSeries.html Time series data library: https://pkg.yangzhuoranyang.com/tsdl/ Understanding time series in R: https://a-little-book-of-r-for-time-series.readthedocs.io/ "],["communication.html", "9 Communication", " 9 Communication Communication of research is central to environmental data science, and while this can use many venues such as professional meetings and publications, outreach on the internet is especially important. This chapter will delve into probably the best way to build a web site for communicating our research: Shiny. In short, Shiny is used for building interactive web apps. Its from the RStudio folks, who provide some useful resources for building a Shiny app: https://shiny.rstudio.com https://shiny.rstudio.com/tutorial Hosting is the one challenge, though you can host on shinyapps.io. https://www.shinyapps.io/ which is free for 5 apps, 25 hours usage/month. [TODO: Add some examples of Shiny app code, though cant be run in bookdown, maybe creating a Shiny Rmarkdown script with runtime: shiny to access via a code package??] "],["appendix-building-a-data-package-for-github.html", "10 Appendix: Building a Data Package for GitHub 10.1 rda files 10.2 Raw data", " 10 Appendix: Building a Data Package for GitHub These are just some notes on building data packages, based mostly on Chapter 14 External Packages of r-pkgs.org, which also covers code packages. For our package, iGIScData, we provided data in two ways: rda files: normal external data that are ready to use as data frames, simple feature (sf) data, and rasters. raw data as CSVs, shapefiles and TIFFs. 10.1 rda files These files need to be prepared from data in R and go in the data folder. The process is made very easy by using usethis::use_data() package: 10.1.1 usethis::use_data() usethis::use_data() is used to add data as rda files to the data folder. These data can be data frames, simple features, rasters, and Im sure other things. I used a script addData.R that I put in the data-raw folder which built the data (usually with read_csv(), st_read(), or raster() and maybe some other processing like mutate, filter, etc.) to create the data set, and then usethis::use_data() to store it in the data folder. Heres an simple example with just a csv converted directly, and it takes care of storing the result in the data folder as an .rda file: sierraFeb &lt;- read_csv(&quot;data-raw/sierraFeb.csv&quot;) usethis::use_data(sierraFeb) 10.1.2 usethis::create_package() This creates the package and uses roxygen to document it. I think you just need to run this once, then the devtools::document() does the rest, and can be run again to update it. 10.1.3 devtools::document() This creates documentation on the data sets, using the file R/data.R, which will need to have lines of code similar to the following to document each data set. Note that the name of the data set goes last, in quotes. The formatting of the field names and descriptions is a bit tricky and doesnt follow normal R rules. As a result, sometimes my field names dont exactly match the actual field names. Maybe Ill get around to changing the original field names with rename. Note that the organization is important, with the title of the data first, a blank line, then a description, etc.: #&#39; Sierra February climate data #&#39; #&#39; Selection from SierraData to only include February data #&#39; #&#39; @format A data frame with 82 entries and 7 variables selected and renamed \\describe{ #&#39; \\item{STATION_NAME}{Station name} #&#39; \\item{COUNTY}{County Name} #&#39; \\item{ELEVATION}{Elevation in meters} #&#39; \\item{LATITUDE}{Latitude in decimal degrees} #&#39; \\item{LONGITUDE}{Longitude in decimal degrees} #&#39; \\item{PRECIPITATION}{February Average Precipitation in mm} #&#39; \\item{TEMPERATURE}{Febrary Average Temperature in degrees C} #&#39; } #&#39; @source \\url{https://www.ncdc.noaa.gov/} &quot;sierraFeb&quot; Once these are on GitHub, a user can simply install the package with devtools::install_github(\"iGISc/iGIScData\")  to use the iGIScData we created. Then to access the data just like built-in data, the user just needs to load that library with library(iGIScData) But we also wanted to provide raw CSVs, shapefiles and rasters, in order to demonstrate how to read those. 10.2 Raw data Raw data (e.g. CSVs, shapefiles, and rasters) are simply stored in the inst/extdata folder. Just create those folders and put the files there. Make sure to include all the files (like the multiple files that go with a shapefile). Then, to access the data once the data package is installed, the user just needs to use the system.file() function to provide the path and then use that with the appropriate read function; e.g. for a CSV, something like: csvPath &lt;- system.file(&quot;extdata&quot;,&quot;TRI_2017_CA.csv&quot;) TRI_2017_CA &lt;- read_csv(csvPath) "]]
