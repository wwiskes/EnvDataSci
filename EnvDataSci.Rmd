--- 
title: "Introduction to Environmental Data Science"
author: "Jerry Davis"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "Background, concepts and exercises for environmental data science methods applying the R language and various libraries for data manipulation, data analysis, spatial data/mapping, and time series. The output format for this example is bookdown::gitbook.  Book is under development, and currently just has five chapters, including this prerequisite chapter."
---

# Prerequisites

This book is intended to work in concert with a series of lectures and discussions among the participants in Geog 604/704 Environmental Data Science at San Francisco State University.  Data packages will be created on GitHub.

Currently (March-April 2021) four chapters of the book are initially being used in will be used in Geog 9031 Introduction to R in the GIS Certificate Program.

Participants need to have R and RStudio installed, and at least the following packages:

- tidyverse (to include ggplot2, dplyr, tidyr, stringr, etc.)
    - ggplot2
    - dplyr
    - stringr
    - tidyr
- lubridate
- palmerpenguins
- sf
- raster
- tmap

## Data
We'll be using data from various sources, but one repository that includes data we've developed in the iGISc at SFSU is stored on GitHub and you'll need to install that package. GitHub packages require a bit more work on 
the user's part since we need to first install `devtools` then use that to install the GitHub data package:

```
install.packages("devtools")
devtools::install_github("iGISc/iGIScData")
```
You may also need the rlang package installed. If you see a message about Rtools, you can ignore
it since that is only needed for building tools from C++ and things like that.

Then you can access it just like other built-in data by including:

```{r}
library(iGIScData)

```

To see what's in it, you'll see the various datasets listed in:

```{r}
data()
```


<img src="logoIGIScSFSU150.png">

<!--chapter:end:index.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
```{r include=FALSE}
library(tidyverse)
```

# Introduction to R

We're assuming you're either new to R or need a refresher.

We'll start with some basic R operations entered directly in the console in RStudio.

## Variables
Variables are objects that store values. Every computer language, like in math, stores
values by assigning them constants or results of expressions.
`x <- 5` uses the R standard assignment operator `<-` though you can also use `=`. 
We'll use `<-` because it is more common and avoids some confusion with other syntax.
```

```
Variable names must start with a letter, have no spaces, and not use any names 
that are built into the R language or used in package libraries, such as
reserved words like `for` or function names like `log()`
```{r}
x <- 5
y <- 8
longitude <- -122.4
latitude <- 37.8
my_name <- "Inigo Montoya"
```
To check the value of a variable or other object, you can just enter the name in 
the console, or even in the code in a code chunk. 
```{r}
x
y
longitude
latitude
my_name


```
This is counter to the way printing out values work in programming, and you will
need to know how this method works as well because you will want to use your code
to develop tools that accomplish things, and there are also limitations to what you
can see by just naming variables.

To see the values of variables in programming mode, use the `print()` function, 
or to concatenate character string output, use `paste()`: 
```{r}
print(x)
print(y)
print(latitude)
paste("The location is latitude", latitude, "longitude", longitude)
paste("My name is", my_name, "-- Prepare to die.")

```
## Functions
Once you have variables or other objects to work with, most of your work 
involves *functions* such as the well-known math functions
```
log10(100)
log(exp(5))
cos(pi)
sin(90 * pi/180)
```
Most of your work will involve functions and there are too many to name, 
even in the base functions, not to mention all the packages we will want to use. 
You will likely have already used the `install.packages()` and `library()` functions 
that add in an array of other functions.
Later we'll also learn how to write our own functions, a capability that is easy to
accomplish and also gives you a sense of what developing your own package might be like.

**Arithmetic operators**
There are of course all the normal arithmetic operators (that are actually functions)
like + - * /.  You're probably familiar with these from using equations in Excel if not 
in some other programming language you may have learned. These operators look a bit different
from how they'd look when creating a nicely formatted equation.$\frac{NIR - R}{NIR + R}$
instead has to look like `(NIR-R)/(NIR+R)`.  Similarly `*` *must* be used to multiply; there's no implied multiplication 
that we expect in a math equation like $x(2+y)$ which would need to be written `x*(2+y)`.

In contrast to those four well-known operators, the symbol used to exponentiate -- raise to a power -- 
varies among programming languages. R uses ** so the the Pythagorean theorem $c^2=a^2+b^2$ would be written `c**2 = a**2 + b**2` 
except for the fact that it wouldn't make sense as a statement to R. 
We'll need to talk about expressions and statements.

## Expressions and Statements

The concepts of expressions and statements are very important to understand in any programming language.

An *expression* in R (or any programming language) has a *value* just like a variable has a value.
An expression will commonly combine variables and functions to be *evaluated* to derive the value
of the expression. Here are some examples of expressions:
```
5
x
x*2
sin(x)
sqrt(a**2 + b**2)
(-b+sqrt(b**2-4*a*c))/2*a
paste("My name is", aname)
```

Note that some of those expressions used previously assigned variables -- x, a, b, c, aname. 

An expression can be entered in the console to display its current value.
```{r}
cos(pi)
print(cos(pi))
```

A *statement* in R does something. It represents a directive we're assigning to the computer, or
maybe the environment we're running on the computer (like RStudio, which then runs R). A simple
`print()` *statement* seems a lot like what we just did when we entered an expression in the console, but recognize that it *does something*:
```{r}
print("Hello, World")
```
Which is the same as just typing "Hello, World", but that's just because the job of the console is to display what we are looking for [where we are the ones *doing something*], or if our statement includes something to display.

Statements in R are usually put on one line, but you can use a semicolon to have multiple statements on one line, if desired:

```{r}
x <- 5; print(x); print(x**2)
```

Many (perhaps most) statements don't actually display anything. For instance:
```{r}
x <- 5
```
doesn't display anything, but it does assign the value 5 to the variable x, so it *does something*. It's an *assignment statement* and uses that special assignment operator `<-` .  Most languages just use `=` which the designers of R didn't want to use, to avoid confusing it with the equal sign meaning "is equal to". 

*An assignment statement assigns an expression to a variable.* If that variable already exists, it is reused with the new value. For instance it's completely legit (and commonly done in coding) to update the variable in an assignment statement.  This is very common when using a counter variable:
```
i = i + 1
```
You're simply updating the index variable with the next value. This also illustrates why it's *not* an equation:  $i=i+1$ doesn't work as an equation (unless i is actually $\infty$ but that's just really weird.)

And `c**2 = a**2 + b**2` doesn't make sense as an R statement because `c**2` isn't a variable to be created. 
The `**` part is interpreted as *raise to a power*.  What is to the left of the assignment operator `=` *must* be a variable to be assigned the value of the expression.

## Data Types
Variables, constants and other data elements in R have data types.
Common types are numeric and character.
```{r}
x <- 5
class(x)
class(4.5)
class("Fred")
```
### Integers
By default, R creates double-precision floating-point numeric variables 
To create integer variables:
- append an L to a constant, e.g. `5L` is an integer 5
- convert with `as.integer`
We're going to be looking at various `as.` functions in R, more on that later, 
but we should look at `as.integer()` now.  Most other languages use `int()` for this,
and what it does is converts *any number* into an integer, *truncating* it to an
integer, not rounding it. 

```{r}
as.integer(5)
as.integer(4.5)
```
To round a number, there's a `round()` function or you can easily use `as.integer` adding 0.5:
```{r}
x <- 4.8
y <- 4.2
as.integer(x + 0.5)
round(x)
as.integer(y + 0.5)
round(y)
```


Integer divison:
```{r}
5 %/% 2
```
Integer remainder from division (the modulus, using a `%%` to represent the modulo):
```{r}
5 %% 2
```
Surprisingly, the values returned by integer division or the remainder are not stored as integers.  R seems to prefer floating point...

## Rectangular data
A common data format used in most types of research is *rectangular* data such as in a spreadsheet,
with rows and columns, where rows might be *observations* and columns might be *variables*.
We'll read this type of data in from spreadsheets or even more commonly from comma-separated-variable (CSV)
text files that spreadsheet programs like Excel commonly read in just like their native format.
```{r include=FALSE}
## sierraFeb <- read_csv("data/sierraFeb.csv")
## Don't need this anymore, as long as the iGIScData is installed from GitHub
```
```{r}
library(iGIScData)
sierraFeb
```
## Data Structures in R
We looked briefly at numeric and character string (we'll abbreviate simply as "string" from here on).
We'll also look at factors and dates/times later on.

### Vectors
A vector is an ordered collection of numbers, strings, vectors, data frames, etc.
What we mostly refer to as vectors are formally called *atomic vectors* which requires
that they be *homogeneous* sets of whatever type we're referring to, such as a vector of numbers, 
or a vector of strings, or a vector of dates/times.

You can create a simple vector with the `c()` function:
```{r}
lats <- c(37.5,47.4,29.4,33.4)
lats
states = c("VA", "WA", "TX", "AZ")
states
zips = c(23173, 98801, 78006, 85001)
zips
```
The class of a vector is the type of data it holds

```{r}

```


```{r}
temp <- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7)
class(temp)
```
Vectors can only have one data class, and if mixed with character types, numeric elements will become character:
```{r}
mixed <- c(1, "fred", 7)
class(mixed)
mixed[3]   # gets a subset, example of coercion
```

#### NA
Data science requires dealing with missing data by storing some sort of null value, called various things:
- null
- nodata
- NA "not available" or "not applicable"
```{r}
as.numeric(c("1","Fred","5")) # note NA introduced by coercion
```
Ignoring NA in statistical summaries is commonly used. Where normally the summary statistic can only return NA...
```{r}
mean(as.numeric(c("1", "Fred", "5")))
```
... with `na.rm=T` you can still get the result for all actual data:
```{r}
mean(as.numeric(c("1", "Fred", "5")), na.rm=T)
```
Don't confuse with `nan` ("not a number") which is used for things like imaginary numbers (explore the help for more on this)

```{r}
is.na(NA)
is.nan(NA)
is.na(as.numeric(''))
is.nan(as.numeric(''))
i <- sqrt(-1)
is.na(i) # interestingly nan is also na
is.nan(i)
```
#### Sequences
An easy way to make a vector from a sequence of values.  The following 3 examples are equivalent:
```
seq(1,10)
c(1:10)
c(1,2,3,4,5,6,7,8,9,10)
```
The seq() function has special uses like using a step parameter:
```{r}
seq(2,10,2)
```
#### Vectorization and vector arithmetic
Arithmetic on vectors operates element-wise
```{r}
elev <- c(52,394,510,564,725,848,1042,1225,1486,1775,1899,2551)
elevft <- elev / 0.3048
elevft

```
Another example, with 2 vectors:
```{r}
temp03 <- c(13.1,11.4,9.4,10.9,8.9,8.4,6.7,7.6,2.8,1.6,1.2,-2.1)
temp02 <- c(10.7,9.7,7.7,9.2,7.3,6.7,4.0,5.0,0.9,-1.1,-0.8,-4.4)
tempdiff <- temp03 - temp02
tempdiff

```
#### Plotting vectors
Vectors of Feb temperature, elevation and latitude at stations in the Sierra:
```{r}
temp <- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7, 4.0, 5.0, 0.9, -1.1, -0.8, -4.4)
elev <- c(52, 394, 510, 564, 725, 848, 1042, 1225, 1486, 1775, 1899, 2551)
lat <- c(39.52, 38.91, 37.97, 38.70, 39.09, 39.25, 39.94, 37.75, 40.35, 39.33, 39.17, 38.21)

```

**Plot individually**
```{r fig.cap="Temperature"}
plot(temp)
```
```{r fig.cap="Elevation"}
plot(elev)
```
```{r fig.cap="Latitude"}
plot(lat)
```

**Then plot as a scatterplot**
```{r fig.cap="Temperature~Elevation"}
plot(elev,temp)
```
#### Named indices
Vector indices can be named.
```{r}
codes <- c(380, 124, 818)
codes
codes <- c(italy = 380, canada = 124, egypt = 818)
codes
str(codes)
```
Why?  I guess so you can refer to observations by name instead of index. 
The following are equivalent:
```{r}
codes[1]
codes["italy"]
```

### Lists
Lists can be heterogeneous, with multiple class types. Lists are actually used a lot in R, but we won't see them for a while.

### Matrices
Vectors are commonly used as a column in a matrix (or as we'll see, a data frame), like a variable
```{r}
temp <- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7, 4.0, 5.0, 0.9, -1.1, -0.8, -4.4)
elev <- c(52, 394, 510, 564, 725, 848, 1042, 1225, 1486, 1775, 1899, 2551)
lat <- c(39.52, 38.91, 37.97, 38.70, 39.09, 39.25, 39.94, 37.75, 40.35, 39.33, 39.17, 38.21)
```
**Building a matrix from vectors as columns**
```{r}
sierradata <- cbind(temp, elev, lat)
class(sierradata)
```

#### Dimensions for arrays and matrices
Note:  a matrix is just a 2D array.  Arrays have 1, 3, or more dimensions.
```{r}
dim(sierradata)
```

```{r}
a <- 1:12
dim(a) <- c(3, 4)   # matrix
class(a)
dim(a) <- c(2,3,2)  # 3D array
class(a)
dim(a) <- 12        # 1D array
class(a)
b <- matrix(1:12, ncol=1)  # 1 column matrix is allowed

```
### Data frames
A data frame is a database with fields (as vectors) with records (rows), so is very important for data analysis and GIS.  They're kind of like a spreadsheet with rules (first row is field names, fields all one type). So even though they're more complex than a list, we use them so frequently they become quite familiar [whereas I continue to find lists confusing, especially when discovering them as what a particular function returns.]

```{r warning=FALSE}
library(palmerpenguins)
data(package = 'palmerpenguins')
head(penguins)
```
**Creating a data frame out of a matrix**
```{r fig.cap="Temperature~Elevation"}
mydata <- as.data.frame(sierradata)
plot(data = mydata, x = elev, y = temp)
```

**Read a data frame from a CSV**

We'll be looking at this more in the next chapter, but a common need is to read data from a spreadsheet stored in the CSV format.  Normally, you'd have that stored with your project and can just specify the file name, but we'll access CSVs from the iGIScData package. Since you have this installed, it will already be on your computer, but not in your project folder. The path to it can be derived using the `system.file()` function.  

Reading a csv in `readr` (part of the tidyverse that we'll be looking at in the next chapter) is done with `read_csv()`:

```{r}
library(readr)
csvPath <- system.file("extdata","TRI_1987_BaySites.csv", package="iGIScData")
TRI87 <- read_csv(csvPath)
TRI87
```

**Sort, Index, & Max/Min**

```{r}
head(sort(TRI87$air_releases))
index <- order(TRI87$air_releases)
head(TRI87$FACILITY_NAME[index])   # displays facilities in order of their air releases
i_max <- which.max(TRI87$air_releases)
TRI87$FACILITY_NAME[i_max]   # was NUMMI at the time
```

### Factors

Factors are vectors with predefined values
- Normally used for categorical data.
- Built on an *integer* vector
- Levels are the set of predefined values.

```{r}
fruit <- factor(c("apple", "banana", "orange", "banana"))
fruit   # note that levels will be in alphabetical order
class(fruit)
typeof(fruit)
```

An equivalent conversion:

```{r}
fruitint <- c(1, 2, 3, 2) # equivalent conversion
fruit <- factor(fruitint, labels = c("apple", "banana", "orange"))
str(fruit)

```

#### Categorical Data and Factors

While character data might be seen as categorical (e.g. "urban", "agricultural", "forest" land covers), to be used as categorical variables they must be made into factors.

```{r}
grain_order <- c("clay", "silt", "sand")
grain_char <- sample(grain_order, 36, replace = TRUE)
grain_fact <- factor(grain_char, levels = grain_order)
grain_char
grain_fact
```

To make a categorical variable a factor:
```{r}
fruit <- c("apples", "oranges", "bananas", "oranges")
farm <- c("organic", "conventional", "organic", "organic")
ag <- as.data.frame(cbind(fruit, farm))
ag$fruit <- factor(ag$fruit)
ag$fruit
```
**Factor example**
```{r}
sierraFeb$COUNTY <- factor(sierraFeb$COUNTY)
str(sierraFeb$COUNTY)
```

## Programming and Logic

Given the exploratory nature of the R language, we sometimes forget that it provides
significant capabilities as a programming language where we can solve more 
complex problems by coding procedures and using logic to control the process
and handle a range of possible scenarios.

Programming languages are used for a wide range of purposes, from developing operating
systems built from low-level code to high-level *scripting* used to run existing functions
in libraries. R and Python are commonly used for scripting, and you may be familiar with
using arcpy to script ArcGIS geoprocessing tools. But whether low- or high-level, some common
operational structures are used in all computer programming languages:

- Conditional operations: *If* a condition is true, do this, and maybe otherwise do something *else*.

  `if x!=0 {print(1/x)} else {print("Can't divide by 0")}`

- Loops

  `for(i in 1:10) print(paste(i, 1/i))`

- Functions (defining your own then using it in your main script)

```{r}
turnright <- function(ang){(ang + 90) %% 360}
turnright(c(260, 270, 280))
```

**Free-standing scripts**

As we move forward, we'll be wanting to develop complete, free-standing scripts that have all of the needed libraries and data.
Your scripts should stand on their own. One example of this that may seem insignificant is using print() statements instead
of just naming the object or variable in the console. While that is common in exploratory work, we need to learn to 
create free-standing scripts.  

However, "free standing" still allows for loading libraries of functions we'll be using. 
We're still talking about high-level (*scripting*), not low-level programming, so we can depend on those libraries that 
any user can access by installing those packages. If we develop our own packages, we just need to provide the user the ability to install
those packages. 

### Subsetting with logic

We'll use a package that includes data from 
Irizarry, Rafael (2020) *Introduction to Data Science* section 2.13.1.

Identify all states with murder rates ≤ that of Italy.


```{r}
library(dslabs)
data(murders)
murder_rate <- murders$total / murders$population * 100000
i <- murder_rate <= 0.71 
murders$abb[i]
```
**which**

```{r}
library(readr)
TRI87 <- read_csv("data/TRI_1987_BaySites.csv")
i <- which(TRI87$air_releases > 1e6)
TRI87$FACILITY_NAME[i]
```

**%in%**
```{r}
library(readr)
csvPath = system.file("extdata","TRI_1987_BaySites.csv", package="iGIScData")
TRI87 <- read_csv(csvPath)
i <- TRI87$COUNTY %in% c("NAPA","SONOMA")
TRI87$FACILITY_NAME[i]

```

### Apply functions

There are many apply functions in R, and they largely obviate the need for looping.  For instance: 

- `apply` derives values at margins of rows and columns, e.g. to sum across rows or down columns
```{r}
# matrix apply – the same would apply to data frames
matrix12 <- 1:12
dim(matrix12) <- c(3,4)
rowsums <- apply(matrix12, 1, sum)
colsums <- apply(matrix12, 2, sum)
sum(rowsums)
sum(colsums)
zero <- sum(rowsums) - sum(colsums)
matrix12
```

Apply functions satisfy one of the needs that spreadsheets are used for.  Consider how of ten you use sum, mean or similar  functions in Excel.

**`sapply`**

sapply applies functions to either:

- all elements of a vector – unary functions only
```{r}
sapply(1:12, sqrt)
```

- or all variables of a data frame (not a matrix), where it works much like a column-based apply (since variables are columns) but more easily interpreted without the need of specifying columns with 2:

```{r}
sapply(cars,mean)  # same as apply(cars,2,mean)
```

```{r}
temp02 <- c(10.7,9.7,7.7,9.2,7.3,6.7,4.0,5.0,0.9,-1.1,-0.8,-4.4)
temp03 <- c(13.1,11.4,9.4,10.9,8.9,8.4,6.7,7.6,2.8,1.6,1.2,-2.1)
sapply(as.data.frame(cbind(temp02,temp03)),mean) # has to be a data frame
```

While various `apply` functions are in base R, the purrr package takes these further.  
See:  <a href="https://github.com/rstudio/cheatsheets/raw/master/purrr.pdf">purrr cheat sheet</a>




## Exercises

1. Assign variables for your name, city, state and zip code, and use `paste()` to combine them, and assign them to the variable `me`. What is the class of `me`?

2. Knowing that trigonometric functions require angles (including azimuth directions) to be provided in radians, and that degrees can be converted into radians by dividing by 180 and multiplying that by pi, derive the sine of 30 degrees with an R expression.  (Base R knows what pi is, so you can just use `pi`)

3. If two sides of a right triangle on a map can be represented as $dX$ and $dY$ and the direct line path between them $c$, and the coordinates of 2 points on a map might be given as $(x1,y1)$ and $(x2,y2)$, with $dX=x2-x1$ and $dY=y2-y1$, use the Pythagorean theorem to derive the distance between them and assign that expression to $c$.

4. You can create a vector uniform random numbers from 0 to 1 using `runif(n=30)` where n=30 says to make 30 of them. Use the `round()` function to round each of the values, and provide what you created and explain what happened.

5. Create two vectors of 10 numbers each with the c() function, then assigning to x and y. Then plot(x,y), and provide the three lines of code you used to do the assignment and plot.

6. Change your code from #5 so that one value is NA (entered simply as `NA`, no quotation marks), and derive the mean value for x.  Then add `,na.rm=T` to the parameters for `mean()`. Also do this for y.  Describe your results and explain what happens.

7. Create two sequences, `a` and `b`, with `a` all odd numbers from 1 to 99, `b` all even numbers from 2 to 100. Then derive c through vector division of `b/a`.  Plot a and c together as a scatterplot.

8. Build the sierradata data frame from the data at the top of the **Matrices** section, also given here:
```
temp <- c(10.7, 9.7, 7.7, 9.2, 7.3, 6.7, 4.0, 5.0, 0.9, -1.1, -0.8, -4.4)
elev <- c(52, 394, 510, 564, 725, 848, 1042, 1225, 1486, 1775, 1899, 2551)
lat <- c(39.52, 38.91, 37.97, 38.70, 39.09, 39.25, 39.94, 37.75, 40.35, 39.33, 39.17, 38.21)
```
Create a data frame from it using the same steps, and plot temp against latitude.

9. From the `sierradata` matrix built with `cbind()`, derive colmeans using the `mean` parameter on the columns `2` for `apply()`.

10. Do the same thing with the sierra data data frame with `sapply()`.









<!--chapter:end:01-intro.Rmd-->

# Introduction to the tidyverse

At this point, we've learned the basics of working with the R language. From here we'll want to explore how to analyze data, both statistically and spatially. We're going to use an exploratory approach with significant application of visualization both in terms of graphs as well as maps. So let's start by exploring this exploratory approach...

## Background: Exploratory Data Analysis

In 1961, John Tukey proposed a new approach to data analysis, defining it as "Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data."  
<img src="img/Tukey1961.png" width="161" height="230" style="horizontal-align:right">He followed this up in 1977 with *Exploratory Data Analysis*. 


Exploratory data analysis (EDA) in part as an approach to analyzing data via summaries and graphics.  The key word is *exploratory*, in contrast with *confirmatory* statistics. Both are important, but ignoring exploration is ignoring enlightenment.

Some purposes of EDA are:

- to suggest hypotheses
- to assess assumptions on which inference will be based
- to select appropriate inferential statistical tools
- to guide further data collection

These concepts led to the development of S at Bell Labs (John Chambers, 1976), then R, built on clear design and extensive, clear graphics.

## The Tidyverse and what we'll explore in this chapter

The Tidyverse refers to a suite of R packages developed at RStudio (see <a href="https://rstudio.com">R Studio</a> and <a href="https://r4ds.had.co.nz">R for Data Science</a>) <img src="img/R4DataSciProcess.png" alt="R for Data Science Process" width="280" height="110"> (figure from Grolemund & Wickham 2017) for facilitating data processing and analysis. While R itself is designed around EDA, the Tidyverse takes it further. Some of the packages in the Tidyverse that are widely used are:

- dplyr : data manipulation like a database
- readr : better methods for reading and writing rectangular data
- tidyr : reorganization methods that extend dplyr's database capabilities
- purrr : expanded programming toolkit including enhanced "apply" methods
- tibble : improved data frame
- stringr : string manipulation library
- ggplot2 : graphing system based on "the grammar of graphics"

In this chapter, we'll be mostly exploring **dplyr**, with a few other things thrown in like reading data frames with **readr**. For simplicity, we can just include `library(tidyverse)` to get everything.

## Tibbles

Tibbles are an improved type of data frame

- part of the Tidyverse
- serve the same purpose as a data frame, and all data frame operations work

Advantages

- display better
- can be composed of more complex objects like lists, etc.
- can be grouped

How created

- Reading from a CSV, using one of a variety of Tidyverse functions similarly named to base functions:
    - `read_csv` creates a tibble (in general, underscores are used in the Tidyverse)
    - `read.csv` creates a regular data frame
- You can also use the `tibble()` function

```{r}
library(tidyverse) # includes readr, ggplot2, and dplyr which we'll use in this chapter
library(iGIScData)
csvPath <- system.file("extdata","TRI_1987_BaySites.csv", package="iGIScData")
TRI87 <- read_csv(csvPath)
a <- rnorm(10)
b <- runif(10)
ab <- tibble(a,b)
ab
```
### `read_csv` vs. `read.csv`

You might be tempted to use read.csv from base R

- They look a lot alike, so you might confuse them
- You don't need to load library(readr)
- read.csv "fixes" some things and that might be desired:
problematic field names like   `MLY-TAVG-NORMAL` become `MLY.TAVG.NORMAL`
- numbers stored as characters are converted to numbers
"01" becomes 1, "02" becomes 2, etc.

However, there are potential problems

- You may not want some of those changes, and want to specify those changes separately
- There are known problems that read_csv avoids

Recommendation:  Use `read_csv` and `write_csv`.

## Statistical summary of variables

A simple statistical summary is very easy to do:

```{r}
summary(eucoakrainfallrunoffTDR)
```

## Visualizing data with a Tukey box plot

```{r}
ggplot(data = eucoakrainfallrunoffTDR) + geom_boxplot(mapping = aes(x=site, y=runoffL_euc))
```

## Database operations with `dplyr`

As part of exploring our data, we'll typically simplify or reduce it for our purposes. 
The following methods are quickly discovered to be essential as part of exploring and analyzing data. 

- **select rows** using logic, such as population > 10000, with `filter`
- **select variable columns** you want to retain with `select`
- **add** new variables and assign their values with `mutate`
- **sort** rows based on a a field with `arrange` 
- **summarize** by group

### Select, mutate, and the pipe

**The pipe `%>%`**:  Read `%>%` as "and then..."  This is bigger than it sounds and opens up a lot of possibilities.  See example below, and observe how the expression becomes several lines long. In the process, we'll see examples of new variables with mutate and selecting (and in the process *ordering*) variables:

```{r}
runoff <- eucoakrainfallrunoffTDR %>%
  mutate(Date = as.Date(date,"%m/%d/%Y"),
         rain_subcanopy = (rain_oak + rain_euc)/2) %>%
  select(site, Date, rain_mm, rain_subcanopy, 
         runoffL_oak, runoffL_euc, slope_oak, slope_euc)
runoff
```


 *Note: to just rename a variable, use `rename` instead of `mutate`. It will stay in position.*
 
### filter
 
 `filter` lets you select observations that meet criteria, similar to an SQL WHERE clause.
 
```{r}
runoff2007 <- runoff %>%
  filter(Date >= as.Date("01/01/2007", "%m/%d/%Y"))
runoff2007
```
 **Filtering out NA with `!is.na`**
 
 Here's an important one. There are many times you need to avoid NAs.  
 We commonly see summary statistics using `na.rm = TRUE` in order to *ignore* NAs when calculating a statistic like `mean`.
 
 To simply filter out NAs from a vector or a variable use a filter:
 `feb_filt <- feb_s %>% filter(!is.na(TEMP))`
 
### Writing a data frame to a csv

Let's say you have created a data frame, maybe with read_csv

`runoff20062007 <- read_csv(csvPath)`

Then you do some processing to change it, maybe adding variables, reorganizing, etc., and you want to write out your new `eucoak`, so you just need to use `write_csv`

`write_csv(eucoak, "data/tidy_eucoak.csv")`

### Summarize by group

You'll find that you need to use this all the time with real data. You have a bunch of data where some categorical variable is defining a grouping, like our site field in the eucoak data. We'd like to just create average slope, rainfall, and runoff for each site. Note that it involves two steps, first defining which field defines the group, then the various summary statistics we'd like to store.  In this case all of the slopes under oak remain the same for a given site -- it's a *site* characteristic -- and the same applies to the euc site, so we can just grab the first value (mean would have also worked of course).

```{r}
eucoakSiteAvg <- runoff %>%
  group_by(site) %>%
  summarize(
    rain = mean(rain_mm, na.rm = TRUE),
    rain_subcanopy = mean(rain_subcanopy, na.rm = TRUE),
    runoffL_oak = mean(runoffL_oak, na.rm = TRUE),
    runoffL_euc = mean(runoffL_euc, na.rm = TRUE),
    slope_oak = first(slope_oak),
    slope_euc = first(slope_euc)
  )
eucoakSiteAvg
```


**Summarizing by group with TRI data**

```{r, message=FALSE}
csvPath <- system.file("extdata","TRI_2017_CA.csv", package="iGIScData")
TRI_BySite <- read_csv(csvPath) %>%
  mutate(all_air = `5.1_FUGITIVE_AIR` + `5.2_STACK_AIR`) %>%
  filter(all_air > 0) %>%
  group_by(FACILITY_NAME) %>%
  summarize(
    FACILITY_NAME = first(FACILITY_NAME),
    air_releases = sum(all_air, na.rm = TRUE),
    mean_fugitive = mean(`5.1_FUGITIVE_AIR`, na.rm = TRUE), 
    LATITUDE = first(LATITUDE), LONGITUDE = first(LONGITUDE))

```

### Count

Count is a simple variant on summarize by group, since the only statistic is the count of events.

```{r}
tidy_eucoak %>% count(tree)

```

**Another way is to use n():**

```{r}
tidy_eucoak %>%
  group_by(tree) %>%
  summarize(n = n())
```

### Sorting after summarizing

Using the marine debris data from NOAA Marine Debris Program's *Marine Debris Monitoring and Assessment Project*
```{r}
shorelineLatLong <- ConcentrationReport %>%
  group_by(`Shoreline Name`) %>%
  summarize(
    latitude = mean((`Latitude Start`+`Latitude End`)/2),
    longitude = mean((`Longitude Start`+`Longitude End`)/2)
  ) %>%
  arrange(latitude)
shorelineLatLong

```

## The dot operator

The dot "." operator derives from UNIX syntax, and refers to "here".

- For accessing files in the current folder, the path is "./filename"

A similar specification is used in piped sequences

- The advantage of the pipe is you don't have to keep referencing the data frame.
- The dot is then used to connect to items inside the data frame:

```{r}
csvPath <- system.file("extdata","TRI_1987_BaySites.csv", package="iGIScData")
TRI87 <- read_csv(csvPath)
stackrate <- TRI87 %>%
  mutate(stackrate = stack_air/air_releases) %>%
  .$stackrate
head(stackrate)
```

## Exercises

1. Create a tibble with 20 rows of two variables `norm` and `unif` with `norm` created with `rnorm()` and `unif` created with `runif()`.

2. Read in "data/TRI_2017_CA.csv" in two ways, as a normal data frame assigned to df and as a tibble assigned to tb. What field names result for what's listed in the CSV as `5.1_FUGITIVE_AIR`?

3. Use the summary function to investigate the variables in either the data.frame or tibble you just created. What type of field and what values are assigned to BIA_CODE?

4. Create a boxplot of `body_mass_g` by `species` from the `penguins` data frame in the palmerpenguins package. Access the data with data(package = 'palmerpenguins'), and also remember `library(ggplot2)` or `library(tidyverse)`.

```{r include=FALSE}
library(tidyverse)
library(palmerpenguins)
data(package = 'palmerpenguins')
```

```{r include=FALSE}
ggplot(penguins, aes(x=species, y=body_mass_g)) + geom_boxplot()
```


5. Use select, mutate, and the pipe to create a penguinMass tibble where the only original variable retained is species, but with body_mass_kg created as $\frac{1}{1000}$ the body_mass_g. The statement should start with `penguinMass <- penguins` and use a pipe plus the other functions after that.

```{r include=FALSE}
penguinMass <- penguins %>%
  mutate(body_mass_kg = body_mass_g / 1000) %>%
  select(species, body_mass_kg)
penguinMass
```

6. Now, also with penguins, create FemaleChinstaps to include only the female Chinstrap penguins. Start with `FemaleChinstraps <- penguins %>%`

```{r include=FALSE}
FemaleChinstraps <- penguins %>%
  filter(sex == "female") %>%
  filter(species == "Chinstrap")
FemaleChinstraps
```

7. Now, summarize by `species` groups to create mean and standard deviation variables from `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. Preface the variable names with either `avg.` or `sd.` Include `na.rm=T` with all statistics function calls.

```{r include=FALSE}
penguins %>%
  group_by(species, sex) %>%
  summarize(avg.bill_length_mm = mean(bill_length_mm, na.rm=T),
            avg.bill_depth_mm = mean(bill_depth_mm, na.rm=T),
            avg.flipper_length_mm = mean(flipper_length_mm, na.rm=T),
            avg.body_mass_g = mean(body_mass_g, na.rm=T),
            sd.bill_length_mm = sd(bill_length_mm, na.rm=T),
            sd.bill_depth_mm = sd(bill_depth_mm, na.rm=T),
            sd.flipper_length_mm = sd(flipper_length_mm, na.rm=T),
            sd.body_mass_g = sd(body_mass_g, na.rm=T))

```

8. Create an penguinSort tibble, sorted by `body_mass_g`.

```{r include=FALSE}
penguins %>%
  arrange(body_mass_g)
```












<!--chapter:end:02-tidyverse.Rmd-->

# Visualization

In this section we'll explore visualization methods in R. Visualization has been a key element of R since its inception, since visualization is central to the exploratory philosophy of the language. The base *plot* system generally does a good job in coming up with the most likely graphical output based on the data you provide. 

```{r include=FALSE}
library(tidyverse)
library(palmerpenguins)
data(package = "palmerpenguins")
```

```{r fig.cap="Flipper length by species"}
plot(penguins$body_mass_g, penguins$flipper_length_mm)
plot(penguins$species, penguins$flipper_length_mm)
```

## ggplot2

We'll mostly focus however on gpplot2, based on the *Grammar of Graphics* because it provides considerable control over your graphics while remaining fairly easily readable, as long as you buy into its grammar.

ggplot2 looks at three aspects of a graph:

- data : where are the data coming from?
- geometry : what type of graph are we creating?
- aesthetics : what choices can we make about symbology and how do we connect symbology to data?

See https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf

The ggplot2 system provides plots of single and multiple variables, using various coordinate systems (including geographic).


## Plotting one variable

- continuous
    - histograms
    - density plots
    - dot plots
- discrete
    - bar
    
```{r}
library(iGIScData)
library(tidyverse)
summary(XSptsNDVI)
ggplot(XSptsNDVI, aes(vegetation)) + 
  geom_bar()
```

### Histogram

**First, to prepare the data, we need to use a pivot_longer on XSptsNDVI:**

```{r}
XSptsPheno <- XSptsNDVI %>%
  filter(vegetation != "pine") %>%
  pivot_longer(cols = starts_with("NDVI"), names_to = "phenology", values_to = "NDVI") %>%
  mutate(phenology = str_sub(phenology, 5, str_length(phenology)))
```

```{r warning=FALSE, fig.cap="Distribution of NDVI, Knuthson Meadow"} 
XSptsPheno <- read_csv("data/XSptsPheno.csv")

XSptsPheno %>%
  ggplot(aes(NDVI)) + 
  geom_histogram(binwidth=0.05)
```

**Normal histogram**: easier to visualize the distribution, see modes
```{r warning=FALSE, fig.cap="Distribution of Average Monthly Temperatures, Sierra Nevada"}
sierraData %>%
    ggplot(aes(TEMPERATURE)) +
  geom_histogram(fill="dark green")
```

**Cumulative histogram with proportions**: easier to see percentiles, median
```{r warning=FALSE, fig.cap="Cumulative Distribution of Average Monthly Temperatures, Sierra Nevada"}
n <- length(sierraData$TEMPERATURE)
sierraData %>%
  ggplot(aes(TEMPERATURE)) +
  geom_histogram(aes(y=cumsum(..count..)/n), fill="dark goldenrod")

```

### Density Plot

Density represents how much out of the total. The total area (sum of widths of bins times densities of that bin) adds up to 1.

```{r fig.cap="Density plot of NDVI, Knuthson Meadow"}
XSptsPheno %>% 
  ggplot(aes(NDVI)) + 
  geom_density()

```

Note that NDVI values are <1 so bins are very small numbers, so in this case densities can be >1.

**Using alpha and mapping phenology as fill color**.  This illustrates two useful ggplot methods:

- "mapping" a variable (phenology) to an aesthetic property (fill color of the density polygon)
- setting a a property (alpha = 0.2) to all polygons of the density plot.  The alpha channel of colors defines its opacity, from invisible (0) to opaque (1) so is commonly used to set as its reverse, transparency.

```{r}
XSptsPheno %>%
  ggplot(aes(NDVI, fill=phenology)) +
  geom_density(alpha=0.2)
```

```{r include=FALSE}
## DON'T REALLY NEED THIS -- STORED AS tidy_eucoak
library(tidyverse)
runoffPivot <- eucoakrainfallrunoffTDR %>%
  pivot_longer(cols = starts_with("runoffL_"), 
               names_to = "tree", values_to = "runoff_L") %>%
  mutate(
    tree = str_sub(tree, str_length(tree)-2, str_length(tree)),
    Date = as.Date(date, "%m/%d/%Y"))  # variable date is just a string, needs converting to date format

euc <- runoffPivot %>%
  filter(tree == "euc") %>%
  mutate(
    rain_subcanopy = rain_euc,
    slope = slope_euc,
    aspect = aspect_euc,
    surface_tension = surface_tension_euc,
    runoff_rainfall_ratio = runoff_rainfall_ratio_euc) %>%
  select(site, `site #`, tree, Date, month, rain_mm, rain_subcanopy, slope, aspect, runoff_L, surface_tension, runoff_rainfall_ratio)

oak <- runoffPivot %>%
  filter(tree == "oak") %>%
  mutate(
    rain_subcanopy = rain_oak,
    slope = slope_oak,
    aspect = aspect_oak,
    surface_tension = surface_tension_oak,
    runoff_rainfall_ratio = runoff_rainfall_ratio_oak) %>%
  select(site, `site #`, tree, Date, month, rain_mm, rain_subcanopy, slope, aspect, runoff_L, surface_tension, runoff_rainfall_ratio)

eucoak <- rbind(euc, oak)
write_csv(eucoak, "data/tidy_eucoak.csv")

```

```{r warning=FALSE, fig.cap="Runoff under Eucalyptus and Oak in Bay Area sites"}
tidy_eucoak %>%
  ggplot(aes(log(runoff_L),fill=tree)) +
  geom_density(alpha=0.2)
```


### boxplot

```{r warning=FALSE, fig.cap="Runoff under Eucalyptus and Oak, Bay Area Sites"}
ggplot(data = tidy_eucoak) +
  geom_boxplot(aes(x = site, y = runoff_L))

```
**Get color from tree within `aes()`**
```{r warning=FALSE, fig.cap="Runoff at Bay Area Sites, colored as Eucalyptus and Oak"}
ggplot(data = tidy_eucoak) +
  geom_boxplot(aes(x=site, y=runoff_L, color=tree))

```

**Visualizing soil CO_2_ data with a Tukey box plot**

```{r warning=FALSE, fig.cap = "Visualizing soil CO_2_ data with a Tukey box plot"}
co2 <- soilCO2_97
co2$SITE <- factor(co2$SITE)  # in order to make the numeric field a factor
ggplot(data = co2, mapping = aes(x = SITE, y = `CO2%`)) +
  geom_boxplot()

```

## Plotting two variables
### Two continuous variables

We've looked at this before -- the scatterplot

```{r warning=FALSE, fig.cap="Scatter plot of February temperature vs elevation"}
ggplot(data=sierraFeb) + 
  geom_point(mapping = aes(TEMPERATURE, ELEVATION))

```

- The aes ("aesthetics") function specifies the variables to use as x and y coordinates 
- geom_point creates a scatter plot of those coordinate points

**Set color for all (*not* in aes())**

```{r warning=FALSE}
ggplot(data=sierraFeb) + 
  geom_point(aes(TEMPERATURE, ELEVATION), color="blue")
```

- color is defined outside of aes, so is applies to all points.
- mapping is first argument of  geom_point, so `mapping = ` is not needed.

### Two variables, one discrete

```{r warning=FALSE, fig.cap="Two variables, one discrete"}
ggplot(tidy_eucoak) +
  geom_bar(aes(site, runoff_L), stat="identity")
```

## Color systems

You can find a lot about color systems. See these sources:

<a href="http://sape.inf.usi.ch/quick-reference/ggplot2/colour">http://sape.inf.usi.ch/quick-reference/ggplot2/colour</a>
<a href="http://applied-r.com/rcolorbrewer-palettes/">http://applied-r.com/rcolorbrewer-palettes/</a>

### Color from variable, in aesthetics

In this graph, color is defined inside aes, so is based on COUNTY

```{r warning=FALSE, fig.cap="Color set within aes()"}
ggplot(data=sierraFeb) + 
  geom_point(aes(TEMPERATURE, ELEVATION, color=COUNTY))

```

**Plotting lines using the same x,y in aesthetics**
```{r warning=FALSE, fig.cap="Using aesthetics settings for both points and lines"}
sierraFeb %>%
  ggplot(aes(TEMPERATURE,ELEVATION)) +
  geom_point(color="blue") +
  geom_line(color="red")
```

Note the use of pipe to start with the data then apply ggplot.

**River map & profile**

```{r fig.cap="Longitudinal Profiles"}
x <- c(1000, 1100, 1300, 1500, 1600, 1800, 1900)
y <- c(500, 700, 800, 1000, 1200, 1300, 1500)
z <- c(0, 1, 2, 5, 25, 75, 150)
d <- rep(NA, length(x))
longd <- rep(NA, length(x))
s <- rep(NA, length(x))
for(i in 1:length(x)){
  if(i==1){longd[i] <- 0; d[i] <-0}
  else{
    d[i] <- sqrt((x[i]-x[i-1])^2 + (y[i]-y[i-1])^2)
    longd[i] <- longd[i-1] + d[i]
    s[i-1] <- (z[i]-z[i-1])/d[i]}}
longprofile <- bind_cols(x=x,y=y,z=z,d=d,longd=longd,s=s)
ggplot(longprofile, aes(x,y)) +
  geom_line(mapping=aes(col=s), size=1.2) + 
  geom_point(mapping=aes(col=s, size=z)) +
  coord_fixed(ratio=1) + scale_color_gradient(low="green", high="red") +
  ggtitle("Simulated river path, elevations and slopes")
ggplot(longprofile, aes(longd,z)) + geom_line(aes(col=s), size=1.5) + geom_point()  +
  scale_color_gradient(low="green", high="red") +
  ggtitle("Elevation over longitudinal distance upstream")
ggplot(longprofile, aes(longd,s)) + geom_point(aes(col=s), size=3) +
  scale_color_gradient(low="green", high="red") +
  ggtitle("Slope over longitudinal distance upstream")
#summary(lm(s~longd, data=longprofile))

```

### Trend line

```{r fig.cap="Trend line using geom_smooth with a linear model"}
sierraFeb %>%
  ggplot(aes(TEMPERATURE,ELEVATION)) +
  geom_point(color="blue") +
  geom_smooth(color="red", method="lm")

```

### General symbology
A useful vignette accessed by `vignette("ggplot2-specs")` lets you see aesthetic specifications for symbols, including:

- Color & fill
- Lines
   - line type, size, ends
- Polygon
   - border color, linetype, size
   - fill
- Points
   - shape
   - size
   - color & fill
   - stroke
- Text
   - font face & size
   - justification

#### Categorical symbology

One example of a "Big Data" resource is EPA's Toxic Release Inventory that tracks releases from a wide array of sources, from oil refineries on down.  One way of dealing with big data in terms of exploring meaning is to use symbology to try to make sense of it.

```{r message=FALSE, fig.cap="EPA Toxic Release Inventory, as a big data set needing symbology clarification"}
csvPath <- system.file("extdata","TRI_2017_CA.csv", package="iGIScData")
TRI <- read_csv(csvPath) %>%
  filter(`5.1_FUGITIVE_AIR` > 100 & `5.2_STACK_AIR` > 100)
ggplot(data = TRI, aes(log(`5.2_STACK_AIR`), log(`5.1_FUGITIVE_AIR`), 
                       color = INDUSTRY_SECTOR)) +
       geom_point()
```

#### Graphs from grouped data

```{r warning=FALSE, fig.cap="NDVI symbolized by vegetation in two seasons"}
XSptsPheno %>%
  ggplot() +
  geom_point(aes(elevation, NDVI, shape=vegetation, 
                 color = phenology), size = 3) +
  geom_smooth(aes(elevation, NDVI, 
                 color = phenology), method="lm") 
```

```{r warning=FALSE, fig.cap="Eucalyptus and Oak: rainfall and runoff"}
ggplot(data = tidy_eucoak) +
  geom_point(mapping = aes(x = rain_mm, y = runoff_L, color = tree)) +
  geom_smooth(mapping = aes(x = rain_mm, y= runoff_L, color = tree), 
              method = "lm") +
  scale_color_manual(values = c("seagreen4", "orange3"))

```

#### Faceted graphs

This is another option to displaying groups of data, with parallel graphs

```{r warning=FALSE, fig.cap="Faceted graph alternative"}
ggplot(data = tidy_eucoak) +
  geom_point(aes(x=rain_mm,y=runoff_L)) +
  geom_smooth(aes(x=rain_mm,y=runoff_L), method="lm") +
  facet_grid(tree ~ .)

```

## Titles and subtitles

```{r warning=FALSE, fig.cap="Titles added"}
ggplot(data = tidy_eucoak) +
  geom_point(aes(x=rain_mm,y=runoff_L, color=tree)) +
  geom_smooth(aes(x=rain_mm,y=runoff_L, color=tree), method="lm") +
  scale_color_manual(values=c("seagreen4","orange3")) +
  labs(title="rainfall ~ runoff", 
       subtitle="eucalyptus & oak sites, 2016")

```

## Pairs Plot

```{r warning=FALSE, fig.cap="Pairs plot for Sierra Nevada stations variables"}
sierraFeb %>%
  select(LATITUDE, ELEVATION, TEMPERATURE, PRECIPITATION) %>%
  pairs()
```

## Exercises

1. Create a bar graph of the counts of the species in the penguins data frame.  What can you say about what it shows?

2. Use bind_cols in dplyr to create a tibble from built-in vectors state.abb and state.region, then use ggplot with geom_bar to create a bar graph of the four regions.

3. Convert the built-in time series `treering` into a tibble `tr`using the `tibble()` functions with the single variable assigned as `treering = treering`, then create a histogram, using that tibble and variable for the `data` and `x` settings needed.

```{r include=FALSE}
library(tidyverse)
tr <- tibble(treering = treering)
str(tr)
ggplot(data=tr, aes(x=treering)) + geom_histogram()
```

4. Create and display a new tibble `st` using `bind_cols` with `Name=state.name`, `Abb=state.abb`, `Region=state.region`, and a tibble created from `state.x77` with `as_tibble`.  

```{r include=FALSE}
#st <- as_tibble(state.x77)
st <- bind_cols(Name=state.name, Abb=state.abb, Region=state.region, as_tibble(state.x77))
st
```


5. From `st`, create a density plot from the variable `Frost` (number of days with frost for that state).  Approximately what is the modal value?

```{r include=FALSE}
#st <- as_tibble(state.x77)
ggplot(data=st, aes(x=Frost)) + geom_density()
```

6. From `st` create a a boxplot of `Area` by `Region`.  Which region has the highest and which has the lowest median Area?  Do the same for `Frost`.

```{r include=FALSE}
ggplot(data=st, aes(x=Region, y=Area)) + geom_boxplot()
ggplot(data=st, aes(x=Region, y=Frost)) + geom_boxplot()
```

7. From st, compare murder rate (y=Murder) to Frost (x) in a scatter plot, colored by Region.

```{r include=FALSE}
st %>%
  ggplot(aes(x=Frost, y=Murder)) + geom_point(aes(col=Region)) + geom_smooth(method="lm")
print(st$Name[which.max(st$Murder)])
```

8. Add a trend line (smooth) with method="lm" to your scatterplot, not colored by Region (but keep the points colored by Region). What can you say about what this graph is showing you?

9. Add a title to your graph.

10. Change your scatterplot to place labels using the Abb variable (still colored by Region) using `geom_label(aes(label=Abb, col=Region))`. Any observations about outliers?

```{r include=FALSE}
st %>%
  ggplot(aes(x=Frost, y=Murder)) + geom_label(aes(label=Abb, col=Region)) + geom_smooth(method="lm")

```


<!--chapter:end:03-visualization.Rmd-->

# Spatial R

We'll explore the basics of simple features (sf) for building spatial datasets, then some common mapping methods, probably:

- ggplot2
- tmap

## Spatial Data 

To work with spatial data requires extending R to deal with it using packages.  Many have been developed, but the field is starting to mature using international open GIS standards.

*`sp`*  (until recently, the dominant library of spatial tools)

- Includes functions for working with spatial data
- Includes `spplot` to create maps
- Also needs `rgdal` package for `readOGR` – reads spatial data frames.  

*`sf`* (Simple Features)

- ISO 19125 standard for GIS geometries
- Also has functions for working with spatial data, but clearer to use.
- Doesn't need many additional packages, though you may still need `rgdal` installed for some tools you want to use.
- Replacing `sp` and `spplot` though you'll still find them in code. We'll give it a try...
- Works with ggplot2 and tmap for nice looking maps.

Cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/sf.pdf

#### simple feature geometry sfg and simple feature column sfc



### Examples of simple geometry building in sf 

sf functions have the pattern st_* 

st means "space and time"

See Geocomputation with R at https://geocompr.robinlovelace.net/ or  https://r-spatial.github.io/sf/
	for more details, but here's an example of manual feature creation of sf geometries (sfg):

```{r message=FALSE}
library(tidyverse)
library(sf)
```


```{r fig.cap="Building simple geometries in sf"}
library(sf)
eyes <- st_multipoint(rbind(c(1,5), c(3,5)))
nose <- st_point(c(2,4))
mouth <- st_linestring(rbind(c(1,3),c(3, 3)))
border <- st_polygon(list(rbind(c(0,5), c(1,2), c(2,1), c(3,2), 
                              c(4,5), c(3,7), c(1,7), c(0,5))))
face <- st_sfc(eyes, nose, mouth, border)  # sfc = sf column 
plot(face)
```

The face was a simple feature column (sfc) built from the list of sfgs. 
An sfc just has the one column, so is not quite like a shapefile.

- But it can have a coordinate referencing system CRS, and so can be mapped.
- Kind of like a shapefile with no other attributes than shape

### Building a mappable sfc from scratch

```{r fig.cap="A simple map built from scratch with hard-coded data as simple feature columns"}
CA_matrix <- rbind(c(-124,42),c(-120,42),c(-120,39),c(-114.5,35),
  c(-114.1,34.3),c(-114.6,32.7),c(-117,32.5),c(-118.5,34),c(-120.5,34.5),
  c(-122,36.5),c(-121.8,36.8),c(-122,37),c(-122.4,37.3),c(-122.5,37.8),
  c(-123,38),c(-123.7,39),c(-124,40),c(-124.4,40.5),c(-124,41),c(-124,42))
NV_matrix <- rbind(c(-120,42),c(-114,42),c(-114,36),c(-114.5,36),
  c(-114.5,35),c(-120,39),c(-120,42))
CA_list <- list(CA_matrix);       NV_list <- list(NV_matrix)
CA_poly <- st_polygon(CA_list);   NV_poly <- st_polygon(NV_list)
sfc_2states <- st_sfc(CA_poly,NV_poly,crs=4326)  # crs=4326 specifies GCS
st_geometry_type(sfc_2states)
library(tidyverse)
ggplot() + geom_sf(data = sfc_2states)

```

**sf class**

Is like a shapefile:  has attributes to which geometry is added, and can be used like a data frame.

```{r fig.cap="Using an sf class to build a map, displaying an attribute"}
attributes <- bind_rows(c(abb="CA", area=423970, pop=39.56e6),
                        c(abb="NV", area=286382, pop=3.03e6))
twostates <- st_sf(attributes, geometry = sfc_2states)
ggplot(twostates) + geom_sf() + geom_sf_text(aes(label = abb))
```

### Creating features from shapefiles or tables

**sf's `st_read` reads shapefiles**

- shapefile is an open GIS format for points, polylines, polygons

You would normally have shapefiles (and all the files that go with them -- .shx, etc.)
stored on your computer, but we'll access one from the iGIScData external data folder:

```{r}
library(iGIScData)
library(sf)
shpPath <- system.file("extdata","CA_counties.shp", package="iGIScData")
CA_counties <- st_read(shpPath)
plot(CA_counties)
```

**`st_as_sf` converts data frames**

- using coordinates read from x and y variables, with crs set to coordinate system (4326 for GCS)

```{r}
sierraFebpts <- st_as_sf(sierraFeb, coords = c("LONGITUDE", "LATITUDE"), crs=4326)
plot(sierraFebpts)
```




```{r, message=FALSE, warning=FALSE, fig.cap="ggplot map of Bay Area TRI sites, census centroids, freeways"}
library(tidyverse)
library(sf)
library(iGIScData)
censusCentroids <- st_centroid(BayAreaTracts)
TRI_sp <- st_as_sf(TRI_2017_CA, coords = c("LONGITUDE", "LATITUDE"), 
        crs=4326) # simple way to specify coordinate reference
bnd <- st_bbox(censusCentroids)
ggplot() +
  geom_sf(data = BayAreaCounties, aes(fill = NAME)) +
  geom_sf(data = censusCentroids) +
  geom_sf(data = CAfreeways, color = "grey") +
  geom_sf(data = TRI_sp, color = "yellow") +
  coord_sf(xlim = c(bnd[1], bnd[3]), ylim = c(bnd[2], bnd[4])) +
  labs(title="Bay Area Counties, Freeways and Census Tract Centroids")
```

### Coordinate Referencing System

Say you have data you need to make spatial with a spatial reference

`sierra <- read_csv("sierraClimate.csv")`

EPSG or CRS codes are an easy way to provide coordinate referencing.  

Two ways of doing the same thing. 

1. Spell it out:
```
GCS <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
wsta = st_as_sf(sierra, coords = c("LONGITUDE","LATITUDE"), crs=GCS)
```

2. Google to find the code you need and assign it to the crs parameter:

`wsta <- st_as_sf(sierra, coords = c("LONGITUDE","LATITUDE"), crs=4326)`

#### *Removing* Geometry

There are many instances where you want to remove geometry from a sf data frame

- Some R functions run into problems with geometry and produce confusing error messages, like "non-numeric argument"

- You're wanting to work with an sf data frame in a non-spatial way

One way to remove geometry:

`myNonSFdf <- mySFdf %>% st_set_geometry(NULL)`

### Spatial join `st_join`

A spatial join with st_join
joins data from census where TRI points occur

```{r, message=FALSE}
TRI_sp <- st_as_sf(TRI_2017_CA, coords = c("LONGITUDE", "LATITUDE"), crs=4326) %>%
  st_join(BayAreaTracts) %>%
  filter(CNTY_FIPS %in% c("013", "095"))
```


### Plotting maps in the base plot system

There are various programs for creating maps from spatial data, and we'll look at a few after we've looked at rasters. As usual, the base plot system often does something useful when you give it data.

```{r}
plot(BayAreaCounties)
```

And with just one variable:

```{r}
plot(BayAreaCounties["POP_SQMI"])
```

There's a lot more we could do with the base plot system, but we'll mostly focus on
some better options in ggplot2 and tmap.


## Raster GIS in R

Simple Features are feature-based, so based on the name I guess it's not surprising that sf doesn't have support for rasters. But we can use the raster package for that. 

**A bit of raster reading and map algebra with Marble Mountains elevation data**

```{r message=FALSE}
library(raster)
rasPath <- system.file("extdata","elev.tif", package="iGIScData")
elev <- raster(rasPath)
slope <- terrain(elev, opt="slope")
aspect <- terrain(elev, opt="aspect")
slopeclasses <-matrix(c(0,0.2,1, 0.2,0.4,2, 0.4,0.6,3,
                        0.6,0.8,4, 0.8,1,5), ncol=3, byrow=TRUE)
slopeclass <- reclassify(slope, rcl = slopeclasses)

plot(elev)
plot(slope)
plot(slopeclass)
plot(aspect)
```

### Raster from scratch

```{r}
new_raster2 <- raster(nrows = 6, ncols = 6, res = 0.5,
                      xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5,
                      vals = 1:36)
plot(new_raster2)

```


## ggplot2 for maps

The Grammar of Graphics is the gg of ggplot.

- Key concept is separating aesthetics from data
- Aesthetics can come from variables (using aes()setting) or be constant for the graph

Mapping tools that follow this lead

- ggplot, as we have seen, and it continues to be enhanced
- tmap (Thematic Maps) https://github.com/mtennekes/tmap
Tennekes, M., 2018, tmap: Thematic Maps in R, *Journal of Statistical Software* 84(6), 1-39

```{r}
ggplot(CA_counties) + geom_sf()

```


Try `?geom_sf` and you'll find that its first parameters is mapping with `aes()` by default. The data property is inherited from the ggplot call, but commonly you'll want to specify data=something in your geom_sf call.

**Another simple ggplot, with labels**

```{r}
ggplot(CA_counties) + geom_sf() +
  geom_sf_text(aes(label = NAME), size = 1.5)

```

**and now with fill color**

```{r}
ggplot(CA_counties) + geom_sf(aes(fill = MED_AGE)) +
  geom_sf_text(aes(label = NAME), col="white", size=1.5)
```

**Repositioned legend, no "x" or "y" labels**

```{r warning=FALSE}
ggplot(CA_counties) + geom_sf(aes(fill=MED_AGE)) +
  geom_sf_text(aes(label = NAME), col="white", size=1.5) +
  theme(legend.position = c(0.8, 0.8)) +
  labs(x="",y="")

```


**Map in ggplot2, zoomed into two counties:**

```{r warning=FALSE}
library(tidyverse); library(sf); library(iGIScData)
census <- BayAreaTracts %>%
   filter(CNTY_FIPS %in% c("013", "095"))
TRI <- TRI_2017_CA %>%
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"), crs=4326) %>%
  st_join(census) %>%
  filter(CNTY_FIPS %in% c("013", "095"),
         (`5.1_FUGITIVE_AIR` + `5.2_STACK_AIR`) > 0)
bnd = st_bbox(census)
ggplot() +
  geom_sf(data = BayAreaCounties, aes(fill = NAME)) +
  geom_sf(data = census, color="grey40", fill = NA) +
  geom_sf(data = TRI) +
  coord_sf(xlim = c(bnd[1], bnd[3]), ylim = c(bnd[2], bnd[4])) +
  labs(title="Census Tracts and TRI air-release sites") +
  theme(legend.position = "none")

```

### Rasters in ggplot2

Raster display in ggplot2 is currently a little awkward, as are rasters in general in the feature-dominated GIS world.

We can use a trick: converting rasters to a grid of points:

```{r}
library(tidyverse)
library(sf)
library(raster)
rasPath <- system.file("extdata","elev.tif", package="iGIScData")
elev <- raster(rasPath)
shpPath <- system.file("extdata","trails.shp", package="iGIScData")
trails <- st_read(shpPath)
elevpts = as.data.frame(rasterToPoints(elev))
ggplot() +
  geom_raster(data = elevpts, aes(x = x, y = y, fill = elev)) +
  geom_sf(data = trails)
```

## tmap

Basic building block is tm_shape(data) followed by various layer elements such as tm_fill()
shape can be features or raster
See Geocomputation with R Chapter 8 "Making Maps with R" for more information.
https://geocompr.robinlovelace.net/adv-map.html

```{r}
library(spData)
library(tmap)
tm_shape(world) + tm_fill() + tm_borders()
```

**Color by variable**

```{r}
library(sf)
library(tmap)
tm_shape(BayAreaTracts) + tm_fill(col = "MED_AGE")

```

**tmap of sierraFeb with hillshade and point symbols**

```{r warning=FALSE}
library(tmap)
library(sf)
library(raster)
tmap_mode("plot")
tmap_options(max.categories = 8)
#sierraFeb <- st_read("data/sierraFeb.csv")
sierra <- st_as_sf(sierraFeb, coords = c("LONGITUDE", "LATITUDE"), crs = 4326)
#hillsh <- raster("data/ca_hillsh_WGS84.tif")
bounds <- st_bbox(sierra)
tm_shape(CAhillsh,bbox=bounds)+
  tm_raster(palette="-Greys",legend.show=FALSE,n=10) + tm_shape(sierra) + tm_symbols(col="TEMPERATURE",
     palette=c("blue","red"), style="cont",n=8) +
  tm_legend() + 
  tm_layout(legend.position=c("RIGHT","TOP"))

```

*Note: "-Greys" needed to avoid negative image, since "Greys" go from light to dark, and to match reflectance as with b&w photography, they need to go from dark to light.*

### Interactive Maps

The word "static" in "static maps" isn't something you would have heard in a cartography class 30 years ago, since essentially *all* maps then were static. Very important in designing maps is considering your audience, and one characteristic of the audience of those maps of yore were that they were printed and thus fixed on paper.  A lot of cartographic design relates to that property:  

- Figure-to-ground relationships assume "ground" is a white piece of paper (or possibly a standard white background in a pdf), so good cartographic color schemes tend to range from light for low values to dark for high values.
- Scale is fixed and there are no "tools" for changing scale, so a lot of attention must be paid to providing scale information.
- Similarly, without the ability to see the map at different scales, inset maps are often needed to provide context.

Interactive maps change the game in having tools for changing scale, and *always* being "printed" on a computer or device where the color of the background isn't necessarily white. We are increasingly used to using interactive maps on our phones or other devices, and often get frustrated not being able to zoom into a static map.

A widely used interactive mapping system is Leaflet, but we're going to use tmap to access Leaflet behind the scenes and allow us to create maps with one set of commands.  The key parameter needed is tmap_mode which must be set to "view" to create an interactive map.

```{r}
tmap_mode("view")
tm_shape(BayAreaTracts) + tm_fill(col = "MED_AGE", alpha = 0.5)
```


```{r}
library(tmap)
library(sf)
tmap_mode("view")
tmap_options(max.categories = 8)
sierra <- st_as_sf(sierraFeb, coords = c("LONGITUDE", "LATITUDE"), crs = 4326)
bounds <- st_bbox(sierra)
tm_basemap(leaflet::providers$Esri.NatGeoWorldMap) +
  tm_shape(sierra) + tm_symbols(col="TEMPERATURE",
  palette=c("blue","red"), style="cont",n=8,size=0.2) +
  tm_legend() + 
  tm_layout(legend.position=c("RIGHT","TOP"))

```




#### Leaflet

Now that we've seen an app that used it, let's look briefly at Leaflet itself, and we'll see that even the Leaflet package in R actually uses JavaScript...

Leaflet is designed as "An open-source JavaScript library for mobile-friendly interactive maps"   https://leafletjs.com
"The **R** package **leaflet** is an interface to the JavaScript library **Leaflet** to create interactive web maps. It was developed on top of the htmlwidgets framework, which means the maps can be rendered in **RMarkdown** (v2) documents (which is why you can see it in this document), Shiny apps, and RStudio IDE / the R console." 

https://blog.rstudio.com/2015/06/24/leaflet-interactive-web-maps-with-r/

https://github.com/rstudio/cheatsheets/blob/master/leaflet.pdf

```{r}
library(leaflet)
m <- leaflet() %>%
  addTiles() %>%  # default OpenStreetMap tiles
  addMarkers(lng=174.768, lat=-36.852,
             popup="The birthplace of R")
m 
```

## Exercises

1. Using the method of building simple sf geometries, build a simple 1x1 square object and plot it. Remember that you have to close the polygon, so the first vertex is the same as the last (of 5) vertices.

```{r include=F}
library(sf)
library(tidyverse)
square <- st_sfc(st_polygon(list(rbind(c(0,0),c(0,1),c(1,1),c(1,0),c(0,0)))))
plot(square)
```

2. Build a map in ggplot of Colorado, Wyoming, and Utah with these boundary vertices in GCS. As with the square, remember to close each figure, and assign the crs to what is needed for GCS: 4326.  

- Colorado: $(-109,41),(-102,41),(-102,37),(-109,37)$
- Wyoming: $(-111,45),(-104,45),(-104,41),(-111,41)$
- Utah: $(-114,42),(-111,42),(-111,41),(-109,41),(-109,37),(-114,37)$
- Arizona: $(-114,37),(-109,37),(-109,31.3),(-111,31.3),(-114.8,32.5),(-114.6,32.7),(-114.1,34.3),(-114.5,35),(-114.5,36),(-114,36)$
- New Mexico: $(-109,37),(-103,37),(-103,32),(-106.6,32),(-106.5,31.8),(-108.2,31.8),(-108.2,31.3),(-109,31.3)$

```{r include=F}
CO <- st_polygon(list(rbind(c(-109,41),c(-102,41),c(-102,37),c(-109,37),c(-109,41))))
WY <- st_polygon(list(rbind(c(-111,45),c(-104,45),c(-104,41),c(-111,41),c(-111,45))))
UT <- st_polygon(list(rbind(c(-114,42),c(-111,42),c(-111,41),c(-109,41),c(-109,37),c(-114,37),c(-114,42))))
AZ <- st_polygon(list(rbind(c(-114,37),c(-109,37),c(-109,31.3),c(-111,31.3),c(-114.8,32.5),c(-114.6,32.7),c(-114.1,34.3),c(-114.5,35),c(-114.5,36),c(-114,36),c(-114,37))))
NM <- st_polygon(list(rbind(c(-109,37),c(-103,37),c(-103,32),c(-106.6,32),c(-106.5,31.8),c(-108.2,31.8),c(-108.2,31.3),c(-109,31.3),c(-109,37))))
sfc5states <- st_sfc(CO,WY,UT,AZ,NM, crs=4326)
ggplot() + geom_sf(data=sfc5states)

```

3. Add in the code for CA and NV and create kind of a western US map...
```{r include=F}
CO <- st_polygon(list(rbind(c(-109,41),c(-102,41),c(-102,37),c(-109,37),c(-109,41))))
WY <- st_polygon(list(rbind(c(-111,45),c(-104,45),c(-104,41),c(-111,41),c(-111,45))))
UT <- st_polygon(list(rbind(c(-114,42),c(-111,42),c(-111,41),c(-109,41),c(-109,37),c(-114,37),c(-114,42))))
AZ <- st_polygon(list(rbind(c(-114,37),c(-109,37),c(-109,31.3),c(-111,31.3),c(-114.8,32.5),c(-114.6,32.7),c(-114.1,34.3),c(-114.5,35),c(-114.5,36),c(-114,36),c(-114,37))))
NM <- st_polygon(list(rbind(c(-109,37),c(-103,37),c(-103,32),c(-106.6,32),c(-106.5,31.8),c(-108.2,31.8),c(-108.2,31.3),c(-109,31.3),c(-109,37))))

CA <- st_polygon(list(rbind(c(-124,42),c(-120,42),c(-120,39),c(-114.5,35),
  c(-114.1,34.3),c(-114.6,32.7),c(-117,32.5),c(-118.5,34),c(-120.5,34.5),
  c(-122,36.5),c(-121.8,36.8),c(-122,37),c(-122.4,37.3),c(-122.5,37.8),
  c(-123,38),c(-123.7,39),c(-124,40),c(-124.4,40.5),c(-124,41),c(-124,42))))
NV <- st_polygon(list(rbind(c(-120,42),c(-114,42),c(-114,36),c(-114.5,36),
  c(-114.5,35),c(-120,39),c(-120,42))))

sfc7states <- st_sfc(CO,WY,UT,AZ,NM,CA,NV, crs=4326)
ggplot() + geom_sf(data=sfc7states)

```

3. Create an sf class from the five states adding the fields `name`, `abb`, `area_sqkm`, and `population`, and create a map labeling with the name.

- Colorado, CO, 269837, 5758736
- Wyoming, WY, 253600, 578759
- Utah, UT, 84899, 3205958
- Arizona, AZ, 295234, 7278717
- New Mexico, NM, 314917, 2096829
- California, CA, 423970, 39368078
- Nevada, NV, 286382, 3080156

```{r include=F}
attributes <- bind_rows(c(name="Colorado", abb="CO", area=269837, pop=5758736),
                        c(name="Wyoming", abb="WY", area=253600, pop=578759),
                        c(name="Utah", abb="UT", area=84899, pop=3205958),
                        c(name="Arizona", abb="AZ", area=295234, pop=7278717),
                        c(name="New Mexico", abb="NM", area=314917, pop=2096829),
                        c(name="California", abb="CA", area=423970, pop=39368078),
                        c(name="Nevada", abb="NV", area=286382, pop=3080156))
SW_States <- st_sf(attributes, geometry = sfc7states)
ggplot(SW_States) + geom_sf(aes(fill=pop)) + geom_sf_text(aes(label = name))
```

4. Create a tibble for the highest peaks in the 7 states, with the following names, elevations in m, longitude and latitude, and add them to that map:

- Wheeler Peak, 4011, -105.4, 36.5
- Mt. Whitney, 4421, -118.2, 36.5
- Boundary Peak, 4007, -118.35, 37.9
- Kings Peak, 4120, -110.3, 40.8
- Gannett Peak, 4209, -109, 43.2
- Mt. Elbert, 4401, -106.4, 39.1
- Humphreys Peak, 3852, -111.7, 35.4

Note: the easiest way to do this is with the tribble function, starting with:
```
peaks <- tribble(
  ~peak, ~elev, ~longitude, ~latitude,
  "Wheeler Peak", 4011, -105.4, 36.5,
```

```{r include=F}
peaks <- tribble(
  ~peak, ~elev, ~longitude, ~latitude,
  "Wheeler Peak", 4011, -105.4, 36.5,
  "Mt. Whitney", 4421, -118.2, 36.5,
  "Boundary Peak", 4007, -118.35, 37.9,
  "Kings Peak", 4120, -110.3, 40.8,
  "Gannett Peak", 4209, -109, 43.2,
  "Mt. Elbert", 4401, -106.4, 39.1,
  "Humphreys Peak", 3852, -111.7, 35.4)
peaksp <- st_as_sf(peaks, coords=c("longitude", "latitude"), crs=4326)

ggplot(SW_States) + geom_sf(aes(fill=pop)) + geom_sf(data=peaksp) + geom_sf_label(data=peaksp, aes(label=peak))

  
```


5. Use a spatial join to add the points to the states to provide a new attribute maximum elevation, and display that using geom_sf_text() with the state polygons.

```{r include=F}
SW_States %>%
  st_join(peaksp) %>%
  ggplot() + geom_sf() + geom_sf_text(aes(label=elev))

```


6. Even though the result isn't terribly useful, send that spatially joined data to the base plot system to see what you get.

```{r include=F}
SW_States %>%
  st_join(peaksp) %>%
  plot()

```

7. From the CA_counties and CAfreeways feature data in iGIScData, make a simple map in ggplot, with freeways colored red.

```{r include=F}
ggplot(CA_counties) + geom_sf() + geom_sf(data=CAfreeways, col="red")
```

8. After adding the raster library, create a raster from the built-in `volcano` matrix of elevations from Auckland's Maunga Whau Volcano, and use plot() to display it.  We'd do more with that dataset but we don't know what the cell size is.

```{r include=F}
library(raster)
v <- raster(volcano)
plot(v)
```

9. Use tmap to create a simple map from the SW_States (polygons) and peaksp (points) data we created earlier.  Hints: you'll want to use tm_text with text set to "peak" to label the points, along with the parameter `auto.placement=TRUE`.

```{r include=F}
library(tmap)
tmap_mode("plot")
tm_shape(SW_States) + tm_borders() +
  tm_shape(peaksp) + tm_symbols(col = "red") + tm_text(text="peak", auto.placement=T)
```

10. Change the map to the view mode, but don't use the state borders since the basemap will have them. Just before adding shapes, set the basemap to leaflet::providers$Esri.NatGeoWorldMap, then continue to the peaks after the + to see the peaks on a National Geographic basemap.

```{r include=F}
tmap_mode("view")
tm_basemap(leaflet::providers$Esri.NatGeoWorldMap) +
tm_shape(peaksp) + tm_symbols(col = "red") + tm_text(text="peak", auto.placement=T)

```



<!--chapter:end:04-spatial.Rmd-->

